<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="fantasykai`blog"><title>如何理解 LSTM神经网络模型 | 枫哲's文栖小筑</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?' + '2bb92548008bd1f1f88213efd40c8dad';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><div class="darkmode-toggle">🌓</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">如何理解 LSTM神经网络模型</h1><a id="logo" href="/.">枫哲's文栖小筑</a><p class="description">君子终日乾乾，夕惕若厉，无咎</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/timeline/"><i class="fa fa-book"> 历史</i></a><a href="/blogroll/"><i class="fa fa-external-link"> 收藏链接</i></a><a href="/2048/"><i class="fa fa-gamepad"> 放松下</i></a><a href="/guestbook/"><i class="fa fa-comments"> 留言</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">如何理解 LSTM神经网络模型</h1><div class="post-meta">2024-02-27<span> | </span><span class="category"><a href="/categories/AI/">AI</a></span></div><a class="disqus-comment-count" href="/posts/63195/#vcomment"><span class="waline-comment-count" id="/posts/63195/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3-LSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">如何理解 LSTM神经网络模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text">循环神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E6%9C%9F%E4%BE%9D%E8%B5%96%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.2.</span> <span class="toc-text">长期依赖的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text">LSTM 网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-%E8%83%8C%E5%90%8E%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">1.4.</span> <span class="toc-text">LSTM 背后的核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%90%E6%AD%A5%E4%BA%86%E8%A7%A3-LSTM"><span class="toc-number">1.5.</span> <span class="toc-text">逐步了解 LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E6%9C%9F%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%9A%84%E5%8F%98%E4%BD%93"><span class="toc-number">1.6.</span> <span class="toc-text">长期短期记忆的变体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.7.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%B4%E8%B0%A2"><span class="toc-number">1.8.</span> <span class="toc-text">致谢</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.9.</span> <span class="toc-text"></span></a></li></ol></li></ol></div></div><div class="post-content"><blockquote>
<p>转载 <a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
</blockquote>
<h1 id="如何理解-LSTM神经网络模型"><a href="#如何理解-LSTM神经网络模型" class="headerlink" title="如何理解 LSTM神经网络模型"></a>如何理解 LSTM神经网络模型</h1><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>人类不会每分每秒都从头开始思考。当你阅读这篇文章时，你会根据对前面单词的理解来理解每个单词。你不会抛弃一切，重新从头开始思考。你的思想具有持久性。</p>
<p>传统的神经网络无法做到这一点，这似乎是一个重大缺陷。例如，假设你想对电影中每个时刻发生的事件进行分类。目前还不清楚传统的神经网络如何利用其对电影中先前事件的推理来指导后续事件。</p>
<p>循环神经网络解决了这个问题。它们是具有循环的网络，可使信息持久存在。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="img"></p>
<p><strong>循环神经网络</strong></p>
<p>在上图中，神经网络的一个部分，A，查看一些输入<code>X_t</code>，并输出一个值<code>h_t</code>. 循环允许信息从网络的一个步骤传递到下一个步骤。</p>
<p>这些循环使得循环神经网络看起来有些神秘。但是，如果你再仔细想想，就会发现它们与普通神经网络并没有太大区别。循环神经网络可以看作是同一网络的多个副本，每个副本都会向后继者传递一条消息。考虑一下如果我们展开循环会发生什么：</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="展开的循环神经网络。"></p>
<p><strong>展开的循环神经网络。</strong></p>
<p>这种链式特性表明循环神经网络与序列和列表密切相关。它们是用于此类数据的神经网络的自然架构。</p>
<p>而且它们确实被使用！在过去的几年中，RNN 在各种问题上的应用取得了令人难以置信的成功：语音识别、语言建模、翻译、图像字幕……等等。在 Andrej Karpathy 的优秀博客文章《循环<a target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">神经网络的不合理有效性》</a>中讨论使用 RNN 可以实现的惊人成就。但它们确实非常了不起。</p>
<p>这些成功的关键在于使用“LSTM”，这是一种非常特殊的循环神经网络，在许多任务中，它的效果比标准版本好得多。几乎所有基于循环神经网络的令人兴奋的结果都是通过它们实现的。本文将探讨的就是这些 LSTM。</p>
<h2 id="长期依赖的问题"><a href="#长期依赖的问题" class="headerlink" title="长期依赖的问题"></a>长期依赖的问题</h2><p>RNN 的吸引力之一是它们可能能够将先前的信息与当前任务联系起来，例如使用先前的视频帧可能有助于理解当前帧。如果 RNN 能够做到这一点，它们将非常有用。但它们能做到吗？这取决于情况。</p>
<p>有时，我们只需要查看最近的信息即可执行当前任务。例如，考虑一个语言模型，它试图根据前面的单词预测下一个单词。如果我们试图预测 <strong>“云在天空中”</strong> 中的最后一个单词，我们不需要任何其他上下文——很明显下一个单词将是天空。在这种情况下，相关信息与需要它的地方之间的差距很小，RNN 可以学习使用过去的信息。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" alt="img"></p>
<p>但也有需要更多背景信息的情况。考虑尝试预测文本“我在法国长大……我能说一口流利的<em>法语</em>”中的最后一个单词。最近的信息表明下一个单词可能是一种语言的名称，但如果我们想缩小范围，确定是哪种语言，我们需要更早的法国背景信息。相关信息与需要信息点之间的差距完全有可能变得非常大。</p>
<p>不幸的是，随着差距的扩大，RNN 无法学会连接信息。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" alt="神经网络难以应对长期依赖关系。"></p>
<p>理论上，RNN 绝对有能力处理这种“长期依赖关系”。人类可以仔细挑选参数来解决这种形式的小问题。遗憾的是，在实践中，RNN 似乎无法学习它们。Hochreiter [(1991) <a target="_blank" rel="noopener" href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">德语]</a>和<a target="_blank" rel="noopener" href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio 等人 (1994)</a>深入探讨了这个问题，他们发现了一些非常根本的原因，说明这个问题可能很难解决。</p>
<p>值得庆幸的是，LSTM 没有这个问题！</p>
<h2 id="LSTM-网络"><a href="#LSTM-网络" class="headerlink" title="LSTM 网络"></a>LSTM 网络</h2><p>长短期记忆网络（通常简称为“LSTM”）是一种特殊的 RNN，能够学习长期依赖关系。它们由<a target="_blank" rel="noopener" href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter 和 Schmidhuber（1997 年）</a>提出，并在后续工作中被许多人改进和推广。1<a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1">它们</a>在解决各种问题方面都表现出色，目前已被广泛使用。</p>
<p>LSTM 的设计明确地避免了长期依赖问题。长时间记忆信息实际上是它们的默认行为，而不是它们需要努力学习的事情！</p>
<p>所有循环神经网络都具有神经网络重复模块链的形式。在标准 RNN 中，此重复模块将具有非常简单的结构，例如单个 tanh 层。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" alt="img"></p>
<p><strong>标准 RNN 中的重复模块包含单层。</strong></p>
<p>LSTM 也具有这种链式结构，但重复模块的结构不同。它不是只有一个神经网络层，而是有四个，并以非常特殊的方式相互作用。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="LSTM 神经网络。"></p>
<p><strong>LSTM 中的重复模块包含四个交互层。</strong></p>
<p>不必担心细节。稍后我们将逐步介绍 LSTM 图。现在，我们先熟悉一下我们将要使用的符号。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p>
<p>在上图中，每条线都承载着一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示逐点运算，如向量加法，而黄色框表示学习到的神经网络层。线合并表示连接，而线分叉表示其内容被复制，副本被发送到不同的位置。</p>
<h2 id="LSTM-背后的核心思想"><a href="#LSTM-背后的核心思想" class="headerlink" title="LSTM 背后的核心思想"></a>LSTM 背后的核心思想</h2><p>LSTM 的关键是细胞状态，即贯穿图表顶部的水平线。</p>
<p>细胞状态有点像传送带。它沿着整个链条笔直运行，只有一些微小的线性相互作用。信息很容易不加改变地沿着它流动。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p>
<p>LSTM 确实有能力从细胞状态中删除或添加信息，并由称为门的结构严格调节。</p>
<p>门是一种选择性地让信息通过的方式。它们由 S 型神经网络层和逐点乘法运算组成。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p>
<p>S 型层输出介于 0 和 1 之间的数字，描述每个组件应通过的量。值为 0 表示“不让任何组件通过”，而值为 1 表示“让所有组件通过！”</p>
<p>LSTM 有三个这样的门，用于保护和控制单元状态。</p>
<h2 id="逐步了解-LSTM"><a href="#逐步了解-LSTM" class="headerlink" title="逐步了解 LSTM"></a>逐步了解 LSTM</h2><p>我们的 LSTM 的第一步是决定要从细胞状态中丢弃哪些信息。这个决定是由一个叫做“遗忘门”的 S 型层做出的。它查看<code>h_&#123;t-1&#125;</code>和<code>x_t</code>，并输出一个介于 0 和 1 对于细胞状态中的每个数字<code>C_&#123;t-1&#125;</code>。 A. 1 代表“完全保留这一点”，而0 代表“彻底摆脱这个。”</p>
<p>让我们回到语言模型的示例，该模型尝试根据所有先前的单词预测下一个单词。在这样的问题中，单元状态可能包括当前主语的性别，以便可以使用正确的代词。当我们看到一个新主语时，我们希望忘记旧主语的性别。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p>
<p>下一步是决定我们要在单元状态中存储哪些新信息。这分为两部分。首先，一个称为“输入门”的 S 型层决定我们要更新哪些值。接下来，一个 tanh 层创建一个新候选值的向量，<code>C̃_t</code>，可以将其添加到状态中。在下一步中，我们将把这两者结合起来，以创建对状态的更新。</p>
<p>在我们的语言模型示例中，我们希望将新主语的性别添加到单元状态中，以替换我们忘记的旧主语。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p>
<p>现在是时候更新旧的细胞状态了，<code>x_&#123;t-1&#125;</code>，进入新的细胞状态 <code>C_t</code> 前面的步骤已经决定了要做什么，我们只需要实际去做。</p>
<p>我们将旧状态乘以 <code>f~_t</code>，忘记我们之前决定忘记的事情。然后我们添加 <code>i_t</code>*<code>C̃_t</code>。这是新的候选值，根据我们决定更新每个状态值的程度进行缩放。</p>
<p>在语言模型的情况下，我们实际上会删除有关旧主题性别的信息并添加新信息，正如我们在前面的步骤中所决定的那样。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p>
<p>最后，我们需要决定要输出什么。此输出将基于我们的细胞状态，但将是一个经过过滤的版本。首先，我们运行一个 S 型层，它决定要输出细胞状态的哪些部分。然后，我们将细胞状态通过双曲函数双曲函数（将值推至− 1 和 1 ）并将其乘以 S 型门的输出，这样我们就只输出我们决定的部分。</p>
<p>对于语言模型示例，由于它刚刚看到一个主语，因此它可能需要输出与动词相关的信息，以防接下来是动词。例如，它可能会输出主语是单数还是复数，这样我们就知道如果接下来是动词，动词应该变位成什么形式。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p>
<h2 id="长期短期记忆的变体"><a href="#长期短期记忆的变体" class="headerlink" title="长期短期记忆的变体"></a>长期短期记忆的变体</h2><p>到目前为止，我所描述的是一个非常普通的 LSTM。但并非所有 LSTM 都与上面的相同。事实上，几乎每篇涉及 LSTM 的论文似乎都使用了略有不同的版本。这些差异很小，但值得一提的是其中的一些。</p>
<p><a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers 和 Schmidhuber (2000)</a>提出的一种流行的 LSTM 变体是添加“窥孔连接”。这意味着我们让门层查看单元状态。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png" alt="img"></p>
<p>上图为所有门添加了窥视孔，但许多论文只会给出一些窥视孔，而不会给出其他窥视孔。</p>
<p>另一种变体是使用耦合的遗忘门和输入门。我们不是分别决定遗忘什么以及应该向什么添加新信息，而是一起做出这些决定。我们只会在要输入某些东西来代替它时才会忘记。我们只会在忘记旧东西时向状态输入新值。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png" alt="img"></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1406.1078v3.pdf">LSTM 的一个稍微更戏剧性的变体是Cho 等人（2014 年）</a>提出的门控循环单元（GRU ）。它将遗忘门和输入门合并为一个“更新门”。它还合并了单元状态和隐藏状态，并做了一些其他更改。由此产生的模型比标准 LSTM 模型更简单，而且越来越受欢迎。</p>
<p><img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" alt="门控循环单元神经网络。"></p>
<p>这些只是最值得注意的 LSTM 变体中的一小部分。还有很多其他的变体，例如<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1508.03790v2.pdf">Yao 等人 (2015)</a>提出的深度门控 RNN 。还有一些完全不同的方法来解决长期依赖关系，例如<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1402.3511v1.pdf">Koutnik 等人 (2014)</a>提出的 Clockwork RNN 。</p>
<p>这些变体中哪一个最好？差异重要吗？<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1503.04069.pdf">Greff 等人（2015 年）</a>对流行的变体进行了很好的比较，发现它们都差不多。Jozefowicz<a target="_blank" rel="noopener" href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">等人（2015 年）</a>测试了超过一万个 RNN 架构，发现有些架构在某些任务上比 LSTM 效果更好。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>之前，我提到了人们使用 RNN 取得的显著成果。基本上，所有这些都是使用 LSTM 实现的。它们在大多数任务上确实表现更好！</p>
<p>LSTM 写成一组方程式后，看起来相当吓人。希望本文一步步介绍它们能让你更容易理解。</p>
<p>LSTM 是我们利用 RNN 实现的一大进步。人们自然会想：还有另一大进步吗？研究人员的普遍看法是：“是的！还有下一步，那就是注意力！”这个想法是让 RNN 的每一步都从更大的信息集合中挑选信息来查看。例如，如果您使用 RNN 创建描述图像的标题，它可能会挑选图像的一部分来查看它输出的每个单词。事实上，<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1502.03044v2.pdf">Xu<em>等人</em>(2015)</a>就是这么做的——如果您想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的成果，而且似乎还有更多的成果即将面世……</p>
<p>注意力并不是 RNN 研究中唯一令人兴奋的线索。例如，<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1507.01526v1.pdf">Kalchbrenner<em>等人</em>(2015)</a>的网格 LSTM看起来非常有前景。在生成模型中使用 RNN 的工作 - 例如<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1502.04623.pdf">Gregor<em>等人</em>(2015)</a>、<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1506.02216v3.pdf">Chung<em>等人</em>(2015)</a>或<a target="_blank" rel="noopener" href="http://arxiv.org/pdf/1411.7610v3.pdf">Bayer &amp; Osendorfer (2015)</a> - 也看起来非常有趣。过去几年对于循环神经网络来说是激动人心的几年，而未来的几年只会更加激动人心！</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>我非常感谢许多人帮助我更好地理解 LSTM、对可视化效果发表评论并对这篇文章提供反馈。</p>
<p>我非常感谢 Google 的同事们提供的有益反馈，特别是<a target="_blank" rel="noopener" href="http://research.google.com/pubs/OriolVinyals.html">Oriol Vinyals</a>、<a target="_blank" rel="noopener" href="http://research.google.com/pubs/GregCorrado.html">Greg Corrado</a>、<a target="_blank" rel="noopener" href="http://research.google.com/pubs/JonathonShlens.html">Jon Shlens</a>、<a target="_blank" rel="noopener" href="http://people.cs.umass.edu/~luke/">Luke Vilnis</a>和<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a>。我还要感谢许多其他朋友和同事抽出时间帮助我，包括<a target="_blank" rel="noopener" href="https://www.linkedin.com/pub/dario-amodei/4/493/393">Dario Amodei</a>和<a target="_blank" rel="noopener" href="http://cs.stanford.edu/~jsteinhardt/">Jacob Steinhardt</a>。我特别感谢<a target="_blank" rel="noopener" href="http://www.kyunghyuncho.me/">Kyunghyun Cho</a>对我的图表的极其周到的反馈。</p>
<p>在写这篇文章之前，我在两个关于神经网络的研讨会系列中练习解释 LSTM。感谢所有参加研讨会的人对我的耐心和反馈。</p>
<hr>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2></div><div class="tags"><a href="/tags/深度学习"><i class="fa fa-tag">深度学习</i></a></div><div class="post-nav"><a class="pre" href="/posts/23213/">基于神经网络的网络流量预测</a><a class="next" href="/posts/53000/">Makedown数学公式参考</a></div><div id="waline"></div><script src="//unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" type="text/css" href="//unpkg.com/@waline/client@v2/dist/waline.css"><script>let metaInfo = ['nick', 'mail', 'link']
let requiredMeta = 'nick,mail'.split(',').filter(item => {
  return metaInfo.indexOf(item) > -1
})
Waline.init({
  el: '#waline',
  comment: true,
  serverURL: 'https://comment.aimak.cn',
  pageSize: '30',
  wordLimit: '500',
  requiredMeta,
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/avatar.png"/></a><p>认真生活.</p><a class="info-icon" href="https://twitter.com/fantasykaicc" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:makren@126.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/fantasykai" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/AI-%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/">AI, 网络运维</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/docker/">docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%89%8D%E7%AB%AF/">前端</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AE%B9%E5%99%A8/">容器</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">36</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/">技术小栈</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E8%AE%B0/">杂记</a><span class="category-list-count">72</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B5%8B%E8%AF%95/">测试</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%93%E5%AD%98/">缓存</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BD%91%E7%BB%9C/">网络</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E5%BD%95/">语录</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%B7%91%E6%AD%A5/">跑步</a><span class="category-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF-%E8%A1%8C%E4%B8%9A%E5%8A%A8%E6%80%81/" style="font-size: 15px;">人工智能, 技术趋势, 行业动态</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15px;">人工智能</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">深度学习</a> <a href="/tags/AI-%E6%99%BA%E8%83%BD%E4%BD%93/" style="font-size: 15px;">AI 智能体</a> <a href="/tags/Deepseek-%E8%87%AA%E6%99%BA%E7%BD%91%E7%BB%9C-%E8%BF%90%E8%90%A5%E5%95%86-%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/" style="font-size: 15px;">Deepseek, 自智网络, 运营商, 网络运维</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" style="font-size: 15px;">知识图谱</a> <a href="/tags/%E6%9E%B6%E6%9E%84/" style="font-size: 15px;">架构</a> <a href="/tags/PMO/" style="font-size: 15px;">PMO</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F/" style="font-size: 15px;">技术团队</a> <a href="/tags/%E5%BC%80%E5%8F%91%E6%95%88%E7%8E%87/" style="font-size: 15px;">开发效率</a> <a href="/tags/%E6%9D%82%E8%AE%B0/" style="font-size: 15px;">杂记</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/thymeleaf/" style="font-size: 15px;">thymeleaf</a> <a href="/tags/spring/" style="font-size: 15px;">spring</a> <a href="/tags/idea/" style="font-size: 15px;">idea</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/Chrome-%E6%95%88%E7%8E%87%E6%8F%92%E4%BB%B6/" style="font-size: 15px;">Chrome 效率插件</a> <a href="/tags/%E8%BF%90%E7%BB%B4/" style="font-size: 15px;">运维</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Mac/" style="font-size: 15px;">Mac</a> <a href="/tags/Jenkins/" style="font-size: 15px;">Jenkins</a> <a href="/tags/%E5%AE%B9%E5%99%A8/" style="font-size: 15px;">容器</a> <a href="/tags/nginx/" style="font-size: 15px;">nginx</a> <a href="/tags/restful/" style="font-size: 15px;">restful</a> <a href="/tags/IPv6/" style="font-size: 15px;">IPv6</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/" style="font-size: 15px;">技术小栈</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 15px;">前端</a> <a href="/tags/CAP/" style="font-size: 15px;">CAP</a> <a href="/tags/js/" style="font-size: 15px;">js</a> <a href="/tags/%E7%BC%93%E5%AD%98/" style="font-size: 15px;">缓存</a> <a href="/tags/ideas/" style="font-size: 15px;">ideas</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">并发</a> <a href="/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">高并发</a> <a href="/tags/Mysql/" style="font-size: 15px;">Mysql</a> <a href="/tags/%E6%B5%8B%E8%AF%95/" style="font-size: 15px;">测试</a> <a href="/tags/%E6%97%A5%E5%BF%97/" style="font-size: 15px;">日志</a> <a href="/tags/mongodb/" style="font-size: 15px;">mongodb</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">网络</a> <a href="/tags/SDN/" style="font-size: 15px;">SDN</a> <a href="/tags/%E8%AF%AD%E5%BD%95/" style="font-size: 15px;">语录</a> <a href="/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/" style="font-size: 15px;">马拉松</a> <a href="/tags/%E5%81%A5%E5%BA%B7/" style="font-size: 15px;">健康</a> <a href="/tags/%E6%8A%A5%E5%91%8A%E5%88%86%E4%BA%AB/" style="font-size: 15px;">报告分享</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/posts/11175/">AI领域最新资讯：突破、趋势与展望</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/11174/">AI领域最新资讯：突破、趋势与展望</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/11173/">AI领域最新资讯：突破、趋势与展望</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/39555/">AI领域最新资讯：突破、趋势与展望</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/63210/">每日资讯</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/63209/">每日资讯</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/63208/">每日资讯</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/46858/">2025年2月AI领域月度回顾：突破、趋势与展望</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/63207/">每日资讯</a></li><li class="post-list-item"><a class="post-list-link" href="/posts/61225/">每日资讯</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><div id="widget-waline-list"></div><script type="text/javascript" id="recent-comment" serverURL="https://comment.aimak.cn" count="5" src="/js/recent-comments.js?v=1.0.0" async="async"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://landscape.cncf.io/" title="CNCF" target="_blank">CNCF</a><ul></ul><a href="https://segmentfault.com/" title="SegmentFault" target="_blank">SegmentFault</a><ul></ul><a href="https://blog.csdn.net/junbaozi/category_11649936_3.html" title="CloudNative" target="_blank">CloudNative</a><ul></ul><a href="https://excalidraw.com/" title="excalidraw" target="_blank">excalidraw</a><ul></ul><a href="https://tudan.blog.csdn.net/?type=lately" title="王坦" target="_blank">王坦</a><ul></ul><a href="https://aspoem.com/zh-Hans" target="_blank"></a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">枫哲's文栖小筑.</a><a rel="nofollow" target="_blank" href="https://beian.miit.gov.cn/">｜苏ICP备18013756号-1</a><!--a(rel='nofollow', target='_blank', href='https://github.com/tufu9441/maupassant-hexo')  Theme--><!--|  by--><!--a(rel='nofollow', target='_blank', href='https://github.com/pagecho')  Cho.--></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>