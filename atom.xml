<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>枫哲&#39;s文栖小筑</title>
  
  <subtitle>君子终日乾乾，夕惕若厉，无咎</subtitle>
  <link href="http://ai.mak.cn/atom.xml" rel="self"/>
  
  <link href="http://ai.mak.cn/"/>
  <updated>2025-02-17T09:22:21.616Z</updated>
  <id>http://ai.mak.cn/</id>
  
  <author>
    <name>fantasykai</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>每日资讯</title>
    <link href="http://ai.mak.cn/2025/02/17/ai/%E6%AF%8F%E6%97%A5%E8%B5%84%E8%AE%AF/"/>
    <id>http://ai.mak.cn/2025/02/17/ai/%E6%AF%8F%E6%97%A5%E8%B5%84%E8%AE%AF/</id>
    <published>2025-02-16T16:00:00.000Z</published>
    <updated>2025-02-17T09:22:21.616Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>2025年，人工智能领域继续快速发展，大模型、AI Agent、生成式AI等技术不断突破，应用场景持续扩展。本文将整理近期国内外AI领域的最新资讯，并给出精准点评和深入解读，帮助读者把握AI技术的最新趋势。</p><hr><h3 id="1-大模型（LLM）的持续进化"><a href="#1-大模型（LLM）的持续进化" class="headerlink" title="1. 大模型（LLM）的持续进化"></a>1. 大模型（LLM）的持续进化</h3><h4 id="资讯："><a href="#资讯：" class="headerlink" title="资讯："></a>资讯：</h4><ul><li><strong>OpenAI发布GPT-5</strong>：GPT-5在理解力、推理能力和多模态处理上进一步提升，尤其在医疗、法律等专业领域的表现接近人类专家。</li><li><strong>谷歌推出Gemini Ultra 2.0</strong>：Gemini Ultra 2.0在复杂任务处理和多轮对话中展现了更强的稳定性。</li></ul><h4 id="点评："><a href="#点评：" class="headerlink" title="点评："></a>点评：</h4><p>大模型的持续进化标志着AI在专业领域的应用潜力。然而，模型规模的扩大也带来了更高的计算成本和能源消耗。</p><h4 id="建议："><a href="#建议：" class="headerlink" title="建议："></a>建议：</h4><p>企业应关注大模型在垂直领域的应用，同时探索模型压缩和优化技术，以降低部署成本。</p><hr><h3 id="2-AI-Agent的崛起"><a href="#2-AI-Agent的崛起" class="headerlink" title="2. AI Agent的崛起"></a>2. AI Agent的崛起</h3><h4 id="资讯：-1"><a href="#资讯：-1" class="headerlink" title="资讯："></a>资讯：</h4><ul><li><strong>微软推出Copilot Pro 2.0</strong>：Copilot Pro 2.0能够自主完成复杂任务，如代码生成、文档撰写和数据分析。</li><li><strong>Anthropic发布Claude Assistant 2.0</strong>：Claude Assistant 2.0在任务规划和工具使用上表现出色，成为企业自动化的新选择。</li></ul><h4 id="点评：-1"><a href="#点评：-1" class="headerlink" title="点评："></a>点评：</h4><p>AI Agent从”被动回答问题”到”主动完成任务”的跨越，标志着AI应用进入新阶段。</p><h4 id="建议：-1"><a href="#建议：-1" class="headerlink" title="建议："></a>建议：</h4><p>企业可以尝试将AI Agent集成到工作流程中，提升效率，但需注意数据安全和隐私保护。</p><hr><h3 id="3-生成式AI的突破"><a href="#3-生成式AI的突破" class="headerlink" title="3. 生成式AI的突破"></a>3. 生成式AI的突破</h3><h4 id="资讯：-2"><a href="#资讯：-2" class="headerlink" title="资讯："></a>资讯：</h4><ul><li><strong>Stability AI发布Stable Video 2.0</strong>：Stable Video 2.0支持从文本生成高质量视频，为内容创作带来新可能。</li><li><strong>MidJourney V7上线</strong>：V7版本在图像生成的真实性和细节上大幅提升。</li></ul><h4 id="点评：-2"><a href="#点评：-2" class="headerlink" title="点评："></a>点评：</h4><p>生成式AI在创意领域的应用不断扩展，但版权和伦理问题仍需关注。</p><h4 id="建议：-2"><a href="#建议：-2" class="headerlink" title="建议："></a>建议：</h4><p>创作者可以利用生成式AI工具提升效率，但需确保内容的原创性和合法性。</p><hr><h3 id="4-国内AI动态"><a href="#4-国内AI动态" class="headerlink" title="4. 国内AI动态"></a>4. 国内AI动态</h3><h4 id="资讯：-3"><a href="#资讯：-3" class="headerlink" title="资讯："></a>资讯：</h4><ul><li><strong>百度发布文心一言5.0</strong>：文心一言5.0在中文理解和多轮对话中表现优异，成为国内大模型的代表。</li><li><strong>阿里云推出通义千问3.0</strong>：通义千问3.0在电商、金融等领域的应用效果显著。</li></ul><h4 id="点评：-3"><a href="#点评：-3" class="headerlink" title="点评："></a>点评：</h4><p>国内大模型在中文场景下的表现逐渐赶超国际水平，但生态建设和应用落地仍需加强。</p><h4 id="建议：-3"><a href="#建议：-3" class="headerlink" title="建议："></a>建议：</h4><p>国内企业应加大研发投入，同时推动大模型在更多行业的应用落地。</p><hr><h3 id="5-AI伦理与法规"><a href="#5-AI伦理与法规" class="headerlink" title="5. AI伦理与法规"></a>5. AI伦理与法规</h3><h4 id="资讯：-4"><a href="#资讯：-4" class="headerlink" title="资讯："></a>资讯：</h4><ul><li><strong>欧盟通过《AI法案》</strong>：该法案对高风险AI应用进行了严格限制，要求透明性和可解释性。</li><li><strong>中国发布《生成式AI管理办法》</strong>：明确生成式AI的内容安全责任，要求企业加强内容审核。</li></ul><h4 id="点评：-4"><a href="#点评：-4" class="headerlink" title="点评："></a>点评：</h4><p>AI伦理和法规的完善是技术健康发展的重要保障。</p><h4 id="建议：-4"><a href="#建议：-4" class="headerlink" title="建议："></a>建议：</h4><p>企业应关注相关法规，确保AI应用的合规性，同时积极参与行业标准的制定。</p><hr><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>2025年，AI技术将继续快速发展，大模型、AI Agent和生成式AI等领域将迎来更多突破。企业应抓住机遇，探索AI在垂直领域的应用，同时关注伦理和法规，确保技术的负责任发展。</p><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://openai.com/">OpenAI GPT-5 发布公告</a></li><li><a href="https://www.microsoft.com/">微软 Copilot Pro 2.0 介绍</a></li><li><a href="https://ec.europa.eu/">欧盟《AI法案》全文</a></li><li><a href="https://www.gov.cn/">中国《生成式AI管理办法》</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;2025年，人工智能领域继续快速发展，大模型、AI Agent、生成式AI等技术不断突破，应用场景持续扩展。本文将整理近期国内外AI领域的最</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="人工智能, 技术趋势, 行业动态" scheme="http://ai.mak.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF-%E8%A1%8C%E4%B8%9A%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>Deepseek 技术分析总结</title>
    <link href="http://ai.mak.cn/2025/02/15/ai/Deepseek%20%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93/"/>
    <id>http://ai.mak.cn/2025/02/15/ai/Deepseek%20%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90%E6%80%BB%E7%BB%93/</id>
    <published>2025-02-14T16:00:00.000Z</published>
    <updated>2025-02-17T09:12:46.163Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>Deepseek 作为一种先进的 AI 技术，在数据处理、模式识别和决策优化方面展现了强大的能力。本文将深入分析 Deepseek 的技术特点，并探讨其在运营商网络运营和运维场景中的应用，为自智能力建设提供新思路。</p><hr><h3 id="1-Deepseek-技术分析"><a href="#1-Deepseek-技术分析" class="headerlink" title="1. Deepseek 技术分析"></a>1. Deepseek 技术分析</h3><h4 id="1-1-核心能力"><a href="#1-1-核心能力" class="headerlink" title="1.1 核心能力"></a>1.1 核心能力</h4><ul><li><strong>高效数据处理</strong>：Deepseek 能够快速处理海量数据，提取关键信息。</li><li><strong>智能模式识别</strong>：通过深度学习算法，Deepseek 可以识别复杂的数据模式，预测潜在问题。</li><li><strong>动态决策优化</strong>：基于实时数据，Deepseek 能够提供最优决策建议，提升运营效率。</li></ul><h4 id="1-2-技术优势"><a href="#1-2-技术优势" class="headerlink" title="1.2 技术优势"></a>1.2 技术优势</h4><ul><li><strong>实时性</strong>：Deepseek 支持实时数据处理和决策，适用于高动态的网络环境。</li><li><strong>可扩展性</strong>：Deepseek 的架构设计支持横向扩展，能够应对不断增长的数据量和计算需求。</li><li><strong>自适应性</strong>：Deepseek 能够根据环境变化自动调整模型参数，保持高效运行。</li></ul><hr><h3 id="2-运营商网络运营与运维的挑战"><a href="#2-运营商网络运营与运维的挑战" class="headerlink" title="2. 运营商网络运营与运维的挑战"></a>2. 运营商网络运营与运维的挑战</h3><h4 id="2-1-数据复杂性"><a href="#2-1-数据复杂性" class="headerlink" title="2.1 数据复杂性"></a>2.1 数据复杂性</h4><p>运营商网络产生的数据量巨大，且类型多样，传统方法难以高效处理。</p><h4 id="2-2-故障预测与诊断"><a href="#2-2-故障预测与诊断" class="headerlink" title="2.2 故障预测与诊断"></a>2.2 故障预测与诊断</h4><p>网络故障的预测和诊断需要高精度的模型和实时分析能力。</p><h4 id="2-3-资源优化"><a href="#2-3-资源优化" class="headerlink" title="2.3 资源优化"></a>2.3 资源优化</h4><p>网络资源的动态分配和优化是提升运营效率的关键。</p><hr><h3 id="3-Deepseek-在运营商网络中的应用"><a href="#3-Deepseek-在运营商网络中的应用" class="headerlink" title="3. Deepseek 在运营商网络中的应用"></a>3. Deepseek 在运营商网络中的应用</h3><h4 id="3-1-智能故障预测"><a href="#3-1-智能故障预测" class="headerlink" title="3.1 智能故障预测"></a>3.1 智能故障预测</h4><ul><li><strong>应用场景</strong>：利用 Deepseek 分析网络流量数据，预测潜在故障。</li><li><strong>具体方案</strong>：<ol><li><strong>数据采集</strong>：采集网络流量、设备状态、日志等数据。</li><li><strong>特征提取</strong>：使用 Deepseek 提取关键特征，如流量异常、设备负载、错误日志等。</li><li><strong>模型训练</strong>：基于历史故障数据，训练故障预测模型。</li><li><strong>实时监控</strong>：部署模型到生产环境，实时监控网络状态。</li><li><strong>预警机制</strong>：当模型检测到潜在故障时，自动触发预警并通知运维人员。</li></ol></li><li><strong>效果</strong>：提前预警，减少故障发生率和影响范围。</li></ul><h4 id="3-2-自动化运维"><a href="#3-2-自动化运维" class="headerlink" title="3.2 自动化运维"></a>3.2 自动化运维</h4><ul><li><strong>应用场景</strong>：通过 Deepseek 实现网络设备的自动化监控和维护。</li><li><strong>具体方案</strong>：<ol><li><strong>设备监控</strong>：利用 Deepseek 实时监控设备状态，如 CPU 使用率、内存占用、端口状态等。</li><li><strong>异常检测</strong>：通过深度学习算法识别设备异常行为，如性能下降、配置错误等。</li><li><strong>自动化修复</strong>：针对常见问题，制定自动化修复脚本，如重启设备、调整配置等。</li><li><strong>知识库构建</strong>：将运维经验转化为知识库，供 Deepseek 学习和参考。</li><li><strong>人机协同</strong>：对于复杂问题，Deepseek 提供建议，由运维人员决策执行。</li></ol></li><li><strong>效果</strong>：降低运维成本，提高运维效率。</li></ul><h4 id="3-3-动态资源优化"><a href="#3-3-动态资源优化" class="headerlink" title="3.3 动态资源优化"></a>3.3 动态资源优化</h4><ul><li><strong>应用场景</strong>：基于 Deepseek 的实时数据分析，动态调整网络资源分配。</li><li><strong>具体方案</strong>：<ol><li><strong>数据采集</strong>：采集网络流量、用户行为、服务质量等数据。</li><li><strong>需求预测</strong>：使用 Deepseek 预测网络资源需求，如带宽、计算资源等。</li><li><strong>资源分配</strong>：根据预测结果，动态调整资源分配策略，如负载均衡、流量调度等。</li><li><strong>性能优化</strong>：通过深度学习算法优化网络性能，如减少延迟、提高吞吐量等。</li><li><strong>效果评估</strong>：实时评估资源优化效果，持续改进模型。</li></ol></li><li><strong>效果</strong>：提升网络资源利用率，优化用户体验。</li></ul><h4 id="3-4-智能客户服务"><a href="#3-4-智能客户服务" class="headerlink" title="3.4 智能客户服务"></a>3.4 智能客户服务</h4><ul><li><strong>应用场景</strong>：利用 Deepseek 提供智能化的客户服务。</li><li><strong>具体方案</strong>：<ol><li><strong>智能问答</strong>：通过 Deepseek 构建智能问答系统，解答用户常见问题。</li><li><strong>故障诊断</strong>：用户反馈问题时，Deepseek 自动分析并定位故障原因。</li><li><strong>个性化推荐</strong>：根据用户需求，推荐合适的套餐或服务。</li><li><strong>情感分析</strong>：通过自然语言处理技术，分析用户情感，提供更人性化的服务。</li><li><strong>数据反馈</strong>：将用户反馈数据用于优化网络和服务。</li></ol></li><li><strong>效果</strong>：提升客户满意度，降低客服成本。</li></ul><h4 id="3-5-网络安全防护"><a href="#3-5-网络安全防护" class="headerlink" title="3.5 网络安全防护"></a>3.5 网络安全防护</h4><ul><li><strong>应用场景</strong>：利用 Deepseek 增强网络安全性。</li><li><strong>具体方案</strong>：<ol><li><strong>威胁检测</strong>：通过 Deepseek 分析网络流量，识别潜在威胁，如 DDoS 攻击、恶意软件等。</li><li><strong>行为分析</strong>：使用深度学习算法分析用户行为，检测异常活动。</li><li><strong>自动响应</strong>：当检测到威胁时，自动触发防护措施，如阻断流量、隔离设备等。</li><li><strong>安全预警</strong>：提前预警潜在安全风险，提供防护建议。</li><li><strong>持续学习</strong>：基于新威胁数据，持续优化安全模型。</li></ol></li><li><strong>效果</strong>：提升网络安全性，减少安全事件发生。</li></ul><h4 id="3-6-智能体构建与意图识别"><a href="#3-6-智能体构建与意图识别" class="headerlink" title="3.6 智能体构建与意图识别"></a>3.6 智能体构建与意图识别</h4><ul><li><strong>应用场景</strong>：通过 Deepseek 构建智能体，实现自然语言交互和任务自动化。</li><li><strong>具体方案</strong>：<ol><li><strong>智能体架构设计</strong>：<ul><li><strong>对话管理</strong>：设计对话流程，支持多轮交互。</li><li><strong>知识库集成</strong>：集成网络运维知识库，提供准确信息。</li><li><strong>API 集成</strong>：与网络管理系统、运维工具等 API 集成，实现任务自动化。</li></ul></li><li><strong>意图识别</strong>：<ul><li><strong>意图定义</strong>：明确用户可能的需求，如”查询网络状态”、”报告故障”、”优化资源”等。</li><li><strong>触发词设计</strong>：为每个意图设计触发词，如”网络状态”、”故障报告”、”资源优化”等。</li><li><strong>槽位填充</strong>：为每个意图设计槽位，如”设备名称”、”故障类型”、”优化目标”等。</li><li><strong>模型训练</strong>：基于历史对话数据，训练意图识别模型。</li></ul></li><li><strong>对话流程设计</strong>：<ul><li><strong>用户输入</strong>：用户通过自然语言表达需求。</li><li><strong>意图识别</strong>：Deepseek 识别用户意图并提取关键信息。</li><li><strong>任务执行</strong>：根据意图调用相应 API 或知识库，执行任务。</li><li><strong>结果反馈</strong>：将任务结果以自然语言形式反馈给用户。</li></ul></li><li><strong>持续优化</strong>：<ul><li><strong>数据收集</strong>：收集用户对话数据，用于模型优化。</li><li><strong>模型更新</strong>：定期更新意图识别模型，提高准确率。</li><li><strong>用户反馈</strong>：通过用户反馈改进对话流程和任务执行。</li></ul></li></ol></li><li><strong>效果</strong>：提升运维效率，改善用户体验。</li></ul><hr><h3 id="4-相关能力建设的思考"><a href="#4-相关能力建设的思考" class="headerlink" title="4. 相关能力建设的思考"></a>4. 相关能力建设的思考</h3><h4 id="4-1-方案落地的困难点"><a href="#4-1-方案落地的困难点" class="headerlink" title="4.1 方案落地的困难点"></a>4.1 方案落地的困难点</h4><ol><li><p><strong>数据质量与整合</strong>：</p><ul><li><strong>问题</strong>：运营商网络数据来源多样，数据质量参差不齐，整合难度大。</li><li><strong>建议</strong>：建立统一的数据治理平台，制定数据标准和清洗流程。</li></ul></li><li><p><strong>模型训练与优化</strong>：</p><ul><li><strong>问题</strong>：网络环境复杂，模型训练需要大量高质量数据，且模型优化周期长。</li><li><strong>建议</strong>：引入迁移学习和联邦学习技术，减少数据依赖，加速模型优化。</li></ul></li><li><p><strong>系统集成与兼容性</strong>：</p><ul><li><strong>问题</strong>：现有网络管理系统和运维工具与 Deepseek 的集成存在技术障碍。</li><li><strong>建议</strong>：采用微服务架构和标准化 API，提高系统兼容性。</li></ul></li><li><p><strong>人才与组织能力</strong>：</p><ul><li><strong>问题</strong>：缺乏既懂 AI 又懂网络运维的复合型人才，组织能力不足。</li><li><strong>建议</strong>：加强内部培训，引入外部专家，构建跨部门协作机制。</li></ul></li></ol><h4 id="4-2-能力建设的待突破点"><a href="#4-2-能力建设的待突破点" class="headerlink" title="4.2 能力建设的待突破点"></a>4.2 能力建设的待突破点</h4><ol><li><p><strong>实时性与稳定性</strong>：</p><ul><li><strong>问题</strong>：网络环境高动态，要求 Deepseek 具备实时处理能力和高稳定性。</li><li><strong>突破点</strong>：优化算法和架构，提升实时性和容错能力。</li></ul></li><li><p><strong>可解释性与可信度</strong>：</p><ul><li><strong>问题</strong>：Deepseek 的决策过程缺乏透明性，影响用户信任。</li><li><strong>突破点</strong>：引入可解释 AI 技术，提高模型的可解释性和可信度。</li></ul></li><li><p><strong>安全与隐私保护</strong>：</p><ul><li><strong>问题</strong>：网络数据涉及用户隐私和商业机密，安全风险高。</li><li><strong>突破点</strong>：采用加密计算和隐私保护技术，确保数据安全。</li></ul></li><li><p><strong>生态建设与标准化</strong>：</p><ul><li><strong>问题</strong>：AI 技术在网络运维中的应用缺乏统一标准和生态支持。</li><li><strong>突破点</strong>：推动行业标准制定，构建开放协同的生态体系。</li></ul></li></ol><h4 id="4-3-值得深度思考的点"><a href="#4-3-值得深度思考的点" class="headerlink" title="4.3 值得深度思考的点"></a>4.3 值得深度思考的点</h4><ol><li><p><strong>人机协同的边界</strong>：</p><ul><li><strong>思考</strong>：在自动化运维中，如何界定人机协同的边界，确保人类的主导作用？</li><li><strong>方向</strong>：探索人机协同的最佳实践，制定明确的角色分工和决策机制。</li></ul></li><li><p><strong>技术伦理与社会责任</strong>：</p><ul><li><strong>思考</strong>：AI 技术在网络运维中的应用可能带来哪些伦理和社会问题？</li><li><strong>方向</strong>：建立技术伦理框架，确保 AI 应用的负责任发展。</li></ul></li><li><p><strong>长期价值与短期收益</strong>：</p><ul><li><strong>思考</strong>：在能力建设中，如何平衡长期价值与短期收益？</li><li><strong>方向</strong>：制定分阶段实施计划，确保短期收益的同时，为长期发展奠定基础。</li></ul></li><li><p><strong>创新与风险控制</strong>：</p><ul><li><strong>思考</strong>：在推动技术创新的同时，如何有效控制风险？</li><li><strong>方向</strong>：建立风险管理机制，确保创新过程中的风险可控。</li></ul></li></ol><hr><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>Deepseek 为运营商网络运营和运维带来了革命性的技术手段，但在方案落地和能力建设中仍面临诸多挑战。通过解决数据质量、模型优化、系统集成和人才建设等问题，突破实时性、可解释性、安全性和生态建设等瓶颈，运营商可以构建高效、智能的自智网络。同时，深入思考人机协同、技术伦理、长期价值和创新风险等问题，将有助于推动 AI 技术在网络运维中的可持续发展。</p><p>未来，随着技术的不断进步和生态的逐步完善，Deepseek 将在运营商网络中发挥更大的价值，助力实现网络运营的智能化、自动化和高效化。运营商应抓住这一机遇，积极布局 AI 技术，构建开放协同的生态体系，推动自智网络的全面发展，为用户提供更优质的网络服务。</p><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://www.deepseek.com/">Deepseek 技术白皮书</a></li><li><a href="https://www.itu.int/">自智网络架构指南</a></li><li><a href="https://www.cisco.com/">运营商智能运维案例</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;Deepseek 作为一种先进的 AI 技术，在数据处理、模式识别和决策优化方面展现了强大的能力。本文将深入分析 Deepseek 的技术特</summary>
      
    
    
    
    <category term="AI, 网络运维" scheme="http://ai.mak.cn/categories/AI-%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="Deepseek, 自智网络, 运营商, 网络运维" scheme="http://ai.mak.cn/tags/Deepseek-%E8%87%AA%E6%99%BA%E7%BD%91%E7%BB%9C-%E8%BF%90%E8%90%A5%E5%95%86-%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>3月份马拉松计划</title>
    <link href="http://ai.mak.cn/2025/01/30/%E8%B7%91%E6%AD%A5/3%E6%9C%88%E4%BB%BD%E9%A9%AC%E6%8B%89%E6%9D%BE%E8%AE%A1%E5%88%92/"/>
    <id>http://ai.mak.cn/2025/01/30/%E8%B7%91%E6%AD%A5/3%E6%9C%88%E4%BB%BD%E9%A9%AC%E6%8B%89%E6%9D%BE%E8%AE%A1%E5%88%92/</id>
    <published>2025-01-29T16:00:00.000Z</published>
    <updated>2025-02-17T09:24:22.068Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>2024年4月，我开始了跑步之旅，经过8个月的训练，总跑量达到了1700公里。12月，成功突破了300公里的月跑量，这一年，在11月的高淳马拉松中创造了1小时35分15秒的半马PB。然而，1月的止点跟腱炎让我不得不停跑一个月。3月新的赛季开始，想要计划在3月和4月的全马中突破3小时40分钟的大关，请deepseek训练计划和评估。</p><h3 id="当前状态评估"><a href="#当前状态评估" class="headerlink" title="当前状态评估"></a>当前状态评估</h3><ul><li><strong>身高&#x2F;体重</strong>：172cm &#x2F; 69kg</li><li><strong>跑量</strong>：2024年4-12月总跑量1700公里，12月跑量突破300公里</li><li><strong>半马PB</strong>：1小时35分15秒（11月3日高淳马拉松）</li><li><strong>LSD</strong>：12月完成2次3公里以上LSD（30公里和35公里）</li><li><strong>伤病</strong>：1月因止点跟腱炎停跑一个月，2月恢复跑步</li></ul><h3 id="全马目标评估"><a href="#全马目标评估" class="headerlink" title="全马目标评估"></a>全马目标评估</h3><p>根据我的半马PB（1小时35分15秒），使用跑力公式推算，全马目标时间约为3小时20分钟至3小时30分钟。然而，考虑到1月的伤病和停跑，3小时40分钟是一个合理且具有挑战性的目标。</p><h3 id="训练计划"><a href="#训练计划" class="headerlink" title="训练计划"></a>训练计划</h3><p>为了在3月和4月的全马中突破3小时40分钟，我制定了以下训练计划：</p><h4 id="1-基础期（2月）"><a href="#1-基础期（2月）" class="headerlink" title="1. 基础期（2月）"></a>1. 基础期（2月）</h4><ul><li><strong>目标</strong>：恢复体能，避免伤病</li><li><strong>周跑量</strong>：40-50公里</li><li><strong>训练内容</strong>：<ul><li>轻松跑（E pace）：80%的跑量</li><li>力量训练：每周2次，重点加强下肢和核心力量</li><li>拉伸和按摩：每天进行，预防伤病</li></ul></li></ul><h4 id="2-提升期（3月）"><a href="#2-提升期（3月）" class="headerlink" title="2. 提升期（3月）"></a>2. 提升期（3月）</h4><ul><li><strong>目标</strong>：提高有氧能力和速度</li><li><strong>周跑量</strong>：60-70公里</li><li><strong>训练内容</strong>：<ul><li>轻松跑（E pace）：60%的跑量</li><li>节奏跑（T pace）：每周1次，6-8公里</li><li>间歇跑（I pace）：每周1次，5x1公里</li><li>LSD：每周1次，25-30公里</li></ul></li></ul><h4 id="3-巅峰期（4月）"><a href="#3-巅峰期（4月）" class="headerlink" title="3. 巅峰期（4月）"></a>3. 巅峰期（4月）</h4><ul><li><strong>目标</strong>：调整状态，准备比赛</li><li><strong>周跑量</strong>：50-60公里</li><li><strong>训练内容</strong>：<ul><li>轻松跑（E pace）：70%的跑量</li><li>节奏跑（T pace）：每周1次，5-6公里</li><li>LSD：每周1次，20-25公里</li><li>减量：比赛前2周逐渐减少跑量，保持轻松跑和短距离节奏跑</li></ul></li></ul><h3 id="力量训练计划"><a href="#力量训练计划" class="headerlink" title="力量训练计划"></a>力量训练计划</h3><p>力量训练是预防伤病和提高跑步效率的关键。以下是我的力量训练计划：</p><h4 id="1-下肢力量训练"><a href="#1-下肢力量训练" class="headerlink" title="1. 下肢力量训练"></a>1. 下肢力量训练</h4><ul><li><strong>深蹲</strong>：3组，每组12次</li><li><strong>弓步蹲</strong>：3组，每组12次（每条腿）</li><li><strong>单腿硬拉</strong>：3组，每组12次（每条腿）</li><li><strong>小腿提踵</strong>：3组，每组15次</li></ul><h4 id="2-核心力量训练"><a href="#2-核心力量训练" class="headerlink" title="2. 核心力量训练"></a>2. 核心力量训练</h4><ul><li><strong>平板支撑</strong>：3组，每组保持60秒</li><li><strong>俄罗斯转体</strong>：3组，每组20次</li><li><strong>仰卧卷腹</strong>：3组，每组15次</li><li><strong>侧桥</strong>：3组，每组保持45秒（每侧）</li></ul><h4 id="3-灵活性训练"><a href="#3-灵活性训练" class="headerlink" title="3. 灵活性训练"></a>3. 灵活性训练</h4><ul><li><strong>动态拉伸</strong>：每次训练前进行10分钟</li><li><strong>静态拉伸</strong>：每次训练后进行15分钟</li><li><strong>泡沫轴放松</strong>：每周3次，重点放松大腿、小腿和臀部</li></ul><h3 id="比赛策略"><a href="#比赛策略" class="headerlink" title="比赛策略"></a>比赛策略</h3><ul><li><strong>配速</strong>：目标配速为5分12秒&#x2F;公里，前半程保持5分15秒&#x2F;公里，后半程根据状态调整</li><li><strong>补给</strong>：每5公里补充一次能量胶，每10公里补充一次电解质饮料</li><li><strong>心理</strong>：保持积极心态，合理分配体力，避免前半程过快</li></ul><p>通过科学的训练和合理的比赛策略，我有信心在3月和4月的全马中突破3小时40分钟的大关，希望我的经验能够帮助到同样在追求全马目标的跑友们。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><a href="https://www.runningquotient.com/">跑力公式计算器</a></li><li><a href="https://www.halhigdon.com/">马拉松训练计划</a></li><li><a href="https://www.runnersworld.com/">跑步伤病预防</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;2024年4月，我开始了跑步之旅，经过8个月的训练，总跑量达到了1700公里。12月，成功突破了300公里的月跑量，这一年，在11月的高淳马</summary>
      
    
    
    
    <category term="跑步" scheme="http://ai.mak.cn/categories/%E8%B7%91%E6%AD%A5/"/>
    
    
    <category term="马拉松" scheme="http://ai.mak.cn/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/"/>
    
  </entry>
  
  <entry>
    <title>2024年人工智能进展及展望</title>
    <link href="http://ai.mak.cn/2024/12/31/ai/2024%E5%B9%B4%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%BF%9B%E5%B1%95%E4%B8%8E2025%E5%B9%B4%E5%B1%95%E6%9C%9B/"/>
    <id>http://ai.mak.cn/2024/12/31/ai/2024%E5%B9%B4%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%BF%9B%E5%B1%95%E4%B8%8E2025%E5%B9%B4%E5%B1%95%E6%9C%9B/</id>
    <published>2024-12-30T16:00:00.000Z</published>
    <updated>2025-02-11T07:46:37.048Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2024年人工智能进展及展望"><a href="#2024年人工智能进展及展望" class="headerlink" title="2024年人工智能进展及展望"></a>2024年人工智能进展及展望</h1><p>2024年是人工智能领域快速发展的一年，大模型、AI Agent、云原生AI等技术取得了显著突破。本文将从技术进展、行业应用和未来展望三个方面，回顾2024年AI领域的重要发展，并展望2025年的趋势。</p><h2 id="一、2024年人工智能技术进展"><a href="#一、2024年人工智能技术进展" class="headerlink" title="一、2024年人工智能技术进展"></a>一、2024年人工智能技术进展</h2><h3 id="1-大模型（LLM）的持续进化"><a href="#1-大模型（LLM）的持续进化" class="headerlink" title="1. 大模型（LLM）的持续进化"></a>1. 大模型（LLM）的持续进化</h3><p>2024年，大语言模型（LLM）在规模、能力和应用场景上都有了显著提升。GPT-4、Claude 3等模型在理解力、推理能力和多模态处理上取得了突破，尤其是在医疗、法律等专业领域的表现令人瞩目。 </p><h3 id="2-AI-Agent的崛起"><a href="#2-AI-Agent的崛起" class="headerlink" title="2. AI Agent的崛起"></a>2. AI Agent的崛起</h3><p>AI Agent成为2024年的热门话题，它通过结合大模型、任务规划和工具使用，实现了从”被动回答问题”到”主动完成任务”的跨越。AI Agent在个人助理、企业自动化等场景中展现了巨大潜力。</p><h3 id="3-云原生AI的普及"><a href="#3-云原生AI的普及" class="headerlink" title="3. 云原生AI的普及"></a>3. 云原生AI的普及</h3><p>云原生AI技术（CNAI）在2024年得到了广泛应用。通过容器化、微服务和Kubernetes等技术，AI应用的开发、部署和运维变得更加高效和灵活。Hugging Face、OpenAI等公司在这一领域取得了显著进展。</p><figure class="highlight plaintext"><figcaption><span>CNCF 推出的云原生 AI 白皮书.md</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startLine: 1</span><br><span class="line">endLine: 30</span><br></pre></td></tr></table></figure><h2 id="二、2024年人工智能行业应用"><a href="#二、2024年人工智能行业应用" class="headerlink" title="二、2024年人工智能行业应用"></a>二、2024年人工智能行业应用</h2><h3 id="1-医疗领域"><a href="#1-医疗领域" class="headerlink" title="1. 医疗领域"></a>1. 医疗领域</h3><p>AI在医疗领域的应用取得了突破性进展。例如，谷歌的Med-PaLM模型在医师职业测试中达到了与人类医生相当的水平，为AI问诊和辅助诊断提供了新的可能性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startLine: 52</span><br><span class="line">endLine: 56</span><br></pre></td></tr></table></figure><h3 id="2-音乐与创意产业"><a href="#2-音乐与创意产业" class="headerlink" title="2. 音乐与创意产业"></a>2. 音乐与创意产业</h3><p>AI在音乐创作和艺术生成领域的应用也取得了显著进展。例如，Riffusion工具通过微调AI画图算法，实现了根据文本生成音乐的功能，展示了AI在创意领域的潜力。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startLine: 44</span><br><span class="line">endLine: 50</span><br></pre></td></tr></table></figure><h3 id="3-企业数字化"><a href="#3-企业数字化" class="headerlink" title="3. 企业数字化"></a>3. 企业数字化</h3><p>AI在企业数字化中的应用进一步深化，尤其是在数据治理、智能决策和自动化流程方面。MaaS（Model as a Service）模式成为企业利用AI技术的重要方式。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startLine: 1</span><br><span class="line">endLine: 10</span><br></pre></td></tr></table></figure><h2 id="三、2025年人工智能展望"><a href="#三、2025年人工智能展望" class="headerlink" title="三、2025年人工智能展望"></a>三、2025年人工智能展望</h2><h3 id="1-更强的自主性与智能化"><a href="#1-更强的自主性与智能化" class="headerlink" title="1. 更强的自主性与智能化"></a>1. 更强的自主性与智能化</h3><p>2025年，AI Agent将进一步提升自主性和智能化水平，能够在更多复杂场景中自主决策和执行任务。随着技术的进步，AI将更加贴近人类的行为模式。</p><h3 id="2-深度行业化与定制化"><a href="#2-深度行业化与定制化" class="headerlink" title="2. 深度行业化与定制化"></a>2. 深度行业化与定制化</h3><p>AI技术将进一步渗透到各个行业，尤其是在医疗、金融、制造等领域，定制化的AI解决方案将成为主流。行业化的AI模型和应用将更加成熟。</p><h3 id="3-伦理与法规的完善"><a href="#3-伦理与法规的完善" class="headerlink" title="3. 伦理与法规的完善"></a>3. 伦理与法规的完善</h3><p>随着AI技术的普及，隐私、安全和伦理问题将受到更多关注。2025年，相关法规和标准将逐步完善，确保AI技术的负责任发展。</p><h3 id="4-持续学习与自适应能力"><a href="#4-持续学习与自适应能力" class="headerlink" title="4. 持续学习与自适应能力"></a>4. 持续学习与自适应能力</h3><p>未来的AI系统将具备更强的持续学习和自适应能力，能够根据环境变化和新数据进行自我调整和优化，进一步提升智能水平。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>2024年，人工智能技术在多个领域取得了显著进展，尤其是在大模型、AI Agent和云原生AI方面。展望2025年，AI将继续向更智能化、行业化和伦理化的方向发展，成为推动社会进步的重要力量。我们期待AI技术在未来带来更多创新和突破，为人类生活和工作带来更多便利。</p><hr><p><strong>参考文献：</strong></p><ol><li><a href="https://example.com/ai-agent">AI Agent简述</a></li><li><a href="https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/">云原生AI白皮书</a></li><li><a href="https://example.com/data-insight">2022年中国数智融合发展洞察</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;2024年人工智能进展及展望&quot;&gt;&lt;a href=&quot;#2024年人工智能进展及展望&quot; class=&quot;headerlink&quot; title=&quot;2024年人工智能进展及展望&quot;&gt;&lt;/a&gt;2024年人工智能进展及展望&lt;/h1&gt;&lt;p&gt;2024年是人工智能领域快速发展的一年，大模</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="人工智能" scheme="http://ai.mak.cn/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>大模型概念总结</title>
    <link href="http://ai.mak.cn/2024/12/06/ai/AI%20Agent%E7%AE%80%E8%BF%B0/"/>
    <id>http://ai.mak.cn/2024/12/06/ai/AI%20Agent%E7%AE%80%E8%BF%B0/</id>
    <published>2024-12-05T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.311Z</updated>
    
    <content type="html"><![CDATA[<p><strong>尽管大部分人是在LLM（大语言模型）兴起后才接触AI Agent，但它并不是一个完全新生的概念（RAG也是一样），只是因为LLM给AI Agent带来了全新的可能与突破。所以本文的</strong> <strong>AI Agent也是特指基于LLM的自主型人工智能实体。</strong></p><p>01</p><p>为什么需要Agent？</p><p>大模型已经非常强大，能够生成内容、回答问题甚至协助编程。那为什么我们还需要 AI Agent？</p><p>简单的说，大模型就像一个“超级大脑”，知识丰富、能力强大，但它的问题是“只懂回答，不懂行动”。你可以让它生成一篇文章、回答一个问题，但如果你希望它主动完成一系列复杂任务，仅靠大模型自身是不够的。 比如，你可以问大模型：</p><p><em><strong>“如何调查与获取竞争对手产品的信息？”</strong></em></p><p>甚至也可以结合RAG让大模型来回答：</p><p><em><strong>“总结我们公司最新某某产品的特点？”</strong></em></p><p>但是如果你让大模型来帮你完成如下任务：</p><p><em><strong>“对比A公司竞品与我公司产品的差异，把结果发送到我邮箱。“</strong></em></p><p>这时候大模型就无能为力了。原因是它只有聪明的”大脑“，但却没有”手脚“、也没有”工具“，因此无法自主的完成任务。所以AI需要这样的进化：</p><p><img src="https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/f2d2298685fb486ebde35a1df8d8435a~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1736404376&x-signature=ZGp+KKOm/CRCSpLlBSuTVYBc1pA=" alt="picture.image"></p><p>这就是为什么需要Agent —— 因为我们需要AI <strong>不 *<em>仅*</em></strong> <strong>是被动的回答问题，更需要能够主动的解决问题。</strong></p><p>02</p><p>什么是Agent？</p><p>所以，AI Agent（智能体） 是一种能利用大模型进行自主的任务规划、决策与执行的系统。 <strong>它的核心思路是让人工智能不仅能回答问题，还能像人一样主动完成一系列关联性的任务；不仅有聪明的“大脑”，还有灵活的“手脚”，必要的时候还会使用“工具”。</strong></p><p><img src="https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/2605f5e64e1e40b0a0b61e3fd897f235~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1736404376&x-signature=WCt4ivDAQtu/YkpUYKjgmcScL6A=" alt="picture.image"></p><p>如果说大模型像一位百科全书式的学者，而AI Agent就像一个“办事能力强的大管家”。这位管家会根据你的需求，把任务拆解成多个步骤，并主动找到资源或工具来完成。比如这个任务：</p><p>“对比A公司与我公司产品的差异，把结果发送到我邮箱。“</p><p>Agent会借助大模型规划任务步骤并执行：</p><ol><li>先从互联网搜索A公司产品信息（使用Web搜索工具）</li><li>再从企业知识库检索我公司信息（使用本地查询工具）</li><li>生成对比报告（让大模型完成）</li><li>发送邮件到邮箱（使用邮件发送工具）</li></ol><p>可以看到，基于大模型的AI Agent，就是把强大的语言模型和一套可以主动行动的机制结合起来，让它不仅能“懂”能“想”，还会“做”。</p><p>所以 <strong>Agent与大模型之间的区别</strong> 可以总结为：</p><ul><li>大模型只是一个大脑，而Agent是一个完整体</li><li>大模型只会告诉你怎么做，而Agent会帮你做</li><li>模型本身不会使用工具，而Agent会使用工具</li><li>大模型通常不会记忆，而Agent则具备记忆能力</li><li>Agent借助大模型来实现理解与规划能力</li></ul><p>03</p><p>Agent的常见应用场景？</p><p>AI Agent可以在大量的领域与场景下展现出非凡的能力，包括但不限于个人助理、客户服务、市场营销、决策支持、游戏仿真、智能家居、无人驾驶、机器人等。以下是一些实际的例子：</p><p><strong>1.智能客服</strong></p><p>比如一家公司需要全天候解答客户问题。AI Agent可以根据客户问题调用大模型生成答案，还能主动查询库存信息、处理订单甚至提供物流状态。</p><p><strong>2.编程助手</strong></p><p>开发人员需要解决某个技术问题，AI Agent不仅能提供代码示例，还能直接运行代码，调试错误，甚至优化性能。</p><p><strong>3.个人助理</strong></p><p>Agent可以帮你管理日程、订餐、处理邮件、监控股票市场，并根据你的偏好提供个性化建议，而不只是回答问题。</p><p><strong>4.智能家居</strong></p><p>家庭中的Agent可以连接灯光、空调、安防摄像头等设备，根据家庭成员的指令与设定，主动调节环境，控制家具设备。</p><p><strong>5.科学研究</strong></p><p>在科研领域，AI Agent可以自动收集最新文献、设计实验流程、分析实验数据，并生成总结报告。</p><p>04</p><p>Agent的基本工作原理？</p><p>AI Agent的工作原理可以总结为以下几个步骤：</p><p><img src="https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/7dd44afc14c747c58b06a9818cfb967f~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1736404376&x-signature=B/I/jLELGQw5pXIknqfUVDvIwbY=" alt="picture.image"></p><p><strong>1.输入理解：</strong> 用户提出一个任务（比如发送一份产品对比报告），Agent首先借助大模型对用户输入指令进行理解和解析，识别任务目标和约束条件。</p><p><strong>2.任务规划：</strong> 基于理解的目标，Agent 会规划完成任务的步骤，并决定采取哪些行动。这可能涉及将目标分解成多个子任务，确定任务优先级与执行顺序等（如获取竞品信息、查询企业产品信息、生成对比报告、发送电子邮件）。</p><p><strong>3.任务执行与反馈：</strong> 通过大模型或外部工具完成每个子任务（如调用搜索引擎、查询数据库、生成对比结果、调用电子邮件发送服务）；在此过程中，Agent会搜集与观察子任务结果，及时处理问题，必要时对任务进行调整（如任务执行发生了错误，可能会进行多次迭代尝试）。</p><p><strong>4.任务完成与交付：</strong> 将任务的结果汇总并输出（如生成对比报告与邮件发送回执）。</p><p>当然，这只是Agent的核心处理流程。在实际应用中，根据环境与需求的差异，可能存在高度定制且复杂Agent工作流。</p><p>05</p><p>Agent系统的基本组成？</p><p>获得广泛认可的Agent架构来自于OpenAI公司的总结：</p><p><img src="https://p6-volc-community-sign.byteimg.com/tos-cn-i-tlddhu82om/2481f33663cc452b847e51789dc6659d~tplv-tlddhu82om-image.image?=&rk3s=8031ce6d&x-expires=1736404376&x-signature=4QZRFK0up8P6l6ERZQtZP47QnGE=" alt="picture.image"></p><p>可以总结成：</p><p><strong>Agent &#x3D; LLM + 记忆 + 规划技能 + 工具使用</strong></p><ol><li><strong>大模型：</strong> 提供核心的语言理解、推理与生成能力，是整个Agent的“大脑”。</li><li><strong>任务规划：</strong> 对复杂任务借助大模型进行分解、规划和调度，并及时观察子任务执行的结果与反馈，对任务及时调整。</li><li><strong>工具使用：</strong> 与外部工具（如API、数据库、硬件设备）进行交互，扩展智能体的能力，执行任务，相当于Agent的“手脚”。</li><li><strong>记忆：</strong> 这是Agent的“存储器”，可用来存储短期的记忆（如一次任务过程中的多次人类交互）或长期记忆（如记录使用者的任务历史、个人信息、兴趣便好等）。</li></ol><p>除此之外，通常Agent还需要提供 一个直观的入口，让用户可以方便地给Agent下达指令或查看结果，这个入口可以是可视化的文字输入、语音输入，或者对外开放的API接口。</p><p>05</p><p>Agent面临最大的挑战？</p><p>尽管LLM的横空出世与爆炸式发展给Agent开辟了新的天地，但必须看到，当前AI Agent仍然处于技术积累与实验阶段，尽管很多大模型厂家推出了Agent平台甚至商店，但主要集中在个人助理、娱乐、写作等对可靠性与确定性相对较低的领域，而在真正的生产力场景，还面临众多挑战。而最主要的问题来自：</p><p>AI Agent将LLM作为核心组件，用于理解用户需求、计划任务、生成响应并执行操作。但在一些对准确性、可预测性、可追溯性要求极高的场景中，LLM的不确定性会带来一些潜在问题。</p><ul><li><strong>错误的任务规划</strong></li></ul><p>LLM误解了用户的意图或语义，可能会导致错误的计划与结果</p><ul><li><strong>错误的工具调用</strong></li></ul><p>如果LLM生成了错误的工具调用逻辑，可能会导致任务失败</p><ul><li><strong>生成错误的建议</strong></li></ul><p>在医疗、法律、金融等需要高度准确的领域，不确定性可能带来严重后果</p><ul><li><strong>安全与伦理问题</strong></li></ul><p>LLM可能会根据不完整的上下文生成带有潜在风险甚至违反伦理的建议</p><ul><li><strong>稳定性和可重复性</strong></li></ul><p>在工业应用中，系统的行为需要可重复且稳定，而LLM由于其生成概率特性，可能在相同条件下输出不同的结果</p><p>在模型厂家、开发工具提供商、应用开发商的共同努力下，这些问题正在不断得到优化与改善。</p><p>06</p><p>Agent未来的发展趋势？</p><p>AI Agent虽然还处于发展的初期，但未来潜力巨大，简单展望其发展趋势：</p><p><strong>1.更强的自主性与智能化</strong></p><p>随着技术进步，AI Agent将拥有更强的人类意图理解、逻辑推理和复杂任务处理能力，能够在更多场景下自主决策，执行多样的任务。</p><p><strong>2.深度行业化与定制化</strong></p><p>更多的领域与行业会定制化自己的AI Agent，比如IT领域的开发助手、医疗领域的诊断助手、智能家居的家庭助手、智能实体机器人等。</p><p><strong>3.更强的个性化，人性化</strong></p><p>AI Agent会具备更强的个性化能力，能够在与使用者的长期交互中学习使用者的习惯、个人信息与兴趣偏好等，以提供更贴心的服务。</p><p><strong>4. 持续学习和自适应能力</strong></p><p>AI Agent将具备持续学习的能力，能够根据环境变化和新数据进行自我调整和优化，持续提升自身的智能水平。</p><p><strong>5. 伦理与法规考量更受重视</strong></p><p>随着AI Agent的普及，对隐私、安全和伦理的关注将促使相关法规和标准的制定，确保AI技术的负责任发展。</p><p>AI Agent的出现，为人工智能技术赋予了主动行动的能力，让它从“被动回答问题”进化到“主动完成任务”。无论是个人生活、企业运营还是科学研究，AI Agent都在逐步展现它的潜力。可以预见，随着技术的不断进步，AI Agent将成为我们工作和生活中不可或缺的助手。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;尽管大部分人是在LLM（大语言模型）兴起后才接触AI Agent，但它并不是一个完全新生的概念（RAG也是一样），只是因为LLM给AI Agent带来了全新的可能与突破。所以本文的&lt;/strong&gt; &lt;strong&gt;AI Agent也是特指基于LLM的自主型</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="深度学习" scheme="http://ai.mak.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>系统稳定性建设</title>
    <link href="http://ai.mak.cn/2024/10/02/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/%E7%B3%BB%E7%BB%9F%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE/"/>
    <id>http://ai.mak.cn/2024/10/02/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/%E7%B3%BB%E7%BB%9F%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE/</id>
    <published>2024-10-01T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.679Z</updated>
    
    <content type="html"><![CDATA[<h1 id="系统稳定性建设"><a href="#系统稳定性建设" class="headerlink" title="系统稳定性建设"></a>系统稳定性建设</h1><blockquote><p>这篇文章以京东为例，从后端研发视角谈系统稳定性建设，包括研发和上线阶段。研发阶段要把控技术方案和代码质量，技术方案需评审，关注限流、熔断降级、超时、重试、兼容、隔离等，还要进行代码 Review。上线阶段是故障高发期，需做到可监控、可灰度、可回滚，应对线上问题要分类、明确生命周期、预防、发现、响应、定位、修复和复盘。</p></blockquote><blockquote><p>作者：京东云开发者<br>链接：<a href="https://mp.weixin.qq.com/s/lPSNDH872Wmy7rkrq_U6_w">https://mp.weixin.qq.com/s/lPSNDH872Wmy7rkrq_U6_w</a></p></blockquote><h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h1><p>京东的期中考试：618即将到来，各个团队都在进行期中考试前的模拟考试：军演压测，故障演练，系统的梳理以检测系统的稳定性以应对高可用，高性能，高并发。我们知道系统的稳定性建设是贯穿整个研发流程：需求阶段，研发阶段，测试阶段，上线阶段，运维阶段；整个流程中的所有参与人员：产品，研发，测试，运维人员都应关注系统的稳定性。业务的发展及系统建设过程中，稳定性就是那个1，其他的是1后面的0，没有稳定性，就好比将万丈高楼建于土沙之上。本篇文章主要从后端研发的视角针对研发阶段和上线阶段谈下稳定性建设，希望起到抛砖引玉的作用，由于本人的水平有限，文中难免有理解不到位或者不全面的地方，欢迎批评指正。</p><h1 id="2-研发阶段"><a href="#2-研发阶段" class="headerlink" title="2. 研发阶段"></a>2. 研发阶段</h1><p>研发阶段主要参与人员是研发，主要产出物是技术方案设计文档和代码，一个是研发阶段的开始，一个是研发阶段的结束，我们要把控好技术文档和代码质量，从而减少线下bug率及线上的故障；</p><h2 id="2-1-技术方案"><a href="#2-1-技术方案" class="headerlink" title="2.1 技术方案"></a>2.1 技术方案</h2><h3 id="2-1-1-技术方案评审"><a href="#2-1-1-技术方案评审" class="headerlink" title="2.1.1 技术方案评审"></a>2.1.1 技术方案评审</h3><p>技术文档的评审需要有本团队的架构师和相关研发，测试，产品，上下游系统的研发同学参与，这样能够最大限度的保证技术方案的实现和产品需求对齐，上下游系统同学也知道我们的实现，采取更加合理的交互方式，测试同学也可以从测试视角给出一些风险点建议，架构师可以确保我们的实现和业界最佳实践的差异，确保合理性，避免过度设计；我们所要做的是开放心态采取大家的意见，严控技术文档的质量；</p><p>技术文档的评审可以采用提问的方式，会议开始前可以将技术文档分享给大家，让大家先阅读10分钟，所有同学开始提问，技术文档设计人其实不用读自己的技术文档给大家介绍，只要将大家的问题回答完，并能够思考下大家的建议，合理的采纳后，其实技术文档的质量就有了很大的保证，有的同学在技术文档评审时，比较反感大家的提问，总感觉在挑战自己，有些问题回答不上来，其实可以换种思路：有些问题回答不上来是正常的，可以先将大家的建议采纳了，会后再思考下合理性；大家对自己技术方案是建言献策，是保证自己技术方案的质量，避免在技术方案阶段就存在重大的线上隐患。</p><h3 id="2-1-2-技术方案关注点"><a href="#2-1-2-技术方案关注点" class="headerlink" title="2.1.2 技术方案关注点"></a>2.1.2 技术方案关注点</h3><p>当我们遇到一个问题的时候，首先要思考的这是一个新问题还是老问题，99.99%遇到的都是老问题，因为我们所从事的是工程技术，不是科学探索；我们所要做的就是看下国内外同行针对这个问题的解法，learn from best practices；所以技术方案的第一步是<strong>对标</strong>，学习最佳实践，这样能让我们避免走弯路；</p><p>同时根据奥卡姆剃刀原理，我们力求技术方案简单，避免过度设计，针对一个复杂的问题，我们的技术方案相对复杂些，简单的问题技术方案相对简单些，我们所要追求的是复杂的问题通过拆解划分，用一个个简单的技术方案解决掉。同时技术文档不仅关注功能的实现，更重要的是关注架构，性能，质量，安全；即如何打造一个高可用系统。打造一个高可用的系统是进行系统稳定性建设的前提，如果我们的系统都不能保证高可用，又谈何系统稳定系建设那，下面介绍下进行系统稳定性建设我们在技术方案中常用的方法及关注点。</p><h4 id="2-1-2-1-限流"><a href="#2-1-2-1-限流" class="headerlink" title="2.1.2.1 限流"></a>2.1.2.1 限流</h4><p>限流一般是从服务提供者provider的视角提供的针对自我保护的能力，对于流量负载超过我们系统的处理能力，限流策略可以防止我们的系统被激增的流量打垮。京东内部无论是同步交互的JSF, 还是异步交互的JMQ都提供了限流的能力，大家可以根据自己系统的情况进行设置；我们知道常见的限流算法包括：计数器算法，滑动时间窗口算法，漏斗算法，令牌桶算法，具体算法可以网上google下，下面是这些算法的优缺点对比。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cba4342107a24b2a8f579efcdc78cede~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1752&h=704&s=173199&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h4 id="2-1-2-2-熔断降级"><a href="#2-1-2-2-熔断降级" class="headerlink" title="2.1.2.2 熔断降级"></a>2.1.2.2 熔断降级</h4><p>熔断和降级是两件事情，但是他们一般是结合在一起使用的。熔断是防止我们的系统被下游系统拖垮，比如下游系统接口性能严重变差，甚至下游系统挂了；这个时候会导致大量的线程堆积，不能释放占用的CPU，内存等资源，这种情况下不仅影响该接口的性能，还会影响其他接口的性能，严重的情况会将我们的系统拖垮，造成雪崩效应，通过打开熔断器，流量不再请求到有问题的系统，可以保护我们的系统不被拖垮。降级是一种有损操作，我们作为服务提供者，需要将这种损失尽可能降到最低，无论是返回友好的提示，还是返回可接受的降级数据。降级细分的话又分为人工降级，自动降级。</p><p><strong>人工降级</strong>：人工降级一般采用降级开关来控制，公司内部一般采用配置中心Ducc来做开关降级，开关的修改也是线上操作，这块也需要做好监控</p><p><strong>自动降级</strong>：自动降级是采用自动化的中间件例如Hystrix，公司的小盾龙等；如果采用自动降级的话；我们必须要对降级的条件非常的明确，比如失败的调用次数等；</p><h4 id="2-1-2-3-超时"><a href="#2-1-2-3-超时" class="headerlink" title="2.1.2.3 超时"></a>2.1.2.3 超时</h4><p>分布式系统中的难点之一：不可靠的网络，京东物流现有的微服务架构下，服务之间都是通过JSF网络交互进行同步通信，我们探测下游依赖服务是否可用的最快捷的方式是设置超时时间。超时的设置可以让系统快速失败，进行自我保护，避免无限等待下游依赖系统，将系统的线程耗尽，系统拖垮。</p><p>超时时间如何设置也是一门学问，如何设置一个合理的超时时间也是一个逐步迭代的过程，比如下游新开发的接口，一般会基于压测提供一个TP99的耗时，我们会基于此配置超时时间；老接口的话，会基于线上的TP99耗时来配置超时时间。</p><p>超时时间在设置的时候需要遵循漏斗原则，从上游系统到下游系统设置的超时时间要逐渐减少，如下图所示。为什么要满足漏斗原则，假设不满足漏斗原则，比如服务A调取服务B的超时时间设置成500ms，而服务B调取服务C的超时时间设置成800ms，这个时候回导致服务A调取服务B大量的超时从而导致可用率降低，而此时服务B从自身角度看是可用的；</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2fc89904ae9d4cc9a52b81bcf63e9102~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1580&h=422&s=40692&e=png&b=fefefe" alt="img"></p><p>﻿﻿</p><h4 id="2-1-2-4-重试"><a href="#2-1-2-4-重试" class="headerlink" title="2.1.2.4 重试"></a>2.1.2.4 重试</h4><p>分布式系统中性能的影响主要是通信，无论是在分布式系统中还是垮团队沟通，communication是最昂贵的；比如我们研发都知道需求的交付有一半以上甚至更多的时间花在跨团队的沟通上，真正写代码的时间是很少的；分布式系统中我们查看调用链路，其实我们系统本身计算的耗时是很少的，主要来自于外部系统的网络交互，无论是下游的业务系统，还是中间件：Mysql, redis, es等等；</p><p>所以在和外部系统的一次请求交互中，我们系统是希望尽最大努力得到想要的结果，但往往事与愿违，由于不可靠网络的原因，我们在和下游系统交互时，都会配置超时重试次数，希望在可接受的SLA范围内一次请求拿到结果，但重试不是无限的重试，我们一般都是配置重试次数的限制，偶尔抖动的重试可以提高我们系统的可用率，如果下游服务故障挂掉，重试反而会增加下游系统的负载，从而增加故障的严重程度。在一次请求调用中，我们要知道对外提供的API，后面是有多少个service在提供服务，如果调用链路比较长，服务之间rpc交互都设置了重试次数，这个时候我们需要警惕重试风暴。如下图service D 出现问题，重试风暴会加重service D的故障严重程度。对于API的重试，我们还要区分该接口是读接口还是写接口，如果是读接口重试一般没什么影响，写接口重试一定要做好接口的幂等性。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2e97c5176a3d4712852076328a54b24c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1576&h=434&s=35294&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h4 id="2-1-2-5-兼容"><a href="#2-1-2-5-兼容" class="headerlink" title="2.1.2.5 兼容"></a>2.1.2.5 兼容</h4><p>我们在对老系统，老功能进行重构迭代的时候，一定要做好兼容，否则上线后会出现重大的线上问题，公司内外有大量因为没有做好兼容性，而导致资损的情况。兼容分为：向前兼容性和向后兼容性，需要好好的区分他们，如下是他们的定义:</p><p><strong>向前兼容性</strong>：向前兼容性指的是旧版本的软件或硬件能够与将来推出的新版本兼容的特性，简而言之旧版本软件或系统兼容新的数据和流量。</p><p><strong>向后兼容性</strong>：向后兼容性则是指新版本的软件或硬件能够与之前版本的系统或组件兼容的特性，简而言之新版本软件或系统兼容老的数据和流量。</p><p>根据新老系统和新老数据我们可以将系统划分为四个象限：<strong>第一象限</strong>：新系统和新数据是我们系统改造上线后的状态，<strong>第三象限</strong>：老系统和老数据是我们系统改造上线前的状态，第一象限和第三象限的问题我们在研发和测试阶段一般都能发现排除掉，线上故障的高发期往往出现在第二和第四象限，<strong>第二象限</strong>是因为没有做好向前兼容性，例如上线过程中，发现问题进行了代码回滚，但是在上线过程中产生了新数据，回滚后的老系统不能处理上线过程中新产生的数据，导致线上故障。<strong>第四象限</strong>是因为没有做好向后兼容性，上线后新系统影响了老流程。针对第二象限的问题，我们可以构造新的数据去验证老的系统，针对第四象限的问题，我们可以通过流量的录制回放解决，录制线上的老流量，对新功能进行验证。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b9ce1bec87cc4778867a0edd7c03efae~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1566&h=1082&s=111193&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h4 id="2-1-2-6-隔离"><a href="#2-1-2-6-隔离" class="headerlink" title="2.1.2.6 隔离"></a>2.1.2.6 隔离</h4><p>隔离是将故障爆炸半径最小化的有效手段，在技术方案设计中，我们通过不同层面的隔离来控制影响范围：</p><h5 id="2-1-2-6-1-系统层面隔离"><a href="#2-1-2-6-1-系统层面隔离" class="headerlink" title="2.1.2.6.1 系统层面隔离"></a>2.1.2.6.1 系统层面隔离</h5><p>我们知道系统的分类可以分为：在线的系统，离线系统（批处理系统），近实时系统（流处理系统），如下是这些系统的定义：</p><p><strong>在线系统</strong>：服务端等待请求的到达，接收到请求后，服务尽可能快的处理，然后返回给客户端一个响应，响应时间通常是在线服务性能的主要衡量指标。我们生活中在手机使用的APP大部分都是在线系统；</p><p><strong>离线系统</strong>：或称批处理系统，接收大量的输入数据，运行一个作业来处理数据，并产出输出数据，作业往往需要定时，定期运行一段时间，比如从几分钟到几天，所以用户通常不会等待作业完成，吞吐量是离线系统的主要衡量指标。例如我们看到的报表数据：日订单量，月订单量，日活跃用户数，月活跃用户数都是批处理系统运算一段时间得到的；</p><p><strong>近实时系统</strong>：或者称流处理系统，其介于在线系统和离线系统之间，流处理系统一般会有触发源：用户的行为操作，数据库的写操作，传感器等，触发源作为消息会通过消息代理中间件：JMQ, KAFKA等进行传递，消费者消费到消息后再做其他的操作，例如构建缓存，索引，通知用户等；</p><p>以上三种系统是需要进行隔离建设的，因为他们的衡量指标及对资源的使用情况完全不一样的，比如我们小组会将在线系统作为一个服务单独部署：jdl-uep-main, 离线系统和近实时系统作为一个服务单独部署：jdl-uep-worker；</p><h5 id="2-1-2-6-2-环境的隔离"><a href="#2-1-2-6-2-环境的隔离" class="headerlink" title="2.1.2.6.2 环境的隔离"></a>2.1.2.6.2 环境的隔离</h5><p>从研发到上线阶段我们会使用不同的环境，比如业界常见的环境分为：开发，测试，预发和线上环境；研发人员在开发环境进行开发和联调，测试人员在测试环境进行测试，运营和产品在预发环境进行UAT，最终交付的产品部署到线上环境提供给用户使用。在研发流程中，我们部署时要遵循从应用层到中间件层再到存储层，都要在一个环境，严禁垮环境的调用，比如测试环境调用线上，预发环境调用线上等。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6e611ff2840f4d948add3e1580f93b07~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1554&h=516&s=48556&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h5 id="2-1-2-6-3-数据的隔离"><a href="#2-1-2-6-3-数据的隔离" class="headerlink" title="2.1.2.6.3 数据的隔离"></a>2.1.2.6.3 数据的隔离</h5><p>随着业务的发展，我们对外提供的服务往往会支撑多业务，多租户，所以这个时候我们会按照业务进行数据隔离；比如我们组产生的物流订单数据业务方就包含京东零售，其他电商平台，ISV等，为了避免彼此的影响我们需要在存储层对数据进行隔离，数据的隔离可以按照不同粒度，第一种是通过租户id字段进行区分，所有的数据存储在一张表中，另外一个是库粒度的区分，不同的租户单独分配对应的数据库。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6bf57661c94a4410ad9d385adf1cd069~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1568&h=1624&s=194739&e=png&b=fefefe" alt="img"></p><p>﻿﻿</p><p>数据的隔离除了按照业务进行隔离外，还有按照环境进行隔离的，比如我们的数据库分为测试库，预发库，线上库，全链路压测时，我们为了模拟线上的环境，同时避免污染线上的数据，往往会创建影子库，影子表等。根据数据的访问频次进行隔离，我们将经常访问的数据称为热数据，不经常访问的数据称为冷数据；将经常访问的数据缓存到缓存，提高系统的性能。不经常访问的数据持久化到数据库或者将不使用的数据结转归档到</p><h5 id="2-1-2-6-4-核心，非核心隔离"><a href="#2-1-2-6-4-核心，非核心隔离" class="headerlink" title="2.1.2.6.4 核心，非核心隔离"></a>2.1.2.6.4 核心，非核心隔离</h5><p>我们知道应用是分级的，京东内部针对应用的重要程度会将应用分为0，1，2，3级应用。业务的流程也分为黄金流程和非黄金流程。在业务流程中，针对不同级别的应用交互，需要将核心和非核心的流程进行隔离。例如在交易业务过程中，会涉及到订单系统，支付系统，通知系统，那这个过程中核心系统是订单系统和支付系统，而通知相对来说重要性不是那么高，所以我们会投入更多的资源到订单系统和支付系统，优先保证这两个系统的稳定性，通知系统可以采用异步的方式与其他两个系统解耦隔离，避免对其他另外两个系统的影响。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/30513d12b06f4508bc1872b578d3a539~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1562&h=414&s=35529&e=png&b=fefefe" alt="img"></p><p>﻿﻿</p><h5 id="2-1-2-6-5-读写隔离"><a href="#2-1-2-6-5-读写隔离" class="headerlink" title="2.1.2.6.5 读写隔离"></a>2.1.2.6.5 读写隔离</h5><p>应用层面，领域驱动设计（DDD）中最著名的CQRS（Command Query Responsibility Segregation）将写服务和读服务进行隔离。写服务主要处理来自客户端的command写命令，而读服务处理来自客户端的query读请求，这样从应用层面进行读写隔离，不仅可以提高系统的可扩展性，同时也会提高系统的可维护性，应用层面我们都采用微服务架构，应用层都是无状态服务，可以扩容加机器随意扩展，存储层需要持久化，扩展就比较费劲。除了应用层面的CQRS，在存储层面，我们也会进行读写隔离，例如数据库都会采用一主多从的架构，读请求可以路由到从库从而分担主库的压力，提高系统的性能和吞吐量。所以应用层面通过读写隔离主要解决可扩展问题，存储层面主要解决性能和吞吐量的问题。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d77b07186a0f460085945023de56420c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1508&h=1050&s=381840&e=png&b=fefefe" alt="img"></p><p>﻿﻿</p><p>﻿</p><h5 id="2-1-2-6-6-线程池隔离"><a href="#2-1-2-6-6-线程池隔离" class="headerlink" title="2.1.2.6.6 线程池隔离"></a>2.1.2.6.6 线程池隔离</h5><p>线程是昂贵的资源，为了提高线程的使用效率，避免创建和销毁的消耗，我们采用了池化技术，线程池来复用线程，但是在使用线程池的过程中，我们也做好线程池的隔离，避免多个API接口复用同一个线程。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59cfe0297d904a949fff209ee14598f5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1574&h=622&s=73589&e=png&b=fefefe" alt="img"></p><p>﻿﻿</p><h2 id="2-2-代码Review"><a href="#2-2-代码Review" class="headerlink" title="2.2 代码Review"></a>2.2 代码Review</h2><p>codeReview是研发阶段的最后一个流程，对线下的bug率和线上质量及稳定性有着重要的作用，针对于代码如何review，谈一些自己的看法：</p><p>•<strong>形成团队代码风格</strong>：首先一个团队的代码应该形成该团队的代码风格，这样能够提高codeReview的效率及协作的效率，作为新加入的成员，应该遵循团队的代码风格规范。</p><p>•<strong>Review的关注点</strong>：代码review切记不要陷入细节，主要以review代码风格为主，如果一个团队形成统一的代码风格，我们通过review风格就能将大部分问题发现，在关注功能的同时，再关注下性能，安全。</p><p>•<strong>结对编程</strong>：在代码编写过程中，我们要培养结对编程的习惯，这样针对某次需求，codeReview时，熟悉该模块的同事把控下细节，架构师把控风格。</p><p>•<strong>控制每次review代码量</strong>：每次提交代码进行review时，不要一次性提交review大量的代码，要将review的内容细分，比如一个方法的实现，一个类等。</p><p>•<strong>开放心态</strong>：review的过程其实是学习提升的过程，通过代码review，虚心接收别人的意见，学习优雅代码的编写方式，提高自己的代码水平。</p><h1 id="3-上线阶段"><a href="#3-上线阶段" class="headerlink" title="3 上线阶段"></a>3 上线阶段</h1><p>我们可以看下公司的故障管理平台白虎所记录的故障：发生系统故障一般都是外部对系统做了改变，往往发生在上线阶段：代码的部署，数据库的更改，配置中心的变动等；上线阶段是故障的高发期；一个系统不可能不出线上问题，我们所要追求的是，降低线上的故障频率，缩短故障恢复时间。针对上线过程出现问题，我们知道业界有著名的上线过程三板斧：可监控，可灰度，可回滚。</p><h2 id="3-1-上线三板斧"><a href="#3-1-上线三板斧" class="headerlink" title="3.1 上线三板斧"></a>3.1 上线三板斧</h2><h3 id="3-1-1-可监控"><a href="#3-1-1-可监控" class="headerlink" title="3.1.1 可监控"></a>3.1.1 可监控</h3><p>上线的过程中，我们的系统要做到可监控，如果没有监控，上线过程中我们对系统的状态是一无所知，是很可怕的。监控什么东西那，其实监控的就是指标。这就涉及到指标的定义，指标我们分为业务指标和技术指标，技术指标又分为软件和硬件。业务指标一般是我们定义的观测业务变化情况的度量，例如订单量，支付量等。技术层面的软件指标：可用率，TP99, 调用量，技术层面的硬件指标：cpu 内存 磁盘 网络IO。目前我们二级部门在做OpsReview，主要review的是可用率，TP99，调用量这几个指标，分别对应系统的可用性，性能，并发。</p><p>做好这些指标的监控后，我们接下来需要做的是针对这些指标做好告警，如果某个指标突破设定的阈值后，需要进行告警通知给我们，针对监控告警指标阈值的设置，建议先严后松，即系统建设初始阶段设置的严格些，避免遗漏告警，出现线上问题，后续随着系统建设的迭代需要设置更合理的告警阈值，避免告警泛滥，造成狼来了的效应。总之上线发布过程的一段时间是事故和问题发生的高峰，这块一定做好指标监控，日志监控，对报警要敏感。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9711b4d6ff324945a072d878de935cfe~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1288&h=1482&s=161810&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h3 id="3-1-2-可灰度"><a href="#3-1-2-可灰度" class="headerlink" title="3.1.2 可灰度"></a>3.1.2 可灰度</h3><p>上线过程中，我们要做到可灰度，通过灰度执行变更以限制爆炸半径，降低影响范围，同时灰度过程要做好兼容。灰度分为不同维度的灰度：机器维度，机房维度，地域维度，业务维度：用户，商家，仓，承运商等。</p><p><strong>机器维度</strong>：我们用行云部署时，可以每个分组先部署一部分机器进行灰度，灰度一段时间比如：24小时没什么问题后，再部署剩余的机器。</p><p><strong>机房维度</strong>：微服务架构下，我们的应用会部署在不同的机房中，可以按照机房维度灰度，比如先部署发布代码在某个机房分组下，观察一段时间再按照比例扩大灰度机房范围直至全量。例如先部署中云信的机房，灰度一段时间后，再逐步灰度有孚的机房。</p><p><strong>地域维度</strong>：现在的部署架构都是多机房互为灾备，异地多活，单元化部署，例如业界美团的外卖业务非常适合做异地多活，单元化部署，因为外卖业务的商户，用户，骑手天然具有聚合性，北京的用户大概率不会在上海点外卖，这样根据业务的属性，在系统建设的时候，从应用层到中间件层，再到存储层可以单元化部署在上海地域的机房和北京地域的机房，功能发布的时候可以灰度某个地域，做到地域级别的容灾。</p><p><strong>业务维度：</strong> 在上线过程中，我们也可以根据业务属性进行灰度，例如上线了某个功能或者产品，根据用户维度灰度，某些用户或者某些商户才能使用该功能，产品。</p><h3 id="3-1-3-可回滚"><a href="#3-1-3-可回滚" class="headerlink" title="3.1.3 可回滚"></a>3.1.3 可回滚</h3><p>线上出现问题时，我们应该优先止损，其次才是分析根因。止损的最快方式就是回滚，<strong>回滚分为代码回滚和数据回滚</strong>，代码回滚即将我们代码恢复到原有的逻辑，代码回滚有两种方式：开关控制和部署回滚。最快捷的方式是开关控制，一键开关打开或者关闭就可以实现回滚到原有的逻辑，操作成本最低，止损最快速。第二种方式就是部署回滚，通过发布平台，例如行云将代码回滚到上个稳定运行的版本。有时候我们代码回滚完，如果没有做好向前兼容性，系统应用依然有问题，例如上线过程中产生了新数据，回滚完后，代码不能处理新的数据。所以这个时候又涉及到数据的回滚，数据的回滚涉及到修数：将产生的新数据无效掉，或者修改为正确的数据等，当数据量比较大时，数据的回滚一般耗时费力，所以建议做好向前兼容性，直接代码回滚。</p><h2 id="3-2-线上问题应对"><a href="#3-2-线上问题应对" class="headerlink" title="3.2 线上问题应对"></a>3.2 线上问题应对</h2><h3 id="3-2-1-常见问题分类"><a href="#3-2-1-常见问题分类" class="headerlink" title="3.2.1 常见问题分类"></a>3.2.1 常见问题分类</h3><p>针对线上的问题，我们第一步是识别出是什么问题，然后才能解决问题，针对线上各种各样的问题我们可以进行聚合，归并分类下，针对每种问题去参考业界的处理方法和团队的内的紧急预案，做到临阵不乱。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/da68cadd57934caca7f07d2d25c3a5a2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=494&h=1556&s=117823&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h3 id="3-2-2-问题生命周期"><a href="#3-2-2-问题生命周期" class="headerlink" title="3.2.2 问题生命周期"></a>3.2.2 问题生命周期</h3><p>当出现问题时，我们也需要清楚一个线上问题的生命周期：从问题发生，到我们发现问题，进而进行响应处理，观测问题是否修复，服务是否恢复正常，到最终针对该问题进行复盘，当发生系统发生问题时，我们越早发现问题，对业务的影响越小，整个流程如下图所示。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f551b0758e664597bccf745008052e3f~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1558&h=510&s=58318&e=png&b=fefefe" alt="img"></p><p>﻿﻿</p><h3 id="3-2-3-如何预防问题"><a href="#3-2-3-如何预防问题" class="headerlink" title="3.2.3 如何预防问题"></a>3.2.3 如何预防问题</h3><p>就像人的身体生病一样，当问题发生已经晚了，我们要投入更多时间和精力到如何预防中，就像扁鹊的大哥一样治未病，防患于未然。根据破窗原理，一个问题出现了，如果放任不管，问题的严重性会越来越大，直到不可挽回。我们可以从研发的规范，研发的流程，变更流程这几个方面进行预防。</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aae8de2df4ae45648418909cf6e30d6d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=910&h=1272&s=137124&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h3 id="3-2-4-如何发现问题"><a href="#3-2-4-如何发现问题" class="headerlink" title="3.2.4 如何发现问题"></a>3.2.4 如何发现问题</h3><p>对于一个系统，如果外界不对其做功，根据熵增原理，其会越来越混乱，直到出现问题，外界对其做功，就涉及到改变，因为改变是人在操作，由于各种不可控的因素，也会导致各种线上问题，所以我们可以看到对于一个系统上线后不出现问题是不可能的，当出现问题时，我们第一步是如何快速的发现问题？对于问题发现的渠道，工作中接触到的有如下几种：自我意识，监控告警，业务反馈；</p><p><strong>自我意识</strong>：我们C2部门每周有一个重要会议OpsReview，各个C3团队会对个团队的核心接口的不规律跳点，毛刺进行可用率，性能，调用量的review，以通过这种主动的，自我意识行为发现潜在的线上问题。同时我们组每天早会的重要一项：UMP监控全域看板的review，我们会对昨天核心接口的可用率，TP99，调用量，进行分析的，对于可用率降低，TP99有毛刺，不规范的流量调用会进行排查原因，尽早自我发现问题，同时也会对机器的CPU, 内存使用率，Mysql, redis , es各种存储进行review。</p><p><strong>监控告警</strong>：这是我们发现问题最常用的渠道，通过主动的监控指标，被动的接收告警来发现问题，告警指标我们分为业务指标和技术指标，具体分类可详见3.1.1可监控部分</p><p><strong>业务反馈</strong>：这种发现问题的方式是我们最不愿意看到的，如果等到业务反馈，说明线上问题已经影响到用户，我们常常因为监控告警的缺失，漏报而导致落后于业务发现问题，所以我们最希望每个人，团队都有这种自我意识，将线上问题提早发现，防患于未然。</p><h3 id="3-2-5-如何响应问题"><a href="#3-2-5-如何响应问题" class="headerlink" title="3.2.5 如何响应问题"></a>3.2.5 如何响应问题</h3><p>出现线上问题后，我们个人对问题的认知是非常有限的，并且这个时候人处于一种高度紧张的状态，所以这个时候一定要群里周知自己的leader，将情况如实表达，不要夸大和缩小问题的范围和影响，同时将问题进行通告。整个问题的响应过程包含以下几步：</p><p>1.<strong>保留现场：</strong> 问题发生的现场是我们排查问题的依据，所以要将现场的日志，数据等信息保存好，比如内存dump, 线程dump，避免机器重启后这些信息的丢失。</p><p>2.<strong>提供信息</strong>：提供自己所知道的信息，协助排查，不要扩大和缩小问题</p><p>3.<strong>恢复服务</strong>：当出现线上问题是，我们追求的是以最快的速度恢复服务，快速止损，业界有快速止血，恢复服务的几板斧：回滚：服务回滚，数据回滚，重启，扩容，禁用节点，功能降级</p><p>4.<strong>双重确认：</strong> 服务恢复后，我们需要确认是否恢复了，可以通过观察：业务指标是否正常，技术指标是否正常，数据是否正常，日志是否正常等来观测问题的恢复情况</p><p>5.<strong>故障通告：</strong> 确认问题没有什么问题后，需要再应急群中周知大家：业务人员，产品经理，系统的上下游，测试人员，SRE等。并让产品和业务进行确认，然后周知用户。</p><h3 id="3-2-6-如何定位问题"><a href="#3-2-6-如何定位问题" class="headerlink" title="3.2.6 如何定位问题"></a>3.2.6 如何定位问题</h3><p>服务恢复后，我们可以回过头来细致的分析下到底是什么原因导致了线上的问题。定位问题也要讲究方法论，这就涉及到定位问题三要素：知识，工具，方法。</p><p><strong>知识</strong>：相对其他行业，计算机行业应该是知识更新迭代最快的行业，所以我们需要不断的去学习，更新自己的知识库，不给自己设限。例如你想解决FullGC问题，你必须对JVM进行系统的学习，想解决慢sql，必须对Mysql进行系统的学习，现在AI大模型这么火，我们也需要对prompt engineering， RAG ， Agent, 多模态等进行学习了解。有了知识我们才能遇到问题时，知道是什么，为什么？</p><p><strong>工具：</strong> 工欲善其事，必先利其器，工程师要善于借助公司工具来提高解决问题的效率，熟练使用公司各种中间件工具，公司已经有的中间件，优先使用公司的中间件，公司内一个中间件团队维护的中间件工具要优于业务研发小组内维护的中间件工具，不要小组内部，或者团队内部重复造轮子，并且小组内人员的流动变更，容易造成中间件没人维护。下图是公司常用的中间件工具：</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9bdeed0714f646dd99e6ecff69a3c6e0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=1410&s=169542&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><p><strong>方法</strong>：解决问题我们要讲究方法，选择正确的方法可以事半功倍，提高我们定位问题及解决问题的效率，下面是我们研发人员常见的排查问题的方法</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b3c3b597ca0f4da48a20cf6412deda63~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1082&h=1614&s=362937&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h3 id="3-2-7-如何修复问题"><a href="#3-2-7-如何修复问题" class="headerlink" title="3.2.7 如何修复问题"></a>3.2.7 如何修复问题</h3><p>有了知识，工具和方法后，其实我们很快的就定位到问题了，定位到问题后，我们就要想办法如何去把问题修复了，以下是问题修复的流程：</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/07a4c47142e541308af2898b444c52b4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1576&h=488&s=64103&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h3 id="3-2-8-如何复盘问题"><a href="#3-2-8-如何复盘问题" class="headerlink" title="3.2.8 如何复盘问题"></a>3.2.8 如何复盘问题</h3><p>问题发生后，我们需要从此次问题中分析根因，并汲取教训和经验，避免犯同样的错误。这就涉及到问题的复盘，如何进行问题的复盘那，一般会经过如下几个步骤：回顾目标，评价结果，分析原因，总结经验。例如我们C2部门每周的opsReview会议上都会有线上问题的复盘：coe，如何进行coe复盘谈一些自己的思考。</p><p>•参考业界的5WHY分析法剖析问题的根因</p><p>•5WHY分析法：5代表的是问题的深度，而不是问题的数量</p><p>•基于问题的答案继续进行提问，5个问题是有关联的，层层递进的，找到问题的根因</p><p>﻿</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/98a9b75f31474104a2fda8d66a5ca778~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1564&h=418&s=36158&e=png&b=ffffff" alt="img"></p><p>﻿﻿</p><h1 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4 参考资料"></a>4 参考资料</h1><p>•﻿<a href="https://link.juejin.cn/?target=https://itrevolution.com/articles/20-years-of-google-sre-10-key-lessons-for-reliability/">itrevolution.com&#x2F;articles&#x2F;20…</a>﻿</p><p>•﻿<a href="https://link.juejin.cn/?target=https://learn.microsoft.com/en-us/previous-versions/msp-n-p/jj591573(v=pandp.10)?redirectedfrom=MSDN">learn.microsoft.com&#x2F;en-us&#x2F;previ…</a>﻿</p><p>•﻿<a href="https://link.juejin.cn/?target=https://sre.google/books/">sre.google&#x2F;books&#x2F;</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;系统稳定性建设&quot;&gt;&lt;a href=&quot;#系统稳定性建设&quot; class=&quot;headerlink&quot; title=&quot;系统稳定性建设&quot;&gt;&lt;/a&gt;系统稳定性建设&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;这篇文章以京东为例，从后端研发视角谈系统稳定性建设，包括研发和上线阶段。研</summary>
      
    
    
    
    <category term="技术小栈" scheme="http://ai.mak.cn/categories/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/"/>
    
    
    <category term="架构" scheme="http://ai.mak.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>实时通信的关键差异解析</title>
    <link href="http://ai.mak.cn/2024/09/15/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/%E5%AE%9E%E6%97%B6%E9%80%9A%E4%BF%A1%E7%9A%84%E5%85%B3%E9%94%AE%E5%B7%AE%E5%BC%82%E8%A7%A3%E6%9E%90/"/>
    <id>http://ai.mak.cn/2024/09/15/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/%E5%AE%9E%E6%97%B6%E9%80%9A%E4%BF%A1%E7%9A%84%E5%85%B3%E9%94%AE%E5%B7%AE%E5%BC%82%E8%A7%A3%E6%9E%90/</id>
    <published>2024-09-14T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时通信的关键差异解析"><a href="#实时通信的关键差异解析" class="headerlink" title="实时通信的关键差异解析"></a>实时通信的关键差异解析</h1><blockquote><p>文章介绍了 OSI 七层网络协议模型相关知识，详细阐述了 WebSocket 和 Server-Sent Events (SSE)的工作原理、特点、适用场景等。重点比较了二者在多个维度的差异，如通信模式、连接建立、适用场景等，并分析了豆包选择 SSE 而非 WebSocket 的原因，最后总结二者均是实现实时通信的有效技术，各有适用场景。</p></blockquote><blockquote><p>链接：<a href="https://juejin.cn/post/7411406818025717770">https://juejin.cn/post/7411406818025717770</a><br>来源：稀土掘金</p></blockquote><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在我们平时使用豆包的过程中，不知道大家有没有发现，界面上的内容不是一次性显示出来的，而是像是以一种实时的方式打印出来的，这种方式我们把他称之为流式打印。这种效果感觉互动性更强，更加真实，那么这种技术是如何实现的呢？其实传统的请求-响应模型（如HTTP&#x2F;HTTPS）并不能很好地满足这种实时流式输出。为此，目前主流的web实时通信技术主要是以以WebSocket和Server-Sent Events (SSE)为主，下面我们就一起介绍一下这两者之间的差异，以及为什么豆包选择使用SSE而不是Websocket？</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/b4e3a8c5623b483db90b0d0444cdca50~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=F4lz8ACEYugkjodb+ZKCP5bSkx0=" alt="image.png"></p><h2 id="二、一些网络协议相关的知识补充"><a href="#二、一些网络协议相关的知识补充" class="headerlink" title="二、一些网络协议相关的知识补充"></a>二、一些网络协议相关的知识补充</h2><h3 id="1-OSI网络协议模型"><a href="#1-OSI网络协议模型" class="headerlink" title="1. OSI网络协议模型"></a>1. OSI网络协议模型</h3><p>OSI（Open System Interconnection）参考模型即开放式系统互联通信参考模型，是一种概念模型，由国际标准化组织（ISO）提出，目的是为了使各种计算机在世界范围内互连为网络。OSI 模型将计算机网络体系结构划分为七层，从下到上分别是：</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/4b14ba20de584e908d2e622a9a0d237c~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=IkykC1FIer//Lqj/OA3TYgq2Dww=" alt="image.png"></p><p><strong>物理层</strong></p><ol><li>定义：物理层主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由 1、0 转化为电流强弱来进行传输，到达目的地后在转化为 1、0，也就是我们常说的数模转换与模数转换）。</li></ol><p><strong>数据链路层</strong></p><ol><li>定义：数据链路层主要将从物理层接收的数据进行 MAC 地址（网卡的地址）的封装与解封装。常把这一层的数据叫做帧。在这一层工作的设备是交换机，数据通过交换机来传输时，会根据数据中包含的 MAC 地址信息进行转发，实现不同设备之间的通信。以太网协议、PPP 协议等。以太网协议是最常见的数据链路层协议之一，它规定了如何在局域网中传输数据帧。PPP 协议则常用于广域网连接中，实现数据的可靠传输。</li></ol><p><strong>网络层</strong></p><ol><li>定义：网络层主要将从下层接收到的数据进行 IP 地址的封装与解封装。在这一层工作的设备是路由器，它根据 IP 地址将数据包从一个网络转发到另一个网络，实现不同网络之间的通信。IP 协议、ICMP 协议等。IP 协议是网络层的核心协议，它规定了如何给网络中的设备分配 IP 地址，以及如何将数据包从源地址传输到目的地址。ICMP 协议用于在 IP 网络中发送控制消息，例如报告错误或进行网络诊断。</li></ol><p><strong>传输层</strong></p><ol><li>定义：传输层定义了一些传输数据的协议和端口号，如 TCP 和 UDP。它主要是将从下层接收的数据进行分段和重组，为应用层提供端到端的可靠数据传输服务。例如，当你在浏览器中访问一个网页时，浏览器和服务器之间的通信就是通过传输层的协议来实现的。TCP（Transmission Control Protocol，传输控制协议）是一种面向连接的、可靠的传输层协议。它通过三次握手建立连接，在数据传输过程中进行流量控制和拥塞控制，确保数据的准确无误传输。UDP（User Datagram Protocol，用户数据报协议）是一种无连接的、不可靠的传输层协议。它不保证数据的可靠传输，但具有传输速度快、开销小的优点，适用于一些对实时性要求较高的应用，如视频会议、在线游戏等。</li></ol><p><strong>会话层</strong></p><ol><li>定义：会话层主要负责建立、管理和终止表示层实体之间的通信会话。它通过在通信双方之间建立、维护和管理会话，确保数据的有序传输。例如，在进行远程登录时，会话层负责建立和管理用户与远程服务器之间的会话，确保用户的输入和服务器的响应能够正确地交互。会话层的主要功能包括会话建立、会话管理和会话终止。在会话建立阶段，通信双方通过协商确定会话的参数，如会话标识符、加密方式等。在会话管理阶段，会话层负责维护会话的状态，确保数据的正确传输和接收。在会话终止阶段，会话层负责安全地终止会话，释放资源。</li></ol><p><strong>表示层</strong></p><ol><li>定义：表示层主要负责数据的表示、加密和压缩等功能。它将应用层的数据转换为适合网络传输的格式，并进行加密和压缩等处理，以提高数据的安全性和传输效率。例如，当你在网上购物时，你的信用卡信息需要进行加密处理，以确保信息的安全传输。表示层就是负责这种加密和转换的工作。表示层的主要功能包括数据格式转换、数据加密和数据压缩。在数据格式转换方面，它可以将不同格式的数据转换为统一的网络标准格式，以便在不同的系统之间进行通信。在数据加密方面，它可以使用各种加密算法对数据进行加密，保护数据的安全性。在数据压缩方面，它可以对数据进行压缩，减少数据的传输量，提高传输效率。</li></ol><p><strong>应用层</strong></p><ol><li>定义：应用层是 OSI 模型的最高层，它直接面向用户，为用户提供各种网络应用服务。例如，电子邮件、文件传输、网页浏览等都是应用层的服务。应用层通过调用下层的服务，实现各种具体的网络应用功能。HTTP（HyperText Transfer Protocol，超文本传输协议）是用于在 Web 上传输超文本的协议。SMTP（Simple Mail Transfer Protocol，简单邮件传输协议）用于发送电子邮件。FTP（File Transfer Protocol，文件传输协议）用于在网络上传输文件等。 总之，OSI 七层网络协议模型为计算机网络的通信提供了一个标准化的框架，使得不同的计算机系统和网络设备能够相互通信和协作。每一层都有其特定的功能和协议，它们共同构成了一个完整的网络通信体系。</li></ol><h3 id="2-WebSocket"><a href="#2-WebSocket" class="headerlink" title="2. WebSocket"></a>2. WebSocket</h3><p>WebSocket 是一种在单个 TCP 连接上进行全双工通信的协议。它使得客户端和服务器之间可以实现实时、双向的数据传输。</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/c3933ff01fa94c51b9a0a1a1008fb706~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=I2pCsBywY8c0pBK5WAg7ogJdAko=" alt="image.png"></p><h4 id="2-1-工作原理"><a href="#2-1-工作原理" class="headerlink" title="2.1 工作原理"></a>2.1 工作原理</h4><p>WebSocket是一种独立的协议，用于在单个TCP连接上提供全双工通信。它通过HTTP&#x2F;HTTPS协议完成初始握手，然后升级到WebSocket协议。</p><p><strong>握手过程</strong></p><ul><li>• 客户端发送一个HTTP请求给服务器，包含特殊的WebSocket头信息，用于请求协议升级（从HTTP&#x2F;HTTPS升级到WebSocket协议）。</li><li>• 服务器接收到这个请求后，检查头信息并确认是否同意升级。如果同意，它会返回一个带有101状态码的HTTP响应，表示协议切换（Switching Protocols）。</li></ul><p><strong>通信过程</strong></p><ul><li>• 在握手成功后，客户端和服务器通过WebSocket协议持久连接进行通信。此连接将保持打开状态，双向数据流可以在任一端发起，而不需要每次发消息都重新建立连接。WebSocket的消息是帧格式的，分为数据帧和控制帧。</li></ul><h4 id="2-2-特点"><a href="#2-2-特点" class="headerlink" title="2.2 特点"></a>2.2 特点</h4><ul><li>• <strong>双向通信</strong>：支持客户端和服务器之间的全双工通信。</li><li>• <strong>低延迟</strong>：适合要求低延迟的实时应用，如在线聊天、多人游戏。</li><li>• <strong>数据帧格式</strong>：WebSocket协议规定了消息的帧格式，包括控制帧和数据帧。</li></ul><h4 id="2-3-适用场景"><a href="#2-3-适用场景" class="headerlink" title="2.3 适用场景"></a>2.3 适用场景</h4><ol><li>实时聊天应用：WebSocket 非常适合实时聊天应用，用户可以即时发送和接收消息，无需不断刷新页面。聊天消息可以实时推送给所有在线用户，提供良好的交互体验。</li><li>在线游戏：在在线游戏中，需要实时传输玩家的操作和游戏状态。WebSocket 可以实现低延迟的通信，确保玩家能够及时响应游戏中的变化。</li><li>股票行情和金融数据：对于股票行情和金融数据的实时更新，WebSocket 可以提供快速的数据传输，使投资者能够及时了解市场动态。</li><li>协作工具：如在线文档编辑、实时绘图等协作工具需要实时同步用户的操作。WebSocket 可以实现多个用户之间的实时协作，提高工作效率。</li><li>物联网（IoT）：在物联网应用中，设备需要实时向服务器发送数据并接收指令。WebSocket 可以为物联网设备提供可靠的通信渠道，实现远程监控和控制。</li></ol><h3 id="3-Server-Sent-Events-SSE"><a href="#3-Server-Sent-Events-SSE" class="headerlink" title="3. Server-Sent Events (SSE)"></a>3. Server-Sent Events (SSE)</h3><h4 id="3-1-工作原理"><a href="#3-1-工作原理" class="headerlink" title="3.1 工作原理"></a>3.1 工作原理</h4><p>Server-Sent Events (SSE) 是一种在客户端和服务器之间传递事件的机制，主要用于<strong>服务器向客户端推送实时数据</strong>。<strong>SSE并不是一种协议而是一种机制</strong>，这也是区别于WebSocket的地方之一。</p><p>虽然SSE并没有像WebSocket那样定义出一个独立的协议，但它却通过标准的HTTP协议实现了类似长连接的功能。SSE规定了特定的<strong>MIME类型和数据格式</strong>，来让服务器持续发送数据流。</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/184fbdff2937467b901993bd34350b96~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=qCf6DSSspZxXzw3LkLbGoui5QXQ=" alt="image.png"></p><h4 id="3-2-特点"><a href="#3-2-特点" class="headerlink" title="3.2 特点"></a>3.2 特点</h4><p>\1. <strong>使用标准HTTP协议</strong></p><p>SSE在现有的HTTP&#x2F;HTTPS协议之上构建，不需要额外的协议或端口。客户端向服务器发送一个普通HTTP请求，服务器响应该请求，并持续发送数据。 如下图所示：</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/075669e9968d4c36b98f70b2570a4861~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=0lGOiKE8nBvzb6oLA/hue27lRx4=" alt="image.png"></p><p>\2. <strong>特定的MIME类型</strong></p><p>在SSE中，服务器的响应内容类型必须是 <code>text/event-stream</code>，这告诉浏览器或者客户端这是一个SSE数据流。 如下图所示：</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/204d08548ec24c869d76cbf91c11ea3f~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=ZOSJ9TZHvGudhbOvsiDyafT9h4I=" alt="image.png"></p><p>\3. <strong>单向通信</strong></p><p>SSE是一种单向通信方式，即服务器推送数据到客户端，客户端通常只是接收和处理数据，不向服务器发送响应的数据流。需要额外进行双向通信时，可以通过Ajax等其他技术辅助手段实现。</p><p>\4. <strong>自动重连和事件流ID</strong>（Last-Event-ID）</p><p>SSE具有自动重连功能。如果连接中断，浏览器会自动重新连接，并且可以通过 <code>Last-Event-ID</code> 头来继续从断开点接收数据。</p><p><img src="https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/22664beed55d464a8552d6649c30ece4~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5LqU5Y-35Y6C5oi_:q75.awebp?rk3s=f64ab15b&x-expires=1734923281&x-signature=7BmGc4RND6Cv0Jt2V9QjEbGRCQQ=" alt="image.png"></p><h4 id="3-3-适用场景"><a href="#3-3-适用场景" class="headerlink" title="3.3 适用场景"></a>3.3 适用场景</h4><ol><li>实时通知：SSE 可以用于实现实时通知，如邮件通知、系统警报等。当有新的通知时，服务器可以立即将通知推送给客户端，用户无需手动刷新页面即可获取最新的通知。</li><li>股票行情和金融数据：与 WebSocket 类似，SSE 也可以用于实时更新股票行情和金融数据。服务器可以将最新的股票价格、指数等数据推送给客户端，让用户能够及时了解市场动态。</li><li>日志监控：在服务器端进行日志监控时，可以使用 SSE 将日志信息实时推送给客户端。这样，管理员可以在浏览器中实时查看服务器的日志，及时发现和解决问题。</li><li>社交网络更新：社交网络应用可以使用 SSE 来实时推送新的动态、消息和通知。用户可以在不刷新页面的情况下获取最新的社交网络更新。</li></ol><h2 id="三、-豆包为什么选择SSE而不是websocket的呢"><a href="#三、-豆包为什么选择SSE而不是websocket的呢" class="headerlink" title="三、 豆包为什么选择SSE而不是websocket的呢?"></a>三、 豆包为什么选择SSE而不是websocket的呢?</h2><p>SSE建立在已有的HTTP协议之上，这是Web开发中最常用和最成熟的协议之一。以下是这两者各个维度的比较</p><table><thead><tr><th>比较类别</th><th>Server-Sent Events（SSE）</th><th>WebSocket</th></tr></thead><tbody><tr><td>通信模式</td><td>单向，服务器向客户端发送数据</td><td>双向，客户端和服务器可互相发送数据</td></tr><tr><td>连接建立</td><td>基于 HTTP 协议，客户端发起特殊请求，服务器响应</td><td>通过握手协议建立全双工连接</td></tr><tr><td>适用场景</td><td>适用于单向的服务器推送场景，如实时通知、股票行情等</td><td>适用于双向通信的实时应用，如在线游戏、实时协作等</td></tr><tr><td>重连机制</td><td>如果连接中断，客户端自动尝试重新连接</td><td>如果连接中断，客户端可尝试重新连接</td></tr><tr><td>数据传输格式</td><td>事件流格式，每个事件由事件类型和数据组成</td><td>可以是文本数据或二进制数据</td></tr><tr><td>复杂性</td><td>相对简单，基于 HTTP 协议，无需处理复杂的双向通信状态</td><td>相对复杂，需要处理更多协议细节和状态管理</td></tr><tr><td>浏览器支持</td><td>广泛支持</td><td>广泛支持</td></tr><tr><td>协议开销</td><td>通常较小，因为基于 HTTP 协议且单向通信</td><td>相对较大一些，由于要建立全双工连接和处理更多状态</td></tr><tr><td>服务器资源占用</td><td>一般情况下相对较低，因为主要是单向推送</td><td>可能较高，因为需要处理双向通信和更多的连接状态变化</td></tr><tr><td>安全性</td><td>依赖于底层 HTTP 的安全机制</td><td>可以使用加密等安全措施，与 HTTP 类似但需额外配置</td></tr><tr><td>开发难度</td><td>对于简单的服务器推送场景较容易开发</td><td>双向通信场景下开发难度相对较高，需要处理更多复杂情况</td></tr><tr><td>与代理服务器兼容性</td><td>通常较好，因为基于 HTTP 协议，与常见代理服务器兼容性高</td><td>可能会遇到一些代理服务器不兼容的情况，需要进行额外配置</td></tr></tbody></table><p>综上所述，豆包选择了SSE而不是Websocket。</p><h2 id="四、-总结"><a href="#四、-总结" class="headerlink" title="四、 总结"></a>四、 总结</h2><p>WebSocket和Server-Sent Events (SSE) 都是实现长连接、实时通信的有效技术，各有优劣。WebSocket适合需要低延迟、双向通信的应用场景，如聊天应用、在线游戏和实时协作工具。SSE则更适用于单向数据推送场景，是一种简单而有效的服务器推送技术，非常适用于各种需要实时更新的应用场景，如网页对话、新闻实时更新、股票行情和实时投票结果展示，它基于 HTTP 协议，具有自动重连和事件流格式等特点，使得客户端能够轻松地接收服务器推送的信息。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;实时通信的关键差异解析&quot;&gt;&lt;a href=&quot;#实时通信的关键差异解析&quot; class=&quot;headerlink&quot; title=&quot;实时通信的关键差异解析&quot;&gt;&lt;/a&gt;实时通信的关键差异解析&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;文章介绍了 OSI 七层网络协议模型相关知</summary>
      
    
    
    
    <category term="技术小栈" scheme="http://ai.mak.cn/categories/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/"/>
    
    
    <category term="架构" scheme="http://ai.mak.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>LSTM详解</title>
    <link href="http://ai.mak.cn/2024/09/08/ai/%E8%AE%B2%E9%80%8F%E4%B8%80%E4%B8%AA%E5%BC%BA%E5%A4%A7%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B%EF%BC%8CLSTM%20%EF%BC%81%EF%BC%81/"/>
    <id>http://ai.mak.cn/2024/09/08/ai/%E8%AE%B2%E9%80%8F%E4%B8%80%E4%B8%AA%E5%BC%BA%E5%A4%A7%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B%EF%BC%8CLSTM%20%EF%BC%81%EF%BC%81/</id>
    <published>2024-09-07T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.341Z</updated>
    
    <content type="html"><![CDATA[<p>咱们简单从 what、why、how三方面进行介绍~</p><p><strong>what：一句话介绍</strong></p><p>咱们一句话介绍：LSTM，全名是「长短期记忆网络」（Long Short-Term Memory），是一种特殊的人工神经网络，主要用来处理和预测时间序列数据（就是那些有时间顺序的数据，比如天气预报、股市行情等）。</p><p><strong>why：为什么需要 LSTM</strong></p><p>其次，咱们要明白，为什么需要LSTM。</p><p>传统的神经网络在处理时间序列数据时有个很大的问题：它们记不住长期的依赖关系。举个例子，如果你在看一部电视剧，前几集提到的一个重要线索在后面几集才会揭示它的意义。普通神经网络就像是有点健忘的观众，只能记住最近几集的内容，早前的线索都忘了。而LSTM就像是一个记性很好的观众，它能够记住前面提到的重要细节，并在需要的时候利用这些信息。</p><p><strong>how：LSTM 怎么做到</strong></p><p>LSTM 是怎么做到的？</p><p>LSTM 通过一个巧妙的设计，让网络能够记住之前的信息，并且在合适的时候把这些信息传递下去。具体来说，LSTM 有几个特殊的「门」（gate）来控制信息的流动：</p><p><strong>1. 遗忘门（Forget Gate）</strong>：决定要忘记哪些信息。比如，不重要的剧情细节可以被遗忘。</p><p><strong>2. 输入门（Input Gate）</strong>：决定要记住哪些新的信息。比如，新出现的重要线索要记住。</p><p><strong>3. 输出门（Output Gate）</strong>：决定输出哪些信息。比如，根据前面的情节做出预测或解释当前情节。</p><p>通过这三个门的控制，LSTM 能够在时间序列数据中选择性地记住和忘记信息，从而在需要的时候准确地做出预测或分类。</p><p><strong>一个简单的例子</strong></p><p>想象一下，你在学习一门语言。刚开始学的时候，你会记住很多新的单词和语法（输入门打开），但随着学习的深入，你会逐渐忘记那些不常用的单词和语法（遗忘门打开）。当你在用这门语言交流时，你会根据上下文选择性地使用你记住的单词和语法（输出门打开）。</p><p>LSTM 就像是这样一个学习过程，能够灵活地记住重要信息并在需要的时候使用这些信息。</p><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="数学原理与公式推导"><a href="#数学原理与公式推导" class="headerlink" title="数学原理与公式推导"></a>数学原理与公式推导</h3><p>LSTM的核心是通过引入不同的“门”机制来控制信息的流动。这些“门”包括遗忘门、输入门和输出门。</p><h4 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h4><p>遗忘门决定了哪些信息需要丢弃。它的输出是一个介于0和1之间的向量，表示每个单元状态应该保留多少信息。</p><h4 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h4><p>输入门决定了哪些新的信息需要存储到单元状态中。</p><p>接着，会生成候选单元状态，它表示可以加入到单元状态中的新信息。</p><h4 id="更新单元状态"><a href="#更新单元状态" class="headerlink" title="更新单元状态"></a>更新单元状态</h4><p>通过遗忘门和输入门来更新单元状态。</p><h4 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h4><p>输出门决定当前单元状态的哪部分需要输出，并且通过一个激活函数（通常是tanh）处理后的结果作为输出。</p><blockquote><p>公式说明：</p><ul><li>：当前时刻的输入。</li><li>：前一时刻的隐状态。</li><li>：当前时刻的单元状态。</li><li>：前一时刻的单元状态。</li><li>：当前时刻的候选单元状态。</li><li>：遗忘门的激活值。</li><li>：输入门的激活值。</li><li>：输出门的激活值。</li><li>：表示sigmoid激活函数。</li><li>：表示tanh激活函数。</li><li>：权重矩阵。</li><li>：偏置向量。</li></ul></blockquote><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><strong>1. 输入预处理</strong>：</p><ul><li>取当前时刻的输入 和前一时刻的隐状态 。</li></ul><p><strong>2. 计算遗忘门</strong>：</p><ul><li>使用 和 计算遗忘门 。</li><li></li></ul><p><strong>3. 计算输入门</strong>：</p><ul><li>使用 和 计算输入门 。</li><li>。</li></ul><p><strong>4. 计算候选单元状态</strong>：</p><ul><li>计算新的候选单元状态 。</li><li>。</li></ul><p><strong>5. 更新单元状态</strong>：</p><ul><li>根据遗忘门和输入门的结果更新单元状态 。</li><li>。</li></ul><p><strong>6. 计算输出门</strong>：</p><ul><li>使用 和 计算输出门 。</li><li>。</li></ul><p><strong>7. 计算当前时刻的隐状态</strong>：</p><ul><li>通过输出门的结果和更新后的单元状态计算当前时刻的隐状态 。</li><li>。</li></ul><p>通过遗忘门、输入门和输出门的机制，LSTM能够有效地记住重要信息，忘记不必要的信息，从而在处理长时间依赖的序列数据时表现出色。每一个时间步的计算过程相对复杂，但通过这些步骤，LSTM可以在保持长期记忆和处理当前输入之间找到平衡。</p><h2 id="一个完整案例"><a href="#一个完整案例" class="headerlink" title="一个完整案例"></a>一个完整案例</h2><p>这里，咱们给到大家一个完整的、详细的LSTM应用示例。</p><p>这个案例中，使用电力消费数据集，该数据集包含自2011年开始的每小时电力消耗数据。目标是根据历史数据预测未来的电力消耗。</p><p>整个代码是完整的，大家可以粘贴在自己的编译器中进行调试，同时也做了很完整的注释供大家学习~</p><h3 id="数据集下载与预处理"><a href="#数据集下载与预处理" class="headerlink" title="数据集下载与预处理"></a>数据集下载与预处理</h3><p>大家可后台回复，“数据集”即可获取所有的数据集~</p><p>使用 <code>pandas</code> 处理数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 下载并读取数据</span><br><span class="line">df = pd.read_csv(&#x27;LD2011_2014.txt&#x27;, sep=&#x27;;&#x27;, index_col=0, parse_dates=True, decimal=&#x27;,&#x27;)</span><br><span class="line"></span><br><span class="line"># 选取其中一个列作为示例</span><br><span class="line">df = df[&#x27;MT_001&#x27;]</span><br><span class="line"></span><br><span class="line"># 处理数据：将数据按小时取平均值，并填补缺失值</span><br><span class="line">df = df.resample(&#x27;H&#x27;).mean().fillna(method=&#x27;ffill&#x27;)</span><br><span class="line"></span><br><span class="line"># 查看数据</span><br><span class="line">print(df.head())</span><br></pre></td></tr></table></figure><h3 id="创建时间序列数据"><a href="#创建时间序列数据" class="headerlink" title="创建时间序列数据"></a>创建时间序列数据</h3><p>创建时间序列数据，以便可以将其输入到LSTM模型中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 创建时间序列数据</span><br><span class="line">def create_dataset(data, time_step=1):</span><br><span class="line">    X, Y = [], []</span><br><span class="line">    for i in range(len(data) - time_step - 1):</span><br><span class="line">        a = data[i:(i + time_step)]</span><br><span class="line">        X.append(a)</span><br><span class="line">        Y.append(data[i + time_step])</span><br><span class="line">    return np.array(X), np.array(Y)</span><br><span class="line"></span><br><span class="line"># 使用过去24小时的数据预测下一小时的消耗</span><br><span class="line">time_step = 24</span><br><span class="line">data = df.values</span><br><span class="line"></span><br><span class="line"># 归一化数据</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line"></span><br><span class="line">scaler = MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">data = scaler.fit_transform(data.reshape(-1, 1))</span><br><span class="line"></span><br><span class="line">X, Y = create_dataset(data, time_step)</span><br><span class="line"></span><br><span class="line"># 划分训练集和测试集</span><br><span class="line">train_size = int(len(X) * 0.7)</span><br><span class="line">test_size = len(X) - train_size</span><br><span class="line">X_train, X_test = X[0:train_size], X[train_size:len(X)]</span><br><span class="line">Y_train, Y_test = Y[0:train_size], Y[train_size:len(Y)]</span><br><span class="line"></span><br><span class="line"># 重塑数据为 [样本, 时间步, 特征]</span><br><span class="line">X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)</span><br><span class="line">X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)</span><br></pre></td></tr></table></figure><h3 id="构建LSTM模型"><a href="#构建LSTM模型" class="headerlink" title="构建LSTM模型"></a>构建LSTM模型</h3><p>使用 <code>tensorflow</code> 构建并训练LSTM模型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.keras.models import Sequential</span><br><span class="line">from tensorflow.keras.layers import Dense, LSTM</span><br><span class="line"></span><br><span class="line"># 构建LSTM模型</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))</span><br><span class="line">model.add(LSTM(50, return_sequences=False))</span><br><span class="line">model.add(Dense(25))</span><br><span class="line">model.add(Dense(1))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;mean_squared_error&#x27;)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_test, Y_test))</span><br></pre></td></tr></table></figure><h3 id="预测和可视化结果"><a href="#预测和可视化结果" class="headerlink" title="预测和可视化结果"></a>预测和可视化结果</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 预测</span><br><span class="line">train_predict = model.predict(X_train)</span><br><span class="line">test_predict = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"># 反归一化预测结果</span><br><span class="line">train_predict = scaler.inverse_transform(train_predict)</span><br><span class="line">test_predict = scaler.inverse_transform(test_predict)</span><br><span class="line">Y_train = scaler.inverse_transform([Y_train])</span><br><span class="line">Y_test = scaler.inverse_transform([Y_test])</span><br><span class="line"></span><br><span class="line"># 可视化结果</span><br><span class="line">plt.figure(figsize=(14, 8))</span><br><span class="line">plt.plot(df.index[:len(Y_train[0])], Y_train[0], label=&#x27;Training Data&#x27;)</span><br><span class="line">plt.plot(df.index[len(Y_train[0]):len(Y_train[0]) + len(Y_test[0])], Y_test[0], label=&#x27;Test Data&#x27;)</span><br><span class="line">plt.plot(df.index[:len(train_predict)], train_predict, label=&#x27;Train Predict&#x27;)</span><br><span class="line">plt.plot(df.index[len(Y_train[0]):len(Y_train[0]) + len(test_predict)], test_predict, label=&#x27;Test Predict&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(&#x27;Date&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Power Consumption&#x27;)</span><br><span class="line">plt.title(&#x27;Electricity Consumption Prediction&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/kibwfTuPM4licXEYiaiafok1IK1FhiaWMF9Tfsjv3OibfQ5hYPJq3uIiczDSBMax3OKmzBtQsNBgic5NEAgDwSw9umrjqw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><h3 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h3><p>可以通过调整模型参数、使用更复杂的架构或更好的优化器来改进模型。</p><p>下面是一些改进模型的建议：</p><ul><li>增加LSTM层的单元数量或层数。</li><li>使用不同的激活函数。</li><li>尝试不同的优化器，如AdamW。</li><li>使用交叉验证进行超参数调优。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras.optimizers import Adam</span><br><span class="line"></span><br><span class="line"># 构建优化后的LSTM模型</span><br><span class="line">model_optimized = Sequential()</span><br><span class="line">model_optimized.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))</span><br><span class="line">model_optimized.add(LSTM(100, return_sequences=True))</span><br><span class="line">model_optimized.add(LSTM(100, return_sequences=False))</span><br><span class="line">model_optimized.add(Dense(50))</span><br><span class="line">model_optimized.add(Dense(1))</span><br><span class="line"></span><br><span class="line">optimizer = Adam(learning_rate=0.001)</span><br><span class="line">model_optimized.compile(optimizer=optimizer, loss=&#x27;mean_squared_error&#x27;)</span><br><span class="line"></span><br><span class="line"># 训练优化后的模型</span><br><span class="line">model_optimized.fit(X_train, Y_train, batch_size=64, epochs=20, validation_data=(X_test, Y_test))</span><br><span class="line"></span><br><span class="line"># 预测并可视化结果</span><br><span class="line">train_predict_optimized = model_optimized.predict(X_train)</span><br><span class="line">test_predict_optimized = model_optimized.predict(X_test)</span><br><span class="line"></span><br><span class="line">train_predict_optimized = scaler.inverse_transform(train_predict_optimized)</span><br><span class="line">test_predict_optimized = scaler.inverse_transform(test_predict_optimized)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(14, 8))</span><br><span class="line">plt.plot(df.index[:len(Y_train[0])], Y_train[0], label=&#x27;Training Data&#x27;)</span><br><span class="line">plt.plot(df.index[len(Y_train[0]):len(Y_train[0]) + len(Y_test[0])], Y_test[0], label=&#x27;Test Data&#x27;)</span><br><span class="line">plt.plot(df.index[:len(train_predict_optimized)], train_predict_optimized, label=&#x27;Optimized Train Predict&#x27;)</span><br><span class="line">plt.plot(df.index[len(Y_train[0]):len(Y_train[0]) + len(test_predict_optimized)], test_predict_optimized, label=&#x27;Optimized Test Predict&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(&#x27;Date&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Power Consumption&#x27;)</span><br><span class="line">plt.title(&#x27;Optimized Electricity Consumption Prediction&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/kibwfTuPM4licXEYiaiafok1IK1FhiaWMF9TfgtzzzTKibr4SjFk5devCS3z3XwBZicIQQthNDFSomkibZhLykH2BvVsBA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>通过以上所有的步骤，咱们成功地构建并优化了一个LSTM模型，预测电力消耗，并对结果进行了可视化，更加容易接受~</p><h2 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h2><p>这里，咱们从模型的优缺点、以及与相似算法的对比，讨论在什么情况下该算法是优选，什么情况下可以考虑其他算法。</p><h3 id="LSTM-模型-优缺点"><a href="#LSTM-模型-优缺点" class="headerlink" title="LSTM 模型 优缺点"></a>LSTM 模型 优缺点</h3><p><strong>优点</strong></p><p><strong>1. 处理长时间依赖性</strong>：LSTM可以有效地捕捉长时间依赖关系，在序列数据中能够记住和利用远距离的相关信息。</p><p><strong>2. 梯度消失问题</strong>：通过门机制（遗忘门、输入门、输出门），LSTM解决了传统RNN中的梯度消失问题，使得模型在训练时更稳定。</p><p><strong>3. 广泛适用</strong>：适用于各种时间序列数据，包括股票预测、天气预报、自然语言处理等。</p><p><strong>缺点</strong></p><p><strong>1. 计算复杂度高</strong>：LSTM结构复杂，训练时间长，尤其在大数据集上，计算资源消耗较大。</p><p><strong>2. 需要大量数据</strong>：LSTM需要大量的训练数据才能发挥出最佳效果，对小数据集的泛化能力较差。</p><p><strong>3. 参数调优复杂</strong>：LSTM有较多的超参数，模型优化需要进行大量的实验和调优，过程复杂且耗时。</p><h3 id="与相似算法的对比"><a href="#与相似算法的对比" class="headerlink" title="与相似算法的对比"></a>与相似算法的对比</h3><p><strong>LSTM vs. 简单RNN</strong></p><ul><li><strong>优点</strong>：LSTM能更好地处理长时间依赖关系，解决了简单RNN中的梯度消失问题。</li><li><strong>缺点</strong>：LSTM结构比简单RNN复杂，训练时间更长。</li></ul><p><strong>LSTM vs. GRU（门控循环单元）</strong></p><ul><li><strong>优点</strong>：LSTM通过三个门（遗忘门、输入门、输出门）控制信息流动，理论上可以捕捉更复杂的依赖关系。</li><li><strong>缺点</strong>：GRU只有两个门（更新门和重置门），结构较简单，计算量小于LSTM，但在很多实际应用中，GRU性能接近甚至优于LSTM。</li></ul><p><strong>LSTM vs. 一维卷积神经网络（1D-CNN）</strong></p><ul><li><strong>优点</strong>：LSTM适用于序列数据，能捕捉时间上的依赖关系。</li><li><strong>缺点</strong>：1D-CNN通过卷积操作捕捉局部时间特征，计算效率高于LSTM。在一些短时间依赖性较强的数据集上，1D-CNN可能表现更好。</li></ul><h3 id="选择LSTM的情境"><a href="#选择LSTM的情境" class="headerlink" title="选择LSTM的情境"></a>选择LSTM的情境</h3><p><strong>适用场景</strong></p><p><strong>1. 长时间依赖关系</strong>：需要捕捉数据中长期的依赖关系时，如自然语言处理中的句子理解，气象数据中的季节变化。</p><p><strong>2. 序列生成</strong>：生成类似文本、时间序列数据时，LSTM能够很好地建模数据的顺序和依赖关系。</p><p><strong>3. 大数据集</strong>：在有足够多训练数据的情况下，LSTM能充分学习复杂的模式和特征。</p><p><strong>考虑其他算法的情境</strong></p><p><strong>1. 短时间依赖关系</strong>：如果数据的依赖关系主要集中在短时间内，1D-CNN或简单RNN可能更适合。</p><p><strong>2. 计算资源有限</strong>：在计算资源受限的情况下，GRU或1D-CNN的计算效率更高。</p><p><strong>3. 小数据集</strong>：在数据量较小的情况下，较为简单的模型（如ARIMA，简单RNN）可能更适合，避免过拟合。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>LSTM在处理复杂的长时间序列数据方面表现出色，尤其适合需要捕捉长期依赖关系的任务。</p><p>但是，LSTM 复杂度和计算资源要求较高，需要大量的训练数据。与其他算法相比，LSTM在处理长时间依赖关系上有明显优势，但在短时间依赖关系或计算资源受限的情况下，其他算法如GRU、1D-CNN可能更为优选。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;咱们简单从 what、why、how三方面进行介绍~&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;what：一句话介绍&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;咱们一句话介绍：LSTM，全名是「长短期记忆网络」（Long Short-Term Memory），是一种特殊的人工神经网络，主要用来处</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="深度学习" scheme="http://ai.mak.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>5公里成绩跑进20分钟！</title>
    <link href="http://ai.mak.cn/2024/08/25/%E8%B7%91%E6%AD%A5/5%E5%85%AC%E9%87%8C%E6%88%90%E7%BB%A9%E8%B7%91%E8%BF%9B20%E5%88%86%E9%92%9F%EF%BC%81/"/>
    <id>http://ai.mak.cn/2024/08/25/%E8%B7%91%E6%AD%A5/5%E5%85%AC%E9%87%8C%E6%88%90%E7%BB%A9%E8%B7%91%E8%BF%9B20%E5%88%86%E9%92%9F%EF%BC%81/</id>
    <published>2024-08-24T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.775Z</updated>
    
    <content type="html"><![CDATA[<h1 id="5公里成绩跑进20分钟！"><a href="#5公里成绩跑进20分钟！" class="headerlink" title="5公里成绩跑进20分钟！"></a>5公里成绩跑进20分钟！</h1><p>5km，在跑者眼里是一个很有意义的路程。</p><p>5km，它不算长，也不算短，是很多跑者从初阶晋级到高阶的一个点。</p><p>唯有突破了5km，才能往更远的距离去跑，实现更大的突破。</p><p>5km，也是很多跑者需要着重训练的一个路程，既可以看出一个人的爆发力，还可以大概预测出你的半马、全马的成绩。</p><p>5km成绩如果能够突破20分钟，是很多业余跑者的梦想。</p><p>一旦5km跑进了20分钟，也就意味着你是业余跑者中的大神般的存在。</p><p><strong>贰</strong></p><p><strong>5km跑进20分，是什么概念？</strong></p><p>也就是意味着，你每公里的配速不能低于4分配速，也就是你的奔跑时速要达到15km&#x2F;h。</p><p>如果能够一直维持在4分配速，那么10km只需要40分钟，半马84分钟，全马2小时48分就能够跑完。</p><p>这在业余跑者中已经算是很高的成就了。</p><p>所以，以此类推，如果想要全马破3小时，你的5km速度就必须要破20分钟！</p><p><strong>叁</strong></p><p>下面介绍三种非常有效的训练方式，只要经过科学系统的训练，想要5km成绩突破20分，是很有希望的。</p><p><strong>1、加强力量训练：</strong></p><p>5km要破20分钟，需要你要有强大的肌肉力量。只有强有力的核心力量以及大小腿力量，你在奔跑的时候才能够完好的控制身体，不至于力量不足导致身体发软。</p><p>很多人在跑步时，往往都会忽视力量训练，认为我只要每天跑就行了。实际上这种认识是错误的。</p><p>在平时，有条件的可以到健身房进行锻炼，腰腹力量，背部力量，腿部力量都要锻炼到。</p><p>没有条件的，也要在家自己练习。比较实用的动作有深蹲、靠墙静蹲、卷腹、平板支撑等。</p><p><strong>2、练好有氧基础：</strong></p><p>想要跑得快，就必须要有足够的有氧能力。</p><p>有氧能力是决定你能跑多远的基础，但凡能够跑得又快又远的人，他的有氧基础都是响当当的。</p><p>在一周时间里，可以花个两三天时间来锻炼有氧基础。</p><p>有氧能力就是指在最大心率的60%强度下奔跑，距离可以控制在10km左右，长期坚持下去，个人的有氧能力会得到很大提升。</p><p><strong>3、提高乳酸阈值：</strong></p><p>想要跑得快，乳酸阈值一定要得到有效提高。</p><p>乳酸阈值是指人体在渐增负荷运动中，血乳酸浓度随运动负荷的渐增而增加，当运动强度达到某一负荷时，血乳酸浓度急剧上升，而在上升的这个起点就是阈值。</p><p>当我们跑得很快的时候，没过多久就会觉得大小腿发酸，就像灌铅了似的，这就是因为你体内的乳酸积累多了，所以就会非常容易感到疲惫。</p><p>而为了跑得更远跑得更快，我们就必须提高自己的乳酸阈值。</p><p>那么，如何才能提高乳酸阈值门槛呢？</p><p><strong>最简单有效的就是间歇跑和法特莱克跑。</strong></p><p>在有氧基础得到有效提高，肌肉力量也足够强大的时候，就可以开始练习间歇跑和冲刺跑了。因为间歇跑和冲刺跑强度都比较大，一星期练一次就够了。</p><p>所为法特莱克跑，是一种加速跑与慢跑交替进行的中长跑训练方法，在跑中插入一系列不定时间、不定距离的加速跑、反复跑甚至快速冲刺，使它们和慢跑或走步交替进行。法特莱克跑是无氧训练中经常用到的一个方法。</p><p>相信经过一段时间的训练，突破5公里成绩能够成功跑进20分钟，也是指日可待的事情。</p><p><strong>事实会证明，所有的辛苦和汗水，都不会白费。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;5公里成绩跑进20分钟！&quot;&gt;&lt;a href=&quot;#5公里成绩跑进20分钟！&quot; class=&quot;headerlink&quot; title=&quot;5公里成绩跑进20分钟！&quot;&gt;&lt;/a&gt;5公里成绩跑进20分钟！&lt;/h1&gt;&lt;p&gt;5km，在跑者眼里是一个很有意义的路程。&lt;/p&gt;
&lt;p&gt;5k</summary>
      
    
    
    
    <category term="跑步" scheme="http://ai.mak.cn/categories/%E8%B7%91%E6%AD%A5/"/>
    
    
    <category term="健康" scheme="http://ai.mak.cn/tags/%E5%81%A5%E5%BA%B7/"/>
    
  </entry>
  
  <entry>
    <title>提升跑步能力，这三项训练缺一不可</title>
    <link href="http://ai.mak.cn/2024/08/11/%E8%B7%91%E6%AD%A5/%E6%8F%90%E5%8D%87%E8%B7%91%E6%AD%A5%E8%83%BD%E5%8A%9B%EF%BC%8C%E8%BF%99%E4%B8%89%E9%A1%B9%E8%AE%AD%E7%BB%83%E7%BC%BA%E4%B8%80%E4%B8%8D%E5%8F%AF/"/>
    <id>http://ai.mak.cn/2024/08/11/%E8%B7%91%E6%AD%A5/%E6%8F%90%E5%8D%87%E8%B7%91%E6%AD%A5%E8%83%BD%E5%8A%9B%EF%BC%8C%E8%BF%99%E4%B8%89%E9%A1%B9%E8%AE%AD%E7%BB%83%E7%BC%BA%E4%B8%80%E4%B8%8D%E5%8F%AF/</id>
    <published>2024-08-10T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.776Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s/DfBvo1-iVbBgpoj1Dfwgyw">https://mp.weixin.qq.com/s/DfBvo1-iVbBgpoj1Dfwgyw</a></p></blockquote><h1 id="提升跑步能力，这三项训练缺一不可"><a href="#提升跑步能力，这三项训练缺一不可" class="headerlink" title="提升跑步能力，这三项训练缺一不可"></a>提升跑步能力，这三项训练缺一不可</h1><p>马拉松世界冠军基普乔格，全名埃利乌德·基普乔格，1984年11月5日出生于肯尼亚西部的裂谷省南迪郡，他的母亲是一位教师，父亲在他刚出生不久就去世了，基普乔格是家里四个孩子中最小的一个。</p><p>据说，基普乔格每天都要跑3公里去上学，因为迟到的学生，要接受老师的体罚。就这样，基普乔格一直跑到16岁，这也不知不觉中锻炼了他跑步的能力。</p><p>2000年，基普乔格结识了肯尼亚的奥运长跑冠军帕特里克·桑(Patrick Sang)。帕特里克·桑和基普乔格住处离得不远，基普乔格很想让帕特里克·桑为自己做一份训练计划。</p><p>而帕特里克·桑一眼就看中了这个跑步的苗子，在几次交流之后，帕特里克·桑就成为了基普乔格的教练。</p><p>而这也开启了属于基普大神的黄金20年时光。</p><p>2019年10月，基普乔格在奥地利维也纳的路拉特公园参加了Ineos 159挑战，再次向两小时的极限发起冲刺。最终以1小时59分40秒完成挑战，成为人类历史上，第一个跑进2小时的马拉松选手。</p><p>虽然这项成绩不被国际田联认可，但基普乔格还是创造了历史，成为人类体育史上的一大亮点。</p><p><strong>二、基普乔格推荐的3项身体素质训练</strong></p><p>我们都以为出生于肯尼亚的基普乔格，天生就是跑步的天才，实际上能够取得今天这样的成就，除了长跑的天赋，就是老天爷赏饭吃之外，也不能缺少他个人的刻苦训练。</p><p>如果我们以为基普乔格，只是靠跑步天分，就能够成为马拉松届的天花板，那么就误解了乔大侠颇为重视的身体素质训练。</p><p>以下分别罗列一下基普乔格推荐并且始终坚持的3项身体素质训练，相信也会对我们自身的跑步训练课有所启发和借鉴。</p><p>正如乔神的教练帕特里克·桑所言：“<strong>心理素质要比身体素质更重要，肯尼亚从不缺才华横溢的苗子，只有心理强大的人才会走的很远。</strong>”。</p><p>从2012年转战马拉松开始， 基普乔格已经驰骋马拉松赛场10年，创造了10年的辉煌，自律和坚持才是基普乔格成功的关键。</p><p><strong>素质训练①：肌肉力量训练</strong></p><p>1、深蹲</p><p>主要用来发展臀部和大腿前侧的力量</p><p>2、弓箭步提膝</p><p>可以模拟跑步发力，增强臀腿肌肉力量</p><p>3、单腿硬拉</p><p>可以增强腘绳肌耐力，增强髋外旋外展肌肉耐力，同时，维持跑步过程中髋关节的稳定性</p><p>4、单腿提踵</p><p>可有效增强脚踝和小腿的肌肉耐力</p><p><strong>素质训练②：核心训练</strong></p><p>1、平板交替抬腿</p><p>可以模拟跑步伸髋动作，增加腹横肌稳定和身体抗旋转的能力</p><p>2、仰卧蹬自行车</p><p>可以增加腹部内外斜肌肌肉耐力，好的跑步姿势，离不开强大的核心肌肉群。</p><p>3、俯身登山跑</p><p>可以模拟跑步发力，提升腹部肌肉和腰大肌肌肉耐力</p><p>4、摸膝卷腹</p><p>可以很好地刺激到腹部肌肉群，增强核心肌肉能力</p><p><strong>素质训练③：跑步专项技术</strong></p><p>1、垫歩提膝</p><p>可有效提升髋关节灵活性</p><p>2、折叠腿</p><p>学会脚跟拉向臀部，增大跑步的车轮半径</p><p>3、直腿跑</p><p>学会用髋关节发力，脚掌落地下压并快速提拉</p><p>4、后蹬跑</p><p>可以提升肌肉弹性以及爆发力，增强腿部韧带刚性，可以有效增大步幅</p><p>基普乔格的马拉松生涯并没有划上句号，正如他所说：<strong>记录就是应该被打破的。</strong></p><p>希望乔大侠能够继续挑战新的世界记录。即使无法再创造之前的运动传奇，也不会影响我们对这位伟大运动员的尊重和热爱。</p><p>乔大侠的成功，与严格的自律和坚持是分不开的，最后以他的一句名言结尾，希望我们也能够通过适合自己强度的训练，能够有所收获。</p><p><strong>“人人皆可超越自己的极限！”</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/DfBvo1-iVbBgpoj1Dfwgyw&quot;&gt;https://mp.weixin.qq.com/s/DfBvo1-iVbBgpoj1Dfwgyw&lt;/a&gt;&lt;/p&gt;
&lt;/bloc</summary>
      
    
    
    
    <category term="跑步" scheme="http://ai.mak.cn/categories/%E8%B7%91%E6%AD%A5/"/>
    
    
    <category term="健康" scheme="http://ai.mak.cn/tags/%E5%81%A5%E5%BA%B7/"/>
    
  </entry>
  
  <entry>
    <title>一米奇迹：怎样让你的工位更健康？</title>
    <link href="http://ai.mak.cn/2024/08/03/%E6%9D%82%E8%AE%B0/%E4%B8%80%E7%B1%B3%E5%A5%87%E8%BF%B9%EF%BC%9A%E6%80%8E%E6%A0%B7%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%B7%A5%E4%BD%8D%E6%9B%B4%E5%81%A5%E5%BA%B7%EF%BC%9F/"/>
    <id>http://ai.mak.cn/2024/08/03/%E6%9D%82%E8%AE%B0/%E4%B8%80%E7%B1%B3%E5%A5%87%E8%BF%B9%EF%BC%9A%E6%80%8E%E6%A0%B7%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%B7%A5%E4%BD%8D%E6%9B%B4%E5%81%A5%E5%BA%B7%EF%BC%9F/</id>
    <published>2024-08-02T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.704Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>节选自：得到头条 | 377期</p></blockquote><p>今天，我将从两个话题出发，为你提供知识服务。第一个是，全国第36个爱国卫生健康月活动落下帷幕。第二个是，2024年一季度，可可价格大幅上涨。</p><p>先来看今天的第一条。刚刚结束的4月，是全国第36个爱国卫生健康月，今年的主题是“健康城镇，健康体重”。很多地方都组织了相关的活动，包括倡导绿色出行、推广卫生习惯，还有针对上班族的运动建议，等等。</p><p>但是，你也知道，这些建议对特别忙的职场人来说，落实起来多少有点难度。你在办公室里度过了最多的时间，但这部分时间的健康体验，往往很难保证。</p><p>因此今天，借着这个话题，我们就说几个，在办公室就能执行的健康方案。</p><p>这些方法来自斯坦福大学医学院的神经科学家安德鲁·休伯曼。他的主要研究课题之一，就是借用神经科学的原理，给上班族设计办公室里的健康方案。休伯曼还把这些内容做成了一个播客，叫休伯曼实验室，有50多万名用户。</p><p>我们给这些方案起了个名字，叫一米奇迹。也就是，在你工位一米之内的范围就能完成，并且效果不错的方法。掌握这些方法既能提升你的工作体验，让人身心变得更健康，同时也能提升工作效率。好，咱们正式开始。</p><p>先介绍个背景信息，休伯曼教授从神经科学的角度，把一天分为三个阶段，这三个阶段你身体内部的激素水平不同，工作状态也不同。</p><p>第一个阶段，是从起床开始算起的7到8小时，也就是从早上到下午的这段时间。这时你体内的多巴胺、去甲肾上腺素、皮质醇的含量都在一个比较高的状态。你整个人也高度专注，适合做逻辑分析之类的工作。咱们可以把这个阶段叫做“分析期”。</p><p>第二个阶段，是起床后的8到16个小时，也就是下午和傍晚。这时你体内的血清素等神经调节剂含量会提高，这些激素会提高大脑的抽象思维能力，适合做一些创造性的工作。咱们可以把这个阶段叫做“创意期”。</p><p>第三个阶段，就是睡前和睡眠阶段，这个时候，最关键的就是好好休息。</p><p>咱们的工作时间主要集中在前两个阶段，也就是，分析阶段和创意阶段。休伯曼教授说，你可以针对性地做一些调节，让这两个阶段的工作更健康。这些调节主要分三种。</p><p>第一种，是调节光照。</p><p>按照上面的分类，<strong>在第一阶段，也就是起床后的7到8小时，你应该尽可能地照亮自己的工作环境，促进体内多巴胺、去甲肾上腺素和皮质醇的释放。</strong> 这能让你更清醒、更专注。假如你在室内工作，最好的座位是自然光线充足的窗户边，而且最好打开窗户，因为玻璃会过滤掉自然光中50%的蓝光，而蓝光对神经的唤醒作用是非常重要的。假如不在窗边，你也要让自己的办公环境足够亮。并且，注意，要打开头顶的灯。<strong>头顶的灯光会直接刺激你眼睛里的一组神经元细胞，叫做“黑质神经节细胞”，这可以让你的大脑更敏锐。</strong></p><p>同时，你还可以通过一个小调整，帮自己保持这种敏锐。这就是，把电脑屏幕，调高一点。视觉焦点，也就是眼神对焦的地方，会对神经产生不同的影响。当你向上看时，神经会变得兴奋，向下看时，你会变得平静甚至犯困。因此，<strong>你可以把电脑高度调高，调到和眼睛平齐，或者更高一点的地方，让自己的视觉焦点在斜上方。这能帮助你保持专注。</strong></p><p>而到了第二阶段，也就是下午3点之后，这时你的抽象思考开始活跃，适合做一些创意性的工作。这时，你需要降低光照强度，可以关掉头顶的主灯，留一些环境灯。光线稍微暗一点，有利于体内血清素的释放，更适合去做一些创造性的工作。</p><p>总之，光照这方面，其实不复杂，主要就是遵循自然规律，早上强，下午弱，天黑就休息。</p><p>休伯曼说的第二种，是调节空间范围。这里他引用很有名的“大教堂效应”，也就是，建筑的高度，会对人的思考模式产生影响。<strong>在天花板低的空间，人更善于处理具象的东西，也就是更有分析能力，而在天花板高的空间里，人更善于处理抽象的东西，也就是更有创意。</strong></p><p>因此，还是按照前面的阶段，起床后的7到8小时，你可以在一个小空间里干活，保持高度的专注力，处理需要逻辑分析的工作。假如环境不允许，比如你就是得处在一个空旷嘈杂的环境里，那么休伯曼说，你可以人为地给自己创造一个小空间。他观察到，斯坦福附近有很多在咖啡厅工作和学习的人，会穿帽衫，戴帽子，这就是人为地给自己创造一个低天花板的小空间。</p><p>而到了第二个阶段，创意工作期，你就可以找一个空间大的地方，去处理偏抽象的工作。</p><p>同时，休伯曼还提到一个因素，叫双耳频率。也就是两只耳朵同时听一段声音，但两只耳朵听到的频率有所不同。当你听这种双耳频率的时候，大脑会慢慢调节出一种特定的专注状态。休伯曼说，听30分钟左右的40赫兹双耳频率，对工作的效果最好。相关的音频网上就有，你可以找来试试看。</p><p>最后咱们接着来看休伯曼说的第三种方法，是调节身体姿势。</p><p>这个方法不算新。关于坐着办公好，还是站着办公好，咱们之前也说过，单独采用哪种姿势，都不如交替着来好，一会儿站着，一会儿坐着。有研究指出，<strong>假如每天有一半以上的时间站着工作，就可以减缓脖子和肩膀的疼痛，改善驼背，并且能让你燃烧更多的卡路里。</strong></p><p>不过，这都是从身体健康层面来理解这个问题，休伯曼提醒，这对大脑的效率也有所影响。人在躺着的时候，警觉性是最低的，坐着次之，而当你站起来或者动起来的时候，大脑中的“蓝斑神经元”会变得活跃，它能促进多巴胺和肾上腺素的释放，让我们变得更加警觉。因此，站着办公和坐着办公五五开，比如半个小时交换一次，效果是最好的。另外，尤其要注意的一点是，休伯曼说，当你站着的时候，要注意保持独立，不要靠在桌子上。</p><p>好，以上就是休伯曼提供的行动清单。简单总结一下，在上午和中午的工作中，你可以打开顶灯，把显示器抬高，在小空间里办公。假如想更专注，你可以戴上帽子，听点儿双耳频率。下午和傍晚，你可以把光线调暗，在大空间里办公。同时，一天中，你最好时常站起来，保证有一半的时间站着办公。最后，也祝你有一个高效健康的办公空间。</p><p>再来看今天的第二条。最近，可可豆又涨价了。就在第一季度，美国可可豆期货的价格突破一万美元，即便近期有回落的迹象，也稳定在9700美元&#x2F;吨左右。英国可可豆期货的价格也同样上涨，最高时8672英镑&#x2F;吨，创下历史新高。根据“彭博社”的报道，从20世纪80年代以来，可可豆的交易价格基本保持在3500美元&#x2F;吨左右。而在今年第一季度，价格却涨了好几倍。</p><p>可可豆涨价会带来哪些影响？</p><p>第一，巧克力可能会涨价。比如，瑞士的巧克力品牌瑞士莲，去年上半年价格就上涨了9%左右。再比如，北美最大的巧克力及糖果制造商好时，2023年第三季度的价格同比上涨了11%左右。再比如，今年，雀巢、费列罗等国际知名巧克力品牌都陆续宣布涨价计划。</p><p>第二，未来巧克力的包装尺寸可能会越来越小，可可的纯度可能也会越来越低。根据“大洋网”的报道，在人均巧克力消费排名全球第一的瑞士，2023年瑞士人均购买的巧克力是11公斤，跟2022年相比有所下降。再比如，根据美国糖果协会的数据显示，2023年，美国的巧克力销量，跟2022年比萎缩了5.3%。换句话说，消费者对巧克力价格的反应很敏感。因此，为了保持市场竞争力，很多品牌开始从包装着手，也就是通过更小的包装，降低价格来维持销量。</p><p>第三，可可豆价格持续上涨，可能会导致可可添加剂产业增长。说白了，就是给可可脂找平替。比如，代可可脂。原来，代可可脂因为含有反式脂肪酸，在市面上不太受欢迎。但由于这次的可可豆危机，代可可脂又出现了翻红的迹象。</p><p>但话说回来，为什么这两年可可豆一直在涨价呢？</p><p>表面上看，是因为极端天气。也就是从2023年5月开始的厄尔尼诺现象，导致可可豆产量下降。再加上可可豆的种植地很集中。目前全世界75%的可可豆，集中在非洲的加纳、喀麦隆、科特迪瓦以及尼日利亚。这就好比鸡蛋都装在同一个篮子里，抗风险能力自然有限。</p><p>但是，假如深入观察，影响可可豆价格的，还有另一个更关键的原因，这就是金融因素。</p><p>作为全球交易量大的农产品之一，可可豆也是种期货。由于全球灾害性天气频发，再加上，可可树本身种植3到5年后才能结果。因此，欧美国家的期货市场就预测可可豆的产量很难在短期内提高，价格也会持续上涨。</p><p>比如，根据《金融时报》的报道，对冲基金们已经在伦敦和纽约的可可期货合约中，投入了将近87亿美元，主要就是押注未来可可豆的价格会继续上涨。</p><p>再比如，根据“彭博社”的报道，还有一些公司会采取“套期保值”策略，也就是在买进可可豆本身的同时，会买入同等数量的期货合约，来降低这笔货物的赔钱风险。</p><p>这些金融市场的行为，会进一步放大气候变化对可可豆价格造成的影响。</p><p>之前，薛兆丰老师在他的经济学课中就讲过，对未来天气变化的趋势，期货市场比国家气象局还要敏感。其中的道理就是，对气候的预测，会影响对农产品产量的预测，从而直接影响期货市场对价格的预测。</p><p>你看，说到这，我们又回到了经济学的视角。借用薛兆丰老师的话说，<strong>经济学研究的并不是关于钱的知识，而是真实世界的运行规律，是人与人之间的协作方式。而一切自然因素带来的影响，都会被人类的协作网络放大。</strong> 从这个角度看，懂一点经济学，是不是很有必要？最近，薛兆丰老师的经济学课又有加餐，假如你对经济学感兴趣，这门课推荐你来看一看。</p><p>最后，总结一下，今天说了两个话题。</p><p>第一，怎样借助神经科学提升工作健康度？我们说了一组很具体的建议。包括，上午找一个小点的空间，把光线调亮，把显示器抬高，并多做逻辑性的工作。下午反过来，把光线调暗，找一个大点的空间，多做创意性的工作。</p><p>第二，可可豆价格为什么上涨？自然因素只是一个基础，关键是人的预期，给这个基础提供了杠杆。换句话说，决定涨价的不仅是产量，更是人的预期。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;节选自：得到头条 | 377期&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今天，我将从两个话题出发，为你提供知识服务。第一个是，全国第36个爱国卫生健康月活动落下帷幕。第二个是，2024年一季度，可可价格大幅上涨。&lt;/p&gt;
&lt;p&gt;先来看今天的第一条</summary>
      
    
    
    
    <category term="杂记" scheme="http://ai.mak.cn/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
    <category term="杂记" scheme="http://ai.mak.cn/tags/%E6%9D%82%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>微服务核心架构梳理</title>
    <link href="http://ai.mak.cn/2024/07/28/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/%E9%80%9A%E7%94%A8%E7%9A%84%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    <id>http://ai.mak.cn/2024/07/28/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/%E9%80%9A%E7%94%A8%E7%9A%84%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/</id>
    <published>2024-07-27T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.687Z</updated>
    
    <content type="html"><![CDATA[<h1 id="通用的高并发架构设计"><a href="#通用的高并发架构设计" class="headerlink" title="通用的高并发架构设计"></a>通用的高并发架构设计</h1><p>关键词：读&#x2F;写分离、数据缓存、缓存更新、CQRS、数据分片、异步写</p><p>本文节选自电子工业出版社博文视点刚刚出版的<strong>《亿级流量系统架构设计与实战》</strong>一书。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><blockquote><p>链接 <a href="https://mp.weixin.qq.com/s/KcPIR5jB6bJUGpEzcmlAGA">https://mp.weixin.qq.com/s/KcPIR5jB6bJUGpEzcmlAGA</a></p></blockquote><p><strong>高并发架构设计的要点</strong></p><p>高并发意味着系统要应对海量请求。从笔者多年的面试经验来看，很多面试者在面对“什么是高并发架构”的问题时，往往会粗略地认为一个系统的设计是否满足高并发架构，就是看这个系统是否可以应对海量请求。再细问具体的细节时，回答往往显得模棱两可，比如每秒多少个请求才是高并发请求、系统的性能表现如何、系统的可用性表现如何，等等。</p><p>为了可以清晰地评判一个系统的设计是否满足高并发架构，在正式给出通用的高并发架构设计方案前，我们先要厘清形成高并发系统的必要条件、高并发系统的衡量指标和高并发场景分类。</p><h3 id="形成高并发系统的必要条件"><a href="#形成高并发系统的必要条件" class="headerlink" title="形成高并发系统的必要条件"></a><strong>形成高并发系统的必要条件</strong></h3><p><strong>◎高性能：</strong>性能代表一个系统的并行处理能力，在同样的硬件设备条件下，性能越高，越能节约硬件资源；同时性能关乎用户体验，如果系统响应时间过长，用户就会产生抱怨。</p><p><strong>◎高可用性：</strong>系统可以长期稳定、正常地对外提供服务，而不是经常出故障、宕机、崩溃。</p><p><strong>◎可扩展性：</strong>系统可以通过水平扩容的方式，从容应对请求量的日渐递增乃至突发的请求量激增。</p><p>我们可以将形成高并发系统的必要条件类比为一个篮球运动员的各项属性：“高性能”相当于这个球员在赛场上的表现力强，“高可用性”相当于这个球员在赛场上总可以稳定发挥，“可扩展性”相当于这个球员的未来成长性好。</p><p><strong>高并发系统的衡量指标</strong></p><h4 id="1-高性能指标"><a href="#1-高性能指标" class="headerlink" title="1. 高性能指标"></a><strong>1. 高性能指标</strong></h4><p>一个很容易想到的可以体现系统性能的指标是，在一段时间内系统的平均响应时间。例如，在一段时间内有10000个请求被成功响应，那么在这段时间内系统的平均响应时间是这10000个请求响应时间的平均值。</p><p>然而，平均值有明显的硬伤并在很多数据统计场景中为大家所调侃。假设你和传奇篮球巨星姚明被分到同一组，你的身高是174cm，姚明的身高是226cm，那么这组的平均身高是2m！这看起来非常不合理。假设在10000个请求中有9900个请求的响应时间分别是1ms，另外100个请求的响应时间分别是100ms，那么平均响应时间仅为1.99ms，完全掩盖了那100个请求的100ms响应时间的问题。平均值的主要缺点是易受极端值的影响，这里的极端值是指偏大值或偏小值——当出现偏大值时，平均值将会增大；当出现偏小值时，平均值将会减小。</p><p>笔者推荐的系统性能的衡量指标是响应时间PCTn统计方式，PCTn表示请求响 应时间按从小到大排序后第n分位的响应时间。假设在一段时间内100个请求的响应时间从小到大排序如图所示，则第99分位的响应时间是100ms，即PCT99&#x3D; 100ms。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/PW0wIHxgg3kOkTPoPAr4hibU1UzUVsKg2tQruDR4cyliaic6QlVKSvjFMw9bVJlSuywW51hczIaQ3uB1adRXjGOkw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>分位值越大，对响应时间长的请求越敏感。比如统计10000个请求的响应时间：</p><p>◎PCT50&#x3D;1ms，表示在10000个请求中50%的请求响应时间都在1ms以内。</p><p>◎PCT99&#x3D;800ms，表示在10000个请求中99%的请求响应时间都在800ms以内。</p><p>◎PCT999&#x3D;1.2s，表示在10000个请求中99.9%的请求响应时间都在1.2s以内。</p><p>从笔者总结的经验数据来看，请求的平均响应时间&#x3D;200ms，且PCT99&#x3D;1s的高并发系统基本能够满足高性能要求。如果请求的响应时间在200ms以内，那么用户不会感受到延迟；而如果请求的响应时间超过1s，那么用户会明显感受到延迟。</p><h4 id="2-高可用性指标"><a href="#2-高可用性指标" class="headerlink" title="2. 高可用性指标"></a><strong>2. 高可用性指标</strong></h4><p>可用性&#x3D;系统正常运行时间&#x2F;系统总运行时间，表示一个系统正常运行的时间占比，也可以将其理解为一个系统对外可用的概率。我们一般使用N个9来描述系统的可用性如何，如表所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/PW0wIHxgg3kOkTPoPAr4hibU1UzUVsKg2stWwyc9UN5PY33xWAkR2w0hRguae7icvIVBUCb3LiaQg2NPgpRRtR58w/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>高可用性要求系统至少保证3个9或4个9的可用性。在实际的系统指标监控中，很多公司会取3个9和4个9的中位数：99.95%（3个9、1个5），作为系统可用性监控的阈值。当监控到系统可用性低于99.95%时及时发出告警信息，以便系统维护者可以及时做出优化，如系统可用性补救、扩容、分析故障原因、系统改造等。</p><h4 id="3-可扩展性指标"><a href="#3-可扩展性指标" class="headerlink" title="3. 可扩展性指标"></a><strong>3. 可扩展性指标</strong></h4><p>面对到来的突发流量，我们明显来不及对系统做架构改造，而更快捷、有效的做法是增加系统集群中的节点来水平扩展系统的服务能力。可扩展性&#x3D;吞吐量提升比例&#x2F;集群节点增加比例。在最理想的情况下，集群节点增加几倍，系统吞吐量就能增加几倍。一般来说，拥有70%～80%可扩展性的系统基本能够满足可扩展性要求。</p><p><strong>高并发场景分类</strong></p><p>我们使用计算机实现各种业务功能，最终将体现在对数据的两种操作上，即读和写，于是高并发请求可以被归类为高并发读和高并发写。比如有的业务场景读多写少，需要重点解决高并发读的问题；有的业务场景写多读少，需要重点解决高并发写的问题；而有的业务场景读多写多，则需要同时解决高并发读和高并发写的问题。将高并发场景划分为高并发读场景和高并发写场景，是因为在这两种场景中往往有不同的高并发解决方案。</p><hr><p><strong>数据库读&#x2F;写分离</strong></p><p>大部分互联网应用都是读多写少的，比如刷帖的请求永远比发帖的请求多，浏览商品的请求永远比下单购买商品的请求多。数据库承受的高并发请求压力，主要来自读请求。我们可以把数据库按照读&#x2F;写请求分成专门负责处理写请求的数据库（写库）和专门负责处理读请求的数据库（读库），让所有的写请求都落到写库，写库将写请求处理后的最新数据同步到读库，所有的读请求都从读库中读取数据。这就是数据库读&#x2F;写分离的思路。</p><p>数据库读&#x2F;写分离使大量的读请求从数据库中分离出来，减少了数据库访问压力，缩短了请求响应时间。</p><p><strong>读&#x2F;写分离架构</strong></p><p>我们通常使用数据库主从复制技术实现读&#x2F;写分离架构，将数据库主节点Master作为“写库”，将数据库从节点Slave作为“读库”，一个Master可以与多个Slave连接，如图所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/PW0wIHxgg3kOkTPoPAr4hibU1UzUVsKg2cib2X6fOvlu35U5MGlqZ6oe8eeSWK1G2ZWSyNg2LW74ozZKJKibbIrpQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>市面上各主流数据库都实现了主从复制技术。</p><h3 id="读-x2F-写请求路由方式"><a href="#读-x2F-写请求路由方式" class="headerlink" title="读&#x2F;写请求路由方式"></a><strong>读&#x2F;写请求路由方式</strong></h3><p>在数据库读&#x2F;写分离架构下，把写请求交给Master处理，而把读请求交给Slave处理，那么由什么角色来执行这样的读&#x2F;写请求路由呢？一般可以采用如下两种方式。</p><h4 id="1-基于数据库Proxy代理的方式"><a href="#1-基于数据库Proxy代理的方式" class="headerlink" title="1. 基于数据库Proxy代理的方式"></a><strong>1. 基于数据库Proxy代理的方式</strong></h4><p>在业务服务和数据库服务器之间增加数据库Proxy代理节点（下文简称Proxy），业务服务对数据库的一切操作都需要经过Proxy转发。Proxy收到业务服务的数据库操作请求后，根据请求中的SQL语句进行归类，将属于写操作的请求（如insert&#x2F;delete&#x2F;update语句）转发到数据库Master，将属于读操作的请求（如select语句）转发到数据库任意一个Slave，完成读&#x2F;写分离的路由。开源项目如中心化代理形式的MySQL-Proxy和MyCat，以及本地代理形式的MySQL-Router等都实现了读&#x2F;写分离功能。</p><h4 id="2-基于应用内嵌的方式"><a href="#2-基于应用内嵌的方式" class="headerlink" title="2. 基于应用内嵌的方式"></a><strong>2. 基于应用内嵌的方式</strong></h4><p>基于应用内嵌的方式与基于数据库Proxy代理的方式的主要区别是，它在业务服务进程内进行请求读&#x2F;写分离，数据库连接框架开源项目如gorm、shardingjdbc等都实现了此形式的读&#x2F;写分离功能。</p><h3 id="主从延迟与解决方案"><a href="#主从延迟与解决方案" class="headerlink" title="主从延迟与解决方案"></a><strong>主从延迟与解决方案</strong></h3><p>数据库读&#x2F;写分离架构依赖数据库主从复制技术，而数据库主从复制存在数据复制延迟（主从延迟），因此会导致在数据复制延迟期间主从数据的不一致，Slave获取不到最新数据。针对主从延迟问题有如下三种解决方案。</p><h4 id="1-同步数据复制"><a href="#1-同步数据复制" class="headerlink" title="1. 同步数据复制"></a><strong>1. 同步数据复制</strong></h4><p>数据库主从复制默认是异步模式，Master在写完数据后就返回成功了，而不管Slave是否收到此数据。我们可以将主从复制配置为同步模式，Master在写完数据后，要等到全部Slave都收到此数据后才返回成功。</p><p>这种方案可以保证数据库每次写操作成功后，Master和Slave都能读取到最新数据。这种方案相对简单，将数据库主从复制修改为同步模式即可，无须改造业务服务。</p><p>但是由于在处理业务写请求时，Master要等到全部Slave都收到数据后才能返回成功，写请求的延迟将大大增加，数据库的吞吐量也会有明显的下滑。这种方案的实用价值较低，仅适合在低并发请求的业务场景中使用。</p><h4 id="2-强制读主"><a href="#2-强制读主" class="headerlink" title="2. 强制读主"></a><strong>2. 强制读主</strong></h4><p>不同的业务场景对主从延迟的容忍性不一样。例如，用户a刚刚发布了一条状态，他浏览个人主页时应该展示这条状态，这个场景不太能容忍主从延迟；而好友用户b此时浏览用户a的个人主页时，可以暂时看不到用户a最新发布的状态，这个场景可以容忍主从延迟。我们可以对业务场景按照主从延迟容忍性的高低进行划分，对于主从延迟容忍性高的场景，执行正常的读&#x2F;写分离逻辑；而对于主从延迟容忍性低的场景，强制将读请求路由到数据库Master，即强制读主。</p><h4 id="3-会话分离"><a href="#3-会话分离" class="headerlink" title="3. 会话分离"></a><strong>3. 会话分离</strong></h4><p>比如某会话在数据库中执行了写操作，那么在接下来极短的一段时间内，此会话的读请求暂时被强制路由到数据库Master，与“强制读主”方案中的例子很像，保证每个用户的写操作立刻对自己可见。暂时强制读主的时间可以被设定为略高于数据库完成主从数据复制的延迟时间，尽量使强制读主的时间段覆盖主从数据复制的实际延迟时间。</p><hr><p><strong>本地缓存</strong></p><h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><p>在计算机世界中，缓存（Cache）无处不在，如CPU缓存、DNS缓存、浏览器缓存等。值得一提的是，Cache在我国台湾地区被译为“快取”，更直接地体现了它的用途：快速读取。缓存的本质是通过空间换时间的思路来保证数据的快速读取。</p><p>业务服务一般需要通过网络调用向其他服务或数据库发送读数据请求。为了提高数据的读取效率，业务服务进程可以将已经获取到的数据缓存到本地内存中，之后业务服务进程收到相同的数据请求时就可以直接从本地内存中获取数据返回，将网络请求转化为高效的内存存取逻辑。这就是本地缓存的主要用途。在本书后面的核心服务设计篇中会大量应用本地缓存，本节先重点介绍本地缓存的技术原理。</p><p><strong>基本的缓存淘汰策略</strong></p><p>虽然缓存使用空间换时间可以提高数据的读取效率，但是内存资源的珍贵决定了本地缓存不可无限扩张，需要在占用空间和节约时间之间进行权衡。这就要求本地缓存能自动淘汰一些缓存的数据，淘汰策略应该尽量保证淘汰不再被使用的数据，保证有较高的缓存命中率。基本的缓存淘汰策略如下。</p><p><strong>◎FIFO（First In First Out）策略：</strong>优先淘汰最早进入缓存的数据。这是最简单的淘汰策略，可以基于队列实现。但是此策略的缓存命中率较低，越是被频繁访问的数据是越早进入队列的，于是会被越早地淘汰。此策略在实践中很少使用。</p><p><strong>◎LFU（Least Frequently Used）策略：</strong>优先淘汰最不常用的数据。LFU策略会为每条缓存数据维护一个访问计数，数据每被访问一次，其访问计数就加1，访问计数最小的数据是被淘汰的目标。此策略很适合缓存在短时间内会被频繁访问的热点数据，但是最近最新缓存的数据总会被淘汰，而早期访问频率高但最近一直未被访问的数据会长期占用缓存。</p><p><strong>◎LRU（Least Recent Used）策略：</strong>优先淘汰缓存中最近最少使用的数据。此策略一般基于双向链表和哈希表配合实现。双向链表负责存储缓存数据，并总是将最近被访问的数据放置在尾部，使缓存数据在双向链表中按照最近访问时间由远及近排序，每次被淘汰的都是位于双向链表头部的数据。哈希表负责定位数据在双向链表中的位置，以便实现快速数据访问。此策略可以有效提高短期内热点数据的缓存命中率，但如果是偶发性地访问冷数据，或者批量访问数据，则会导致热点数据被淘汰，进而降低缓存命中率。</p><p>LRU策略和LFU策略的缺点是都会导致缓存命中率大幅下降。近年来，业界出现了一些更复杂、效果更好的缓存淘汰策略，比如W-TinyLFU策略。</p><hr><p><strong>分布式缓存</strong></p><h3 id="-2"><a href="#-2" class="headerlink" title=""></a></h3><p>由于本地缓存把数据缓存在服务进程的内存中，不需要网络开销，故而性能非常高。但是把数据缓存到内存中也有较多限制，举例如下。</p><p><strong>◎无法共享：</strong>多个服务进程之间无法共享本地缓存。</p><p><strong>◎编程语言限制：</strong>本地缓存与程序绑定，用Golang语言开发的本地缓存组件不可以直接为用Java语言开发的服务器所使用。</p><p><strong>◎可扩展性差：</strong>由于服务进程携带了数据，因此服务是有状态的。有状态的服务不具备较好的可扩展性。</p><p><strong>◎内存易失性：</strong>服务进程重启，缓存数据全部丢失。</p><p>我们需要一种支持多进程共享、与编程语言无关、可扩展、数据可持久化的缓存，这种缓存就是分布式缓存。</p><h3 id="分布式缓存选型"><a href="#分布式缓存选型" class="headerlink" title="分布式缓存选型"></a><strong>分布式缓存选型</strong></h3><p>主流的分布式缓存开源项目有Memcached和Redis，两者都是优秀的缓存产品，并且都具有缓存数据共享、与编程语言无关的能力。不过，相对于Memcached而言，Redis更为流行，主要体现如下。</p><p><strong>◎数据类型丰富：</strong>Memcached仅支持字符串数据类型缓存，而Redis支持字符串、列表、集合、哈希、有序集合等数据类型缓存。</p><p><strong>◎数据可持久化：</strong>Redis通过RDB机制和AOF机制支持数据持久化，而Memcached没有数据持久化能力。</p><p><strong>◎高可用性：</strong>Redis支持主从复制模式，在服务器遇到故障后，它可以通过主从切换操作保证缓存服务不间断。Redis具有较高的可用性。</p><p><strong>◎分布式能力：</strong>Memcached本身并不支持分布式，因此只能通过客户端，以一致性哈希这样的负载均衡算法来实现基于Memcached的分布式缓存系统。而Redis有官方出品的无中心分布式方案Redis Cluster，业界也有豆瓣Codis和推特Twemproxy的中心化分布式方案。</p><p>由于Redis支持丰富的数据类型和数据持久化，同时拥有高可用性和高可扩展性，因此它成为大部分互联网应用分布式缓存的首选。</p><h3 id="如何使用Redis缓存"><a href="#如何使用Redis缓存" class="headerlink" title="如何使用Redis缓存"></a><strong>如何使用Redis缓存</strong></h3><p>使用Redis缓存的逻辑如下。</p><p>（1）尝试在Redis缓存中查找数据，如果命中缓存，则返回数据。</p><p>（2）如果在Redis缓存中找不到数据，则从数据库中读取数据。</p><p>（3）将从数据库中读取到的数据保存到Redis缓存中，并为此数据设置一个过期时间。</p><p>（4）下次在Redis缓存中查找同样的数据，就会命中缓存。</p><p>将数据保存到Redis缓存时，需要为数据设置一个合适的过期时间，这样做有以下两个好处。</p><p>◎如果没有为缓存数据设置过期时间，那么数据会一直堆积在Redis内存中，尤其是那些不再被访问或者命中率极低的缓存数据，它们一直占据Redis内存会造成大量的资源浪费。设置过期时间可以使Redis自动删除那些不再被访问的缓存数据，而对于经常被访问的缓存数据，每次被访问时都重置过期时间，可以保证<strong>缓存命中率高。</strong></p><p>◎当数据库与Redis缓存由于各种故障出现了数据不一致的情况时，过期时间是一个很好的兜底手段。例如，设置缓存数据的过期时间为10s，那么数据库和Redis缓存即使出现数据不一致的情况，最多也就持续10s。过期时间可以保证数据库和Redis缓存仅在此时间段内有数据不一致的情况，因此可以保证<strong>数据的最终一致性。</strong></p><p>在上述逻辑中，有一个极有可能带来风险的操作：某请求访问的数据在Redis缓存中不存在，此请求会访问数据库读取数据；而如果有大量的请求访问数据库，则可能导致数据库崩溃。Redis缓存中不存在某数据，只可能有两种原因：一是在Redis缓存中从未存储过此数据，二是此数据已经过期。下面我们就这两种原因来做有针对性的优化。</p><h3 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a><strong>缓存穿透</strong></h3><p>当用户试图请求一条连数据库中都不存在的非法数据时，Redis缓存会显得形同虚设。</p><p>（1）尝试在Redis缓存中查找此数据，如果命中，则返回数据。</p><p>（2）如果在Redis缓存中找不到此数据，则从数据库中读取数据。</p><p>（3）如果在数据库中也找不到此数据，则最终向用户返回空数据</p><p>可以看到，Redis缓存完全无法阻挡此类请求直接访问数据库。如果黑客恶意持续发起请求来访问某条不存在的非法数据，那么这些非法请求会全部穿透Redis缓存而直接访问数据库，最终导致数据库崩溃。这种情况被称为“缓存穿透”。</p><p>为了防止出现缓存穿透的情况，当在数据库中也找不到某数据时，可以在Redis缓存中为此数据保存一个空值，用于表示此数据为空。这样一来，之后对此数据的请求均会被Redis缓存拦截，从而阻断非法请求对数据库的骚扰。</p><p>不过，如果黑客访问的不是一条非法数据，而是大量不同的非法数据，那么此方案会使得Redis缓存中存储大量无用的空数据，甚至会逐出较多的合法数据，大大降低了Redis缓存命中率，数据库再次面临风险。我们可以使用布隆过滤器来解决缓存穿透问题。</p><p>布隆过滤器由一个固定长度为m的二进制向量和k个哈希函数组成。当某数据被加入布隆过滤器中后，k个哈希函数为此数据计算出k个哈希值并与m取模，并且在二进制向量对应的N个位置上设置值为1；如果想要查询某数据是否在布隆过滤器中，则可以通过相同的哈希计算后在二进制向量中查看这k个位置值：</p><p>◎如果有任意一个位置值为0，则说明被查询的数据一定不存在；</p><p>◎如果所有的位置值都为1，则说明被查询的数据可能存在。之所以说可能存在，是因为哈希函数免不了会有数据碰撞的可能，在这种情况下会造成对某数据的误判，不过可以通过调整m和k的值来降低误判率。</p><p>虽然布隆过滤器对于“数据存在”有一定的误判，但是对于“数据不存在”的判定是准确的。布隆过滤器很适合用来防止缓存穿透：将数据库中的全部数据加入布隆过滤器中，当用户请求访问某数据但是在Redis缓存中找不到时，检查布隆过滤器中是否记录了此数据。如果布隆过滤器认为数据不存在，则用户请求不再访问数据库；如果布隆过滤器认为数据可能存在，则用户请求继续访问数据库；如果在数据库中找不到此数据，则在Redis缓存中设置空值。虽然布隆过滤器对“数据存在”有一定的误判，但是误判率较低。最后在Redis缓存中设置的空值也很少，不会影响Redis缓存命中率。</p><h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a><strong>缓存雪崩</strong></h3><p>如果在同一时间Redis缓存中的数据大面积过期，则会导致请求全部涌向数据库。这种情况被称为“缓存雪崩”。缓存雪崩与缓存穿透的区别是，前者是很多缓存数据不存在造成的，后者是一条缓存数据不存在导致的。</p><p>缓存雪崩一般有两种诱因：大量数据有相同的过期时间，或者Redis服务宕机。第一种诱因的解决方案比较简单，可以在为缓存数据设置过期时间时，让过期时间的值在预设的小范围内随机分布，避免大部分缓存数据有相同的过期时间。第二种诱因取决于Redis的可用性，选取高可用的Redis集群架构可以极大地降低Redis服务宕机的概率。</p><hr><p><strong>高并发读场景总结：CQRS</strong></p><p>无论是数据库读&#x2F;写分离、本地缓存还是分布式缓存，其本质上都是读&#x2F;写分离，这也是在微服务架构中经常被提及的CQRS模式。CQRS（Command Query Responsibility Segregation，命令查询职责分离）是一种将数据的读取操作与更新操作分离的模式。query指的是读取操作，而command是对会引起数据变化的操作的总称，新增、删除、修改这些操作都是命令。</p><h3 id="CQRS的简要架构与实现"><a href="#CQRS的简要架构与实现" class="headerlink" title="CQRS的简要架构与实现"></a><strong>CQRS的简要架构与实现</strong></h3><h3 id="为了避免引入微服务领域驱动设计的相关概念，下图给出了CQRS的简要架构。"><a href="#为了避免引入微服务领域驱动设计的相关概念，下图给出了CQRS的简要架构。" class="headerlink" title="为了避免引入微服务领域驱动设计的相关概念，下图给出了CQRS的简要架构。"></a>为了避免引入微服务领域驱动设计的相关概念，下图给出了CQRS的简要架构。</h3><p><img src="https://mmbiz.qpic.cn/mmbiz_png/PW0wIHxgg3kOkTPoPAr4hibU1UzUVsKg2dladKjFx9dFgJp2LmqE1PCo9zvMgPftvkAtO4WRibadn6ntClbtxibjQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>（1）当业务服务收到客户端发起的command请求（即写请求）时，会将此请求交给写数据存储来处理。</p><p>（2）写数据存储完成数据变更后，将数据变更消息发送到消息队列。</p><p>（3）读数据存储负责监听消息队列，当它收到数据变更消息后，将数据写入自身。</p><p>（4）当业务服务收到客户端发起的query请求（即读请求）时，将此请求交给读数据存储来处理。</p><p>（5）读数据存储将此请求希望访问的数据返回。</p><p>写数据存储、读数据存储、数据传输通道均是较为宽泛的代称，其中写数据存储和读数据存储在不同的高并发场景下有不同的具体指代，数据传输通道在不同的高并发场景下有不同的形式体现，可能是消息队列、定时任务等。</p><p>◎对于数据库读&#x2F;写分离来说，写数据存储是 Master，读数据存储是 Slave，消息队列的实现形式是数据库主从复制。</p><p>◎对于分布式缓存场景来说，写数据存储是数据库，读数据存储是 Redis 缓存，消息队列的实现形式是使用消息中间件监听数据库的binlog数据变更日志。</p><p>无论是何种场景，都应该为写数据存储选择适合高并发写入的存储系统，为读数据存储选择适合高并发读取的存储系统，消息队列作为数据传输通道要足够健壮，保证数据不丢失。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;通用的高并发架构设计&quot;&gt;&lt;a href=&quot;#通用的高并发架构设计&quot; class=&quot;headerlink&quot; title=&quot;通用的高并发架构设计&quot;&gt;&lt;/a&gt;通用的高并发架构设计&lt;/h1&gt;&lt;p&gt;关键词：读&amp;#x2F;写分离、数据缓存、缓存更新、CQRS、数据分片、异步写</summary>
      
    
    
    
    <category term="技术小栈" scheme="http://ai.mak.cn/categories/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/"/>
    
    
    <category term="架构" scheme="http://ai.mak.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>conda、pip、anaconda、miniconda、miniforge、mambaforge 区分</title>
    <link href="http://ai.mak.cn/2024/07/14/python/conda%E3%80%81pip%E3%80%81anaconda%E3%80%81miniconda%E3%80%81miniforge%E3%80%81mambaforge%20%E5%8C%BA%E5%88%86/"/>
    <id>http://ai.mak.cn/2024/07/14/python/conda%E3%80%81pip%E3%80%81anaconda%E3%80%81miniconda%E3%80%81miniforge%E3%80%81mambaforge%20%E5%8C%BA%E5%88%86/</id>
    <published>2024-07-13T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="conda、pip、anaconda、miniconda、miniforge、mambaforge-区分"><a href="#conda、pip、anaconda、miniconda、miniforge、mambaforge-区分" class="headerlink" title="conda、pip、anaconda、miniconda、miniforge、mambaforge 区分"></a>conda、pip、anaconda、miniconda、miniforge、mambaforge 区分</h1><p><strong>核心属性和本质：</strong></p><ul><li><strong>conda</strong> 和 <strong>pip</strong> 的本质是包管理工具，它们帮助用户管理和安装软件包。</li><li><strong>anaconda</strong>、<strong>miniconda</strong>、<strong>miniforge</strong> 和 <strong>mambaforge</strong> 的本质是环境管理工具，它们提供了一套完整的工具来管理编程环境。</li></ul><p><strong>来个比较容易理解的比喻</strong>：<br> 在一个巨大的图书馆里，有各种各样的书，你想去借书看。这个图书馆就像是一个编程语言的世界，而书籍就是各种各样的代码库和工具。<br> <strong>conda</strong> 是一个聪明的图书管理员，它不仅能帮你找到书，还能告诉你哪些书是相互关联的，确保你借的每本书都能很好地一起工作。它不仅管理Python的书籍，还管理其他语言的书籍。<br> 解释：一个开源的包管理系统和环境管理系统，可以用来安装、运行和升级包和它们的依赖关系。它支持多种语言。</p><p><strong>pip</strong> 是另一个图书管理员，但它专注于Python的书籍。它很擅长找到你想要的Python书籍，但有时候它不太关心这些书是否能很好地相互协作。<br> 解释：是Python的默认包管理工具，专为python打造，用于安装和管理Python库。</p><p><strong>anaconda</strong> 是一个巨大的书架，上面预先放好了很多最常用的书籍，这样你就不用每次都去找图书管理员了。它特别适合那些不想花太多时间挑选书籍的人。<br> 解释：是一个发行版，由Anaconda公司开发，里面除了conda，还包含了Python以及许多科学计算相关的包。</p><p><strong>miniconda</strong> 是一个更小的书架，上面只有几本你开始阅读时必须的书籍。如果你需要更多的书，你可以去找conda图书管理员帮你。<br> 解释：是anaconda的一个轻量级版本，只包含conda和其依赖项，没有预装其他任何包。Miniconda的优势在于它的体积较小，安装过程更快，用户可以根据需要自行选择和安装所需的包。</p><p><strong>miniforge</strong> 和 <strong>mambaforge</strong> 是类似于miniconda的两个不同品牌的小书架，但它们来自不同的制造商。它们提供了一些不同的特性，比如更快的书籍检索速度（mambaforge）或者更多地支持开源书籍（miniforge）。<br> 解释：miniforge是一个社区驱动的miniconda替代品，与Miniconda相比，提供了更多的编译器和开发工具，适用于更复杂的环境和依赖关系。社区持续在维护和更新，所以一般用这个就足够了。<br> mambaforge脱胎于Miniforge，做了一些优化，以更高的效率实现了和conda 同样的功能，对conda中低效的部分进行了重写。不过从 23 年 9 月份开始，Mambaforge 已经不鼓励使用了，可以放弃。</p><p>mambaforge的官方申明：</p><blockquote><p>Mambaforge（Discouraged as of September 2023）<br> With the <a href="https://link.juejin.cn/?target=https://link.zhihu.com/?target=https://github.com/conda-forge/miniforge/releases/tag/23.3.1-0">release</a> of Miniforge3-23.3.1-0, that incorporated the changes in <a href="https://link.juejin.cn/?target=https://link.zhihu.com/?target=https://github.com/conda-forge/miniforge/pull/277">#277</a>, the packages and configuration of Mambaforge and Miniforge3 are now <strong>identical</strong>. The only difference between the two is the name of the installer and, subsequently, the default installation directory. Given its wide usage, there are no plans to deprecate Mambaforge. If at some point we decide to deprecate Mambaforge, it will be appropriately announced and communicated with sufficient time in advance.<br> As of September 2023, the new usage of Mambaforge is thus discouraged. Bug reports specific to Mambaforge will be closed as won’t fix.</p></blockquote><p>意思就是，Miniforge已经把 Mambaforge 的功能给合并过来了，虽然目前Mambaforge还能用，不过不推荐，随时可能弃用，而且里面的 bug 也不会修了。</p><p>总结一下，绝大部分用户使用Miniforge 即可，常用做法是，conda 命令创建虚拟环境，conda install 安装 torch 包，剩下的包交给 pip 来搞定，兼具安全和高效。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;conda、pip、anaconda、miniconda、miniforge、mambaforge-区分&quot;&gt;&lt;a href=&quot;#conda、pip、anaconda、miniconda、miniforge、mambaforge-区分&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="技术小栈" scheme="http://ai.mak.cn/categories/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/"/>
    
    
    <category term="python" scheme="http://ai.mak.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>怎样读书，才算对得起自己？</title>
    <link href="http://ai.mak.cn/2024/07/07/%E6%9D%82%E8%AE%B0/%E6%80%8E%E6%A0%B7%E8%AF%BB%E4%B9%A6%EF%BC%8C%E6%89%8D%E7%AE%97%E5%AF%B9%E5%BE%97%E8%B5%B7%E8%87%AA%E5%B7%B1%EF%BC%9F/"/>
    <id>http://ai.mak.cn/2024/07/07/%E6%9D%82%E8%AE%B0/%E6%80%8E%E6%A0%B7%E8%AF%BB%E4%B9%A6%EF%BC%8C%E6%89%8D%E7%AE%97%E5%AF%B9%E5%BE%97%E8%B5%B7%E8%87%AA%E5%B7%B1%EF%BC%9F/</id>
    <published>2024-07-06T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.732Z</updated>
    
    <content type="html"><![CDATA[<p>怎样读书，才算对得起自己？</p><blockquote><p>节选自：得到头条 | 360期</p></blockquote><p>今天，我将从两个话题出发，为你提供知识服务。第一个是，423世界读书日特刊。第二个是，法国巴黎恢复服务员赛跑。</p><p>先来看今天的第一条。今天是4月23日，世界读书日，也是咱们得到的破万卷节。你大概率上能看到好多人在谈论读书的方法，或者推荐这一年的重磅书。这些信息都很有价值，建议你有空去看看。</p><p>回到今天的节目，咱们换个角度，把姿态放轻松点，权当是同学之间互相交流。我想跟你说说，我昨天早上一个小时，在得到电子书随意刷书，获得的收获。没错，就是早上一个小时，在得到电子书的新书榜上随手翻翻。</p><p>有人可能会问，只是随手翻翻，能有多大的收获？你别说，收获还不少。接下来就跟你汇报一下我这一个小时乱翻书的收获。主要也是想跟你交流一下，乱翻书，快刷书，是不是也算一件很有趣的事？</p><p>好，咱们正式开始。这一个小时，我主要有三个收获。</p><p>第一，是安顿感。也就是，以前某些觉得很沉重的事，突然变得轻松了。</p><p>比如，昨天，得到电子书的新书榜上，有本书排名很靠前，复旦大学的经济学者，梁捷老师写的《表层的真理》。这是一本普通人了解经济学的入门书。刚翻开前言部分，就看到一个好玩的故事。</p><p>说的是，曾经有经济学家专门研究非洲的足球比赛。他们主要想搞清楚，踢足球，是怎么影响非洲人的国家认同的。我们都知道，非洲的很多国家是由部落组成的。尽管这些国家已经建成几十年，但很多人对国家其实没什么认同感，他们始终觉得，自己是某个部落的人。这也导致，很多国家内部，经常发生部落冲突，死伤惨重。</p><p>那么，<strong>怎么让大家建立国家认同呢？几位经济学家就去调查数据。结果发现，足球是个特别关键的因素。</strong> 在足球比赛之前你问当地人，说你觉得自己是某个国家的人，还是某个部落的人？他们大概率会回答，我是部落的人。但是，在足球比赛结束你再去问，会有很多人改变想法，觉得自己是某个国家的人。尤其这个国家的足球队取得胜利，会有37%的人改变想法，从部落认同转变为国家认同。有资格参加非洲杯的国家，他们的国内冲突，要比没有资格参加非洲杯的国家，少9%。</p><p>这个感觉就好比，一个公司里，就算AB两个部门矛盾再深，一旦整个公司派出球队，跟别的公司比赛，这两个部门的关系也会好转很多。说白了，想团结团队，就去组织足球赛。</p><p>但是，这个故事，还不是这本《表层的真理》的核心。作者是想通过这个故事跟你说，你看，这就是现在的经济学家们正在研究的事。艰深吗？枯燥吗？并没有。因此，别对经济学犯怵。</p><p>你看，到这一步，我就有了两个选择。我可以接着读这本书，看更多有趣的故事。我也可以顺手就把这个故事转发给身边刚毕业的朋友，然后跟他说，学点经济学吧，你看，经济学多好玩啊。总之，读这本书的短短几分钟，肯定不亏，对得起自己。</p><p>第二个收获是，重逢。也就是，<strong>很多自己经历过的事，你读书时会发现，原来作者也有一模一样的经历，就像在大街上突然遇见多年不见的朋友</strong>。</p><p>比如，在新书榜上，有本书叫《吃着吃着就老了》，作者是著名的纪录片导演，也是《风味人间》的总导演陈晓卿老师。里面主要说的是他自己这些年的跟吃有关的回忆。书里面有几段说的是他工作时，单位附近的餐馆。</p><p>为什么说惊喜？因为我过去跟陈晓卿老师，就在同一栋大楼里办公。尽管没见过面，但他讲的餐馆，我正好都吃过。这个时候你再看书里的内容，就多了一层微妙的感受。</p><p>比如，说到单位楼下新开的连锁饼店，陈晓卿老师说，馅饼还行，但老板胆子大，还做老友粉，味道不好评价。但对我来说，我第一次吃老友粉这个东西，就在这家店。要不是后来去南宁吃到正宗的，我还一直觉得老友粉就是楼下饼店的样子。</p><p>再比如，说到北京的基辅罗斯，是家俄罗斯餐厅。也熟悉，我去那参加过朋友的婚礼，里面有俄罗斯人唱歌表演。顺着玉渊潭公园南门一路往西走就是。假如是晚上一个人走，还有点打怵。这条路总给人种灯下黑的感觉。</p><p>你看，作者讲的是他的回忆。但读者从中读到的，可能是自己经历过的事。</p><p>你看，到这里，我又有两个选择。继续读，能读到更多的故事。但就此打住也行，就像好久不见的朋友重逢。即使只是畅聊几句，也觉得很尽兴。这个感觉是不是很奇妙？</p><p>第三个收获是，不同。也就是，<strong>能看到很多不同的观点。这些观点你未必同意，但是，你会因为知道它而感到有趣。会觉得原来某件事，还可以从这个角度去思考</strong>。</p><p>比如，得到电子书有本最近上线的新书，叫《百岁生活》。作者是一位日本的精神科医生，叫和田秀树。书很薄，只有5万多字。看的人也不多。但是翻开之后，还是能发现一点不一样的东西。</p><p>比如，这本书里一直强调一个概念，叫幸龄。作者一直说，假如一个人能活到80岁以上，这就是幸运，就别想着整天去治病了。因为这会儿身体很多机能在衰退，有可能会因为治了这个毛病，导致其他的问题。这时假如身体出毛病，目标就不是根治，轻易也别做手术，而是想办法让自己能尽可能舒服地活着。这位和田秀树还举了很多日本的数据。比如有的地方因为特殊原因，医院停运了一段时间。结果这段时间离世的80岁以上老人，反而减少了等等。</p><p>这个观点一定对吗？未必。估计很多人也不会同意。但是，这本书至少给了我们一个提醒，就是在面对疾病时，系统的健康观很重要。不能只思考某个局部，而是要尽量全面地考虑自己的健康状况。</p><p>你看，读到这，我又有两个选择。一是继续读下去，看看这位作者还说了什么。二是带着这个提醒，转身就走，去读别的书。总之，至少读这本书的几分钟不白费，对得起自己。</p><p>好，关于昨天早上的刷书收获，咱们先说到这。注意，我们可不是想说，一定要按照某种方法读书。而是想告诉你，<strong>随手乱翻书，其实是一件很好玩，也很有收获的事。假如你现在有类似的感受，别犹豫，也来试试</strong>。同时，423前后，也是咱们得到的破万卷节。得到电子书有优惠活动，优惠力度很大，趁现在别错过。</p><p>再来看今天的第二条。前段时间，法国巴黎恢复了一项奇怪的体育比赛，服务员赛跑。也就是，穿着制服的男女服务员，手托餐盘，餐盘里放一个羊角面包、一个空咖啡杯和一杯水，从巴黎市政厅出发，沿环线跑完2公里赛程。</p><p>这本来是巴黎的传统活动，有100多年的历史。但最近13年一直停办，直到今年，又恢复了举办。</p><p>一方面，是在为夏天的巴黎奥运会造势，另一方面，这个比赛也有强烈的代表意义。咖啡馆是巴黎的标志之一。巴黎市市长伊达尔戈说，大大小小的咖啡馆是“巴黎的灵魂”。比如最著名的花神咖啡馆，15世纪就开业了。据说海明威在这里写了《太阳照常升起》。毕加索在这里的玻璃上作过画。萨特和波伏瓦也每天在这里畅谈，1943年萨特在这里写完了代表作《存在与虚无》。</p><p>其实，直到今天，在咖啡馆写作也特别流行。比如，奥斯卡最佳影片《寄生虫》的导演和编剧奉俊昊，就特别喜欢在咖啡馆写作，而且他会找一个角落，特意背对人群，这样，既能享受咖啡馆的环境，又能让自己专注于手头的任务。</p><p>顺着这个话题，咱们就来说说，一个写作者，怎么写才能写得更快更多？前段时间，美国专栏作家托德·布里森专门写文章做了分享，他给很多美国大媒体供过稿，还每天给读者写一份电子报，算是很高产了。假如你也是一名创作者，尤其是持续创作者，接下来的这三个技巧，你可以做个参考。</p><p><strong>第一个技巧是，给自己设定限制</strong>。说得具体点，就是把电脑电源拔掉。</p><p>这是布里森从一次咖啡馆写作中得来的。他去咖啡店写稿，原本计划集中精力写4小时。但是坐下之后他发现忘了带电源，剩余电量只够写一个多小时。于是，他集中注意力，用一个小时就写完了稿子。这篇稿子，最后成了他2020年最受欢迎的一篇文章。</p><p>你看， 有时候拥有无限的保障，可能反而会模糊你的注意力。因此有时候，需要把自己放在一个没有兜底的环境里。有了这个限制之后，没准反而会提高效率。</p><p><strong>第二个技巧是，不要永远只盯着一篇稿子，你可以尝试同时面对三篇，每20分钟切换一次</strong>。</p><p>布里森说，这个技巧来自奥斯卡得主，《乔乔兔》的编剧塔伊加·维迪提。维迪提完成这部剧本，用了六年时间，听起来很久，但这六年里，他还同时完成了很多其他剧本。方法就是，不同项目同时开展，经常切换。具体到每天的写作上，你可以设置一个20分钟的闹钟，同时写三篇稿子，每20分钟切换一次。当你切回第一个项目的时候，过去了40分钟，这个时间，不至于让你忘了之前的内容，也能让你产生一些新的想法。</p><p><strong>第三个技巧比较细节，跟打字这个动作有关，说的是，眼睛不要看屏幕，手里不要停，一口气把自己的想法打下来</strong>。</p><p>眼睛不看屏幕，是因为，盯着屏幕上的单词，会过多占用你的注意力，有时候你看着自己写的东西，纠结一个用词或者一个句子，速度就会被大大拖慢。布里森建议，确定好光标的位置，然后开始打字，尽可能少看屏幕。假如你用台式机写作，可以把显示器关掉，假如你用笔记本写作，就把屏幕压低。</p><p>写错了也无所谓，关键是，快速把你大脑里的句子写下来，等写完了之后再统一检查。布里森认为，要把写初稿当成一场冲刺，而不是马拉松，一鼓作气地写完，比什么都重要。</p><p>在这里，引用美国科幻作家雷·布拉德伯里的一句话，他一生写了超过500部作品，最有名的一句口号就是，你的直觉知道该写什么，因此别阻挡它。</p><p>最后，总结一下，今天说了两个话题。</p><p>第一，关于怎样阅读？读书不只是为了对书负责，更是为了对人负责，对自己负责。乱翻书，快刷书，往往能获得意外之喜。</p><p>第二，关于怎样书写？关键在于，不让书写这个动作干扰思考。就像刚才说的，你的直觉知道应该写什么，因此别阻挡它。</p><p>这两个话题，一个是关于阅读，一个是关于书写。顺着这两个话题，最后还有个特别分享。前不久，脱不花老师给我推荐了一首诗，说很适合在423这天读一读。这首诗的来历很特别。今年10月，NASA准备往木卫二发射一颗探测器，木卫二也叫欧罗巴，因此这枚探测器就叫欧罗巴快船。有趣的是，这回NASA请桂冠诗人阿达·利蒙专门给木卫二写了一首诗，刻在了欧罗巴快船上，诗的名字叫《赞美神秘》。</p><p>你看，这个事是不是也反映了一个真相。这就是，<strong>哪怕在宇宙深处，只要有人类在，那么有两件事就会始终存在，这就是，阅读与书写</strong>。</p><p>下面是这首诗的全文，我使用的是网络上Tina与大鸣两位老师翻译的版本，翻译得很精彩。</p><blockquote><p>色深邃，如浓墨写意天空。仰望熟悉的满天星斗，脱口而出我们的祈愿。从地球了解太空，仿佛阅读一本关于宇宙的书，准确无误，专业易懂。然而，苍穹之下，仍有未解之谜。鲸鱼的歌声，风中摇动的树枝上，鸣鸟的对歌吟唱。我们是心怀敬畏的生灵，好奇于一切美好、叶子和花开，悲伤和快乐，阳光和阴影。使我们联系在一起的不是黑暗，也不是遥远冰冷的空间，而是水的馈赠。每一滴雨水。每一条小溪，像似每根血管，和每一次的脉搏。木卫二，我们恰巧也由水而生。源自于浩瀚而瞩目的海洋。我们的构成也充满奇迹，有着伟大而平凡的爱，有着小而未知的世界。更有穿越黑暗，探究宇宙的呼唤。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;怎样读书，才算对得起自己？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;节选自：得到头条 | 360期&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今天，我将从两个话题出发，为你提供知识服务。第一个是，423世界读书日特刊。第二个是，法国巴黎恢复服务员赛跑。&lt;/p&gt;
&lt;p&gt;先来看</summary>
      
    
    
    
    <category term="杂记" scheme="http://ai.mak.cn/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
    <category term="杂记" scheme="http://ai.mak.cn/tags/%E6%9D%82%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>针对不同层次进行性能测试的监控</title>
    <link href="http://ai.mak.cn/2024/07/06/%E6%B5%8B%E8%AF%95/%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%B1%82%E6%AC%A1%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E7%9A%84%E7%9B%91%E6%8E%A7/"/>
    <id>http://ai.mak.cn/2024/07/06/%E6%B5%8B%E8%AF%95/%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%B1%82%E6%AC%A1%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E7%9A%84%E7%9B%91%E6%8E%A7/</id>
    <published>2024-07-05T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.532Z</updated>
    
    <content type="html"><![CDATA[<h1 id="针对不同层次进行性能测试的监控"><a href="#针对不同层次进行性能测试的监控" class="headerlink" title="针对不同层次进行性能测试的监控"></a>针对不同层次进行性能测试的监控</h1><p>在执行性能测试的过程中，监控服务端的资源消耗等也是必备内容，监控的结果是帮助测试发现问题的眼睛。然而在实操过程中大家喜欢用JMeter 工具提供的插件进行监控，但是并不推荐使用这种方式，原因如下：</p><ul><li>指标相对简单且固定，结果数据粗糙且界面显示并不是很友好；</li><li>较大地增加了客户端压测机的资源开销，影响性能测试结果；</li><li>特定环境下，在服务器上安装插件是不被允许的，会很不方便。</li></ul><p>所以在这里，我们先来聊一下监控要点；</p><h1 id="一、层次清晰"><a href="#一、层次清晰" class="headerlink" title="一、层次清晰"></a><strong>一、层次清晰</strong></h1><p>从执行一次性能测试来看，需要监控的内容有很多，重点是要能理清楚不同的监控类型，以及分别能够解决什么问题？</p><h2 id="1、硬件层"><a href="#1、硬件层" class="headerlink" title="1、硬件层"></a><strong>1、硬件层</strong></h2><p>硬件层是最容易想到的一个层面，一般包含了 <strong>CPU 的使用率</strong>、<strong>内存使用率</strong>、<strong>磁盘</strong>和<strong>网络读写速度</strong>等，通过这些指标能够反馈出系统运行的基本情况，以及不同的 TPS 量级会消耗多少硬件资源。</p><h2 id="2、系统层"><a href="#2、系统层" class="headerlink" title="2、系统层"></a><strong>2、系统层</strong></h2><p><strong>系统层监控</strong>包括<strong>连接请求数</strong>、<strong>拒绝数</strong>、<strong>丢包率</strong>、<strong>请求超时</strong>等，相对于基础的硬件监控而言，<strong>这些指标更能够反映出目前系统存在的瓶颈</strong>，从而为根因问题的定位提供有力的线索。</p><h2 id="3、链路层"><a href="#3、链路层" class="headerlink" title="3、链路层"></a><strong>3、链路层</strong></h2><p><strong>链路层是直接面向架构和代码的</strong>，它的监控能够帮助我们更加准确地看到<strong>代码执行了哪些函数</strong>，<strong>涉及哪些服务</strong>，并且能够较为清晰地<strong>看到函数之间的调用耗时</strong>，还可以帮助<strong>定位代码存在的问题</strong>。</p><h2 id="4、业务层"><a href="#4、业务层" class="headerlink" title="4、业务层"></a><strong>4、业务层</strong></h2><p><strong>业务层监控本意是帮助判断用户输入是否合规，代码逻辑是否健壮</strong>。对于性能测试而言，业务层的监控可以帮助我们发现脚本参数问题以及高并发下业务逻辑运行是否正常等，比如随着测试的进行，可能会存在商品库存不足的情况。如果有业务层面的监控，当库存低于某阈值时，可以进行一定的提示以规避此类问题。</p><h1 id="二、定向深入"><a href="#二、定向深入" class="headerlink" title="二、定向深入"></a><strong>二、定向深入</strong></h1><p>首先我们通过基本的监控可以获得一些异常点，比如 CPU 高了、磁盘在等待，这些说白了是表象问题。就比如说某位同事今天没来，通过没来这个现象并不能直接下定论说他生病了。对于监控也是这样，是否有定位根因问题的手段，CPU 高了，需不需要进行线程分析，需要哪些权限和定位工具，这些在监控部署时都需要考虑到。</p><h2 id="1、CPU"><a href="#1、CPU" class="headerlink" title="1、CPU"></a><strong>1、CPU</strong></h2><p>top 是我们查看各个进程的资源占用状况最常用的命令，如下代码所示，这个命令简单却包含很大的信息量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">top - 18:17:47 up 158 days,  9:32,  2 users,</span><br><span class="line"></span><br><span class="line">load average: 0.07, 0.15, 0.21</span><br><span class="line"></span><br><span class="line">Tasks: 154 total,   1 running, 152 sleeping,   0 stopped,   1 zombie</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">Cpu(s):  3.9 us,  1.3 sy,  0.0 ni, 94.6 <span class="built_in">id</span>,  0.2 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br><span class="line"></span><br><span class="line">KiB Mem :  8010676 total,   337308 free,  6036100 used,  1637268 buff/cache</span><br><span class="line"></span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.  1223072 avail Mem</span><br></pre></td></tr></table></figure><p>从以上的信息中，我们来介绍几个常用的重点指标</p><h3 id="（1）load-average"><a href="#（1）load-average" class="headerlink" title="（1）load average"></a><strong>（1）load average</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load average: 0.07, 0.15, 0.21</span><br></pre></td></tr></table></figure><p>三个数字都是代表进程队列的长度，从左到右分别表示一分钟、 五分钟和十五分钟的数据，数字越小压力值就越低，数字越大则压力越高，然而这个数值多小算小呢？多大算大呢？以单核处理器为例，打个比方就像收费站的一个 ETC 通道一样：</p><ul><li><strong>0</strong> 表示没有任何车辆需要通过；</li><li>从 <strong>0 到 1</strong> 可以认为很流畅，车辆不需要任何等待就可以通过；</li><li><strong>1</strong> 表示正好在这个通道可接受范围之内；</li><li><strong>超过 1</strong> 就已经有车辆在后面排队了。</li></ul><p>所以理想情况下，希望平均负载值在 1 以下。如果是 1 就代表目前没有可用资源了。在实际情况中，很多运维会把理想负载设置在 0.7 以下，这也是业内的一个“<strong>经验值</strong>”。</p><p>上面说的是一个单核处理器的情况，多核 CPU 的话，负载数值 &#x2F; CPU 核数在 0.00~1.00 之间表示正常，理想值也是在 0.7 以内。</p><h3 id="（2）CPU状态"><a href="#（2）CPU状态" class="headerlink" title="（2）CPU状态"></a><strong>（2）CPU状态</strong></h3><p>从 top 中你也可以看到每种类型进程消耗的 CPU 时间百分比，如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">%</span><span class="language-bash">Cpu(s):  3.9 us,  1.3 sy,  0.0 ni, 94.6 <span class="built_in">id</span>,  0.2 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br></pre></td></tr></table></figure><ul><li><strong>us</strong> 列显示了用户进程所花费 CPU 时间的百分比。这个数值越高，说明用户进程消耗的 CPU 时间越多，可以用来分析代码中的 CPU 消耗热点。</li><li><strong>sy</strong> 列表示系统进程消耗的 CPU 时间百分比。</li><li><strong>ni</strong> 列表示改变优先级的进程占用 CPU 的百分比。</li><li><strong>id</strong> 列表示 CPU 处于空闲状态的时间百分比。</li><li><strong>wa</strong> 列显示了 I&#x2F;O 等待所占用的 CPU 时间的百分比，这里 wa 的参考值为 0.5，如果长期高于这个参考值，需要注意是否存在磁盘瓶颈。</li><li><strong>hi</strong> 列表示硬件中断占用 CPU 时间百分比。</li><li><strong>si</strong> 列表示软件中断占用 CPU 时间百分比。</li><li><strong>st</strong> 列表示当系统运行在虚拟机中时，当前虚拟机在等待 CPU 为它服务的时间；</li></ul><p>在已经输入 top 的情况下再输入数字 1，可以查看 CPU 的核数和每个核的运行状态。如下图是两核 CPU 的运行状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">%</span><span class="language-bash">Cpu0  :  3.0 us,  1.7 sy,  0.0 ni, 95.3 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">Cpu1  :  2.4 us,  1.0 sy,  0.0 ni, 96.6 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br></pre></td></tr></table></figure><h2 id="2、内存"><a href="#2、内存" class="headerlink" title="2、内存"></a><strong>2、内存</strong></h2><p>最常见的是通过 free 来查看 Linux 内存使用情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@JD ~]# free -m</span><br><span class="line"></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line"></span><br><span class="line">Mem:           7822        5917         302         373        1602        1195</span><br><span class="line"></span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure><p>通过单词的意思我们也能大概看出来 total、used、free 表示什么，它们分别是总的<strong>物理内存大小、已经被使用的物理内存和空闲的物理内存值</strong>是多少。</p><h2 id="3、磁盘"><a href="#3、磁盘" class="headerlink" title="3、磁盘"></a><strong>3、磁盘</strong></h2><h3 id="（1）iostat"><a href="#（1）iostat" class="headerlink" title="（1）iostat"></a><strong>（1）iostat</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> [root@JD ~]# iostat -x</span><br><span class="line"></span><br><span class="line">Linux 3.10.0-514.el7.x86_64 (JD)        01/18/2021      _x86_64_        (2 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line"></span><br><span class="line">           5.24    0.00    1.57    0.07    0.00   93.12</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line"></span><br><span class="line">vda               0.00     0.29    0.57    5.30    20.50   630.14   221.82     0.07   11.53   59.83    6.36   1.18   0.69</span><br></pre></td></tr></table></figure><p>通过这个命令你能看到磁盘实时运行的情况，一般可以优先看 idle、util 和 svctm 这几列的数值：</p><ul><li><strong>idle</strong> 代表磁盘空闲百分比；</li><li><strong>util</strong> 接近 100%，表示磁盘产生的 I&#x2F;O 请求太多，I&#x2F;O 系统已经满负荷在工作，该磁盘可能存在瓶颈；</li><li><strong>svctm</strong> 代表平均每次设备 I&#x2F;O 操作的服务时间 (毫秒)。</li></ul><h3 id="（2）iotop"><a href="#（2）iotop" class="headerlink" title="（2）iotop"></a><strong>（2）iotop</strong></h3><p>iotop 这个命令并不是 linux 原生的，需要安装，以 CentOS 7.0 为例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@JD ~]# yum -y install iotop</span><br></pre></td></tr></table></figure><p>安装完成之后，直接输入 iotop，示意如下，就能清楚地看到哪些进程在消耗磁盘资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6448 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % ifrit-agent</span><br><span class="line"></span><br><span class="line">14647 be/4 root        0.00 B/s    7.70 K/s  0.00 %  0.00 % java -Dserver.port=9080</span><br></pre></td></tr></table></figure><h2 id="4、网络"><a href="#4、网络" class="headerlink" title="4、网络"></a><strong>4、网络</strong></h2><h3 id="（1）netstat"><a href="#（1）netstat" class="headerlink" title="（1）netstat"></a><strong>（1）netstat</strong></h3><p>netstat 能提供 TCP 和 UDP 的连接状态等统计信息，可以简单判断网络是否存在堵塞。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@JD ~]# netstat</span><br><span class="line"></span><br><span class="line">Active Internet connections (w/o servers)</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State</span><br><span class="line"></span><br><span class="line">tcp        0      1 JD:49190                169.254.169.250:http    FIN_WAIT1</span><br><span class="line"></span><br><span class="line">tcp        0      0 JD:39444                169.254.169.254:http    TIME_WAIT</span><br><span class="line"></span><br><span class="line">tcp        0      0 JD:us-srv               worker-18.:sentinel-ent ESTABLISHED</span><br></pre></td></tr></table></figure><ul><li><strong>Proto：</strong>协议名（可以 TCP 协议或者 UDP 协议）。</li><li><strong>recv-Q：</strong>网络接收队列还有多少请求在排队。</li><li><strong>send-Q：</strong>网络发送队列有多少请求在排队。</li></ul><blockquote><p><strong>recv-Q 和 send-Q</strong> 如果长期不为 0，很可能存在网络拥堵，这个是判断网络瓶颈的重要依据。</p></blockquote><ul><li><strong>Foreign Address：</strong>与本机端口通信的外部 socket。</li><li><strong>State：</strong>TCP 的连接状态。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;针对不同层次进行性能测试的监控&quot;&gt;&lt;a href=&quot;#针对不同层次进行性能测试的监控&quot; class=&quot;headerlink&quot; title=&quot;针对不同层次进行性能测试的监控&quot;&gt;&lt;/a&gt;针对不同层次进行性能测试的监控&lt;/h1&gt;&lt;p&gt;在执行性能测试的过程中，监控服务端的</summary>
      
    
    
    
    <category term="技术小栈" scheme="http://ai.mak.cn/categories/%E6%8A%80%E6%9C%AF%E5%B0%8F%E6%A0%88/"/>
    
    
    <category term="测试" scheme="http://ai.mak.cn/tags/%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Langchain-Chatchat大语言模型本地知识库</title>
    <link href="http://ai.mak.cn/2024/06/30/ai/Langchain-Chatchat%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93/"/>
    <id>http://ai.mak.cn/2024/06/30/ai/Langchain-Chatchat%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93/</id>
    <published>2024-06-29T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.316Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Langchain-Chatchat大语言模型本地知识库"><a href="#Langchain-Chatchat大语言模型本地知识库" class="headerlink" title="Langchain-Chatchat大语言模型本地知识库"></a>Langchain-Chatchat大语言模型本地知识库</h2><blockquote><p>链接：<a href="https://juejin.cn/post/7305572311812751371">https://juejin.cn/post/7305572311812751371</a> 来源：稀土掘金</p></blockquote><ul><li><a href="https://github.com/chatchat-space/Langchain-Chatchat">https://github.com/chatchat-space/Langchain-Chatchat</a></li></ul><p>Langchain-Chatchat是一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。</p><p>💡Langchain-Chatchat建立了全流程可使用开源模型实现的本地知识库问答应用，使用基于 Streamlit 的 WebUI 进行操作。 <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fd0aeef1d58043b5acb630ea8ca82b2c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1664&h=904&s=154826&e=png&b=fcfcfc" alt="img"></p><p>⛓️ Langchain-Chatchat实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的 top k个 -&gt; 匹配出的文本作为上下文和问题一起添加到 prompt中 -&gt; 提交给 LLM生成回答。 <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/602aad3c063343f9859e3218f2faebb9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1262&h=792&s=42649&e=png&b=fefefe" alt="img"></p><p>学术Fun将上述工具制作成一键启动包，内置chatglm3模型，点击即可使用，避免大家配置Python环境出现各种问题，下载地址： <a href="https://link.juejin.cn/?target=https://xueshu.fun/3278/">xueshu.fun&#x2F;3278&#x2F;</a>，<code>在此页面右侧区域点击下载！</code></p><p><code>注意电脑配置如下</code>：</p><ul><li>windows 10&#x2F;11</li><li><code>8G显存以上英伟达显卡</code></li></ul><h2 id="下载使用教程"><a href="#下载使用教程" class="headerlink" title="下载使用教程"></a>下载使用教程</h2><ul><li>下载压缩包 下载地址： <a href="https://link.juejin.cn/?target=https://xueshu.fun/3278/">xueshu.fun&#x2F;3278&#x2F;</a>，<code>在此页面右侧区域点击下载！</code></li><li>解压，解压后，最好路径和我的保持一致，<code>D:\AI\Langchain-ChatGLM</code>,如下图所示，双击<code>启动.exe</code>文件运行</li></ul><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ed68fff2413c46f1be2df610c5fc952e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1647&h=1957&s=124411&e=png&b=ffffff" alt="img"></p><ul><li>启动过程中，会联网更新streamlit包，包很小只有10几兆，更新streamlit包是防止有的同学没有解压到和我本地一致的目录，会出现streamlit命令找不到的错误，启动完成后浏览器会自动打开使用界面，如下所示，本地访问地127.0.0.1:8501</li></ul><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7a759e002a934b128e2eabfbaad968b8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3164&h=1825&s=124939&e=png&b=fdfdfd" alt="img"></p><h3 id="本地知识库使用"><a href="#本地知识库使用" class="headerlink" title="本地知识库使用"></a>本地知识库使用</h3><ul><li>按照下图步骤，建立本地知识库</li></ul><ol><li>点击知识库管理</li><li>新建知识库</li></ol><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0ac324e79d0a4c4b8853ab9ecedb8b85~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3240&h=1177&s=128148&e=png&b=fbfbfb" alt="img"></p><ol><li>新建知识库名称，不支持中文</li><li>填写知识库简介</li><li>点击新建 <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/643bce05021443c08da606786bea686d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3135&h=1168&s=102393&e=png&b=fbfbfb" alt="img"></li><li>上传知识库文件，支持格式如图中所示</li><li>点击添加文件到知识库</li></ol><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c7ca0ed6c95445379f3d13bac5539b4e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3268&h=1474&s=150063&e=png&b=fbfbfb" alt="img"></p><h3 id="使用知识库问答"><a href="#使用知识库问答" class="headerlink" title="使用知识库问答"></a>使用知识库问答</h3><ol><li>选择对话</li><li>选择知识库问题对话模式</li><li>选择刚刚建立的知识库名称</li><li>输入内容进行知识库对话，对话内容会从上传的知识库文件中索引 <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c42e87581e22479d991e0a09225108e7~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3085&h=1606&s=166025&e=png&b=fdfdfd" alt="img"></li></ol><h3 id="联网问答功能"><a href="#联网问答功能" class="headerlink" title="联网问答功能"></a>联网问答功能</h3><p>还支持联网在线搜索问答，使用方式见下图： <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/32de74065e5747eeb73d72b67a21522a~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=3312&h=1752&s=388767&e=png&b=fcfcfc" alt="img"></p><p>)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Langchain-Chatchat大语言模型本地知识库&quot;&gt;&lt;a href=&quot;#Langchain-Chatchat大语言模型本地知识库&quot; class=&quot;headerlink&quot; title=&quot;Langchain-Chatchat大语言模型本地知识库&quot;&gt;&lt;/a&gt;La</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="深度学习" scheme="http://ai.mak.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于大模型的智能问答 - LangChain</title>
    <link href="http://ai.mak.cn/2024/06/15/ai/LangChain%20%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94/"/>
    <id>http://ai.mak.cn/2024/06/15/ai/LangChain%20%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94/</id>
    <published>2024-06-14T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.315Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LangChain-基于大模型的智能问答"><a href="#LangChain-基于大模型的智能问答" class="headerlink" title="LangChain 基于大模型的智能问答"></a>LangChain 基于大模型的智能问答</h1><p> 在构建基于大模型的智能问答系统时，LangChain 提供了一个强大的框架，支持各种模块来帮助开发者构建更复杂、更智能的语言处理应用。以下是构建此类系统的一些关键组件和步骤。</p><p>模型接入：使用 LangChain 支持的模型，通过 API 接入外部模型，或利用 <strong>api-for-open-llm</strong> 框架调用 本地llm模型。</p><p>向量库与嵌入模型：利用 Milvus 向量库和嵌入模型（如 m3e 向量模型）来增强问答系统的检索能力，使其能够从大量文本数据中快速检索相关信息。</p><p>链式调用和代理行动决策：使用 LangChain 的链（Chains）和代理（Agents）模块来构建复杂的问答逻辑，使系统能够根据用户输入做出连贯的决策并提供相关的回答。</p><p>提示词写法：根据需要编写适当的提示词，以引导模型提供准确的回答。可以使用 XML 标记来定义上下文和历史记录，以便模型更好地理解问题和背景。</p><p>通过以上步骤，可以构建一个基于大模型的智能问答系统，该系统能够理解用户的问题并提供准确、相关的回答。此外，通过不断优化模型、提示词和检索策略，可以进一步提高问答系统的性能和用户体验。</p><p><strong>langchain&#x2F;dify的智能体</strong></p><p>Langchain支持各种模型的接入、提示词管理、基于向量库的检索增强、索引优化、链式调用和代理行动决策，适用于创建自治代理、模拟、个人助理、问答系统、聊天机器人、数据查询、代码理解、API交互、信息提取、文本摘要和模型评估等多种应用场景。</p><p>dify则是国内开源的一个agent平台，类似于带界面的Langchain。</p><p><strong>m﻿ilvus向量库</strong></p><p>Milvus是在2019年创建的，其唯一目标是存储、索引和管理由深度神经网络和其他机器学习（ML）模型生成的大规模嵌入向量。</p><p>作为一个专门设计用于处理输入向量查询的数据库，它能够处理万亿级别的向量索引。与现有的关系型数据库主要处理遵循预定义模式的结构化数据不同，Milvus从底层设计用于处理从非结构化数据转换而来的嵌入向量。Milvus使得向应用中添加相似性搜索变得容易。</p><p><strong>嵌入模型</strong></p><p>嵌入模型（Embedding Model）是一种在自然语言处理（NLP）和机器学习中广泛使用的技术，旨在将高维的稀疏数据（如单词、句子或图像）转换为低维的密集向量表示。这种向量表示能够捕捉到数据的语义信息和结构特征，使得计算机能够更有效地处理和分析数据。</p><p>在NLP中，单词嵌入（Word Embedding）是最常见的嵌入模型之一。它将每个单词映射到一个固定长度的实数向量，使得语义上相似的单词在向量空间中也相互接近。这种表示方式可以有效地减少数据的维度，并捕捉单词之间的语义关系，如同义词、反义词和上下文相关性。</p><p>嵌入模型通常通过无监督学习方法从大量文本数据中学习得到，如Word2Vec、GloVe和fastText等。这些模型利用单词的共现信息和上下文关系来学习单词的向量表示。</p><p>除了单词嵌入，还有句子嵌入（Sentence Embedding）、段落嵌入（Paragraph Embedding）和图像嵌入（Image Embedding）等，它们分别用于将句子、段落和图像转换为密集向量表示，以便进行后续的机器学习任务，如文本分类、情感分析、图像识别等。</p><p>目前中文嵌入模型效果较好的推荐m3e向量模型。</p><p><strong>本地自有模型&#x2F;外部模型接口</strong></p><p>本地部署可以使用api-for-open-llm，该项目是一个开源框架，提供了统一的后端接口，使得以 OpenAI ChatGPT API 的方式调用各类开源大模型变得简单。</p><p>它支持流式响应、文本嵌入模型、langchain 的各类功能，并允许通过简单修改环境变量将开源模型作为 ChatGPT 的替代模型。此外，该项目还支持加载自行训练的 lora 模型，以及 vLLM 推理加速和处理并发请求，为各类应用提供强大的后端支持。</p><p>接口的话推荐使用gemini，在今年5月份之前gemini还是可以免费使用的。</p><p><strong>提示词写法</strong></p><p>以基于知识库的官网问答系统的提示词为例，下面是一个写好的提示词样例：</p><p>在XML标记中使用以下上下文作为您学到的知识。{上下文}</p><p>当回答用户:-如果你不知道，就说你不知道。</p><p>-如果你不知道，当你不确定，要求澄清。</p><p>避免提到你是从上下文中获得信息的。并根据用户提问的语言进行回答。</p><p>下面是人与助手之间的聊天历史记录，位于&lt;历史&gt;{聊天记录} XML标记中。</p><p><strong>总结</strong></p><p>基于大模型的智能问答系统利用 LangChain 框架和相关技术，如 Milvus 向量库和嵌入模型，提供了一个强大的解决方案，用于构建理解自然语言并提供准确回答的系统。通过集成不同的模型、管理提示词、利用向量检索和链式调用，这个系统能够处理复杂的用户查询，并根据上下文提供相关的信息。此外，系统的灵活性和可扩展性使得开发者可以根据特定需求定制和优化问答逻辑，从而提高用户体验和满意度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LangChain-基于大模型的智能问答&quot;&gt;&lt;a href=&quot;#LangChain-基于大模型的智能问答&quot; class=&quot;headerlink&quot; title=&quot;LangChain 基于大模型的智能问答&quot;&gt;&lt;/a&gt;LangChain 基于大模型的智能问答&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="深度学习" scheme="http://ai.mak.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>怎样拥有不缺席的力量</title>
    <link href="http://ai.mak.cn/2024/06/14/%E6%9D%82%E8%AE%B0/%E6%80%8E%E6%A0%B7%E6%8B%A5%E6%9C%89%E4%B8%8D%E7%BC%BA%E5%B8%AD%E7%9A%84%E5%8A%9B%E9%87%8F/"/>
    <id>http://ai.mak.cn/2024/06/14/%E6%9D%82%E8%AE%B0/%E6%80%8E%E6%A0%B7%E6%8B%A5%E6%9C%89%E4%B8%8D%E7%BC%BA%E5%B8%AD%E7%9A%84%E5%8A%9B%E9%87%8F/</id>
    <published>2024-06-13T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.731Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>节选自：得到头条 | 398</p></blockquote><p>今天，将从两个话题出发，为你提供知识服务。第一个是，2024年父亲节特刊。第二个是，离职博主，成为新晋流量密码。</p><p>先来看今天的第一条。这周日，是2024年的父亲节。其实，父亲节本来没有固定日期，很多地方的父亲节日期也不一样。比如，韩国的父亲节是5月8日，确切说，应该叫双亲节。相当于把父亲节和母亲节放在了一起。再比如，泰国的父亲节是12月5日，因为这天是被称为泰国国父的泰国第九任国王，普密蓬·阿杜德的生日，这也是泰国历史上在位时间最长的国王。</p><p>而全世界使用最多的父亲节日期，包括咱们中国在内，是每年6月的第三个星期日。放在今年，就是这周日，6月16日。</p><p>谈起父亲节这个话题，好多人都特别容易沉重，好像不拿出点父爱如山的架势，这个话题就谈论不下去。但是今天，咱们稍微放松点。先不谈父亲这个角色的历史使命，也不说父亲给孩子一生的影响。这些宏大的话题先放一放。我主要想跟你分享一下，我最近看到的一句描述父母的话，是这么说的。</p><p><strong>真正健康而美丽的人格，一定是雌雄同体的。这个世界上最美的艺术品，莫过于英气逼人的母亲和温情脉脉的父亲。</strong></p><p>注意，这句话说的，恰恰和好多人认为的父爱如山不太一样。它强调的，是父亲这个角色中，柔软的一面。</p><p>说这话的人，是武汉中德心理医院的创始人，中国心理卫生协会精神分析学组的副组长曾奇峰。这句话出自曾奇峰为一本书写的序。书名就叫《何以为父》。作者是斯坦福大学的一位精神分析学家，迈克尔·J.戴蒙德。</p><p>按照曾奇峰的观点，一个好父亲，至少要满足这么三个条件。</p><p>首先，好父亲多少带点孩子气。你不用想着非得为孩子做点什么，假如做不了，就陪着孩子一起玩耍。</p><p>其次，好父亲未必严厉，他往往是温情脉脉的，就像前面说的，美丽的人格，都是雌雄同体的。</p><p>最后，好父亲一定不拧巴。<strong>不是说一个人成为父亲之后，就必须得变成某个特定的样子。而是你在保持自我的同时，又承担了父亲的角色。</strong> 换句话说，父亲不一定非得严肃，不一定非得扮演大山。借用曾奇峰的话说，你看父亲的父字，就像一双手举着两根棍子。但是，注意，只是举着，那个棍子不是还没打下去吗？打下去，这个字就不念父了。说白了，父亲大可不必刻意严厉，不必整天说教。当爹的，不用强行塑造爹味儿。</p><p>那么，作为父亲，最重要的角色是什么呢？<strong>按照《何以为父》中的观点，你能做的最重要的事情之一，就是在孩子成长的各个阶段，都陪伴在孩子身边。没错，能做到不缺席，是成为一个好父亲最重要的前提。</strong> </p><p>有个法国记者叫马修·阿伦，他在跟踪调查一起入狱案件的时候，有一个意外的发现。他发现，在美国的监狱里，很多穷凶极恶的犯人，都特别看重母亲节这个日子。每逢母亲节，很多人还会想方设法为母亲准备一份礼物。但父亲节，压根就没人提，就好像这个日子不存在一样。</p><p>为什么？美国司法部在1988年发布过一组数据，有63%的自杀者，来自缺失父亲的家庭。70%的少年犯，来自没有父亲的家庭。当然，这是很久以前的数据，根据最新的统计，单亲家庭孩子在各方面呈现出的数据，已经与双亲家庭差不多。但是，父亲缺席造成的影响依然存在。加拿大有个组织，叫父亲参与研究联盟。他们早前做过一份统计，发现在父亲陪伴下长大的孩子，在耐挫力、抗压力，以及控制情绪的能力上，都比缺少父亲陪伴的孩子要高一点。</p><p>但是，说到这，你可能会说，仅仅是陪伴就够了吗？这个要求未免太低了吧？假如你这么想，不好意思，你可能就把这个事给想简单了。</p><p>这个事的难度，不在于搞懂宏观道理，而在于，抗拒微观影响。也就是，道理层面我都理解，但是，具体到行动，总有一些很具体的微观影响，阻挡我实践这个道理。</p><p>比如，孩子出生的时候。多数孩子是在医院的产房里出生的。父亲不是第一时间在身边，在见到孩子的第一面时，很多父亲都不敢伸手去抱孩子。</p><p>再比如，在孩子的青春期，孩子会叛逆，会习惯性地跟父母抬杠。这个时期，同学朋友在他们的世界里占据更大的比重，他们想要自己独立的空间，想离父母稍微远一点。</p><p>再比如，在孩子的成长过程中，父母大概率会经历一个跌落神坛的过程。在童年时期，全世界最厉害的人是你。而到了叛逆期，全世界最无知的人也是你。你就想象这个状态，好多父母都会不知所措。既然不知道怎么办，索性就不办，干脆放手，不管了。你看，到这步，缺席是不是又出现了？</p><p>换句话说，<strong>始终不缺席这事，难度比咱们想象中要高。总有千奇百怪的原因，阻挡你践行这个道理。</strong> 之前有位经济学家叫罗伯特·哈姆林，写过一本书，叫《好爸爸不缺席》。这个书名其实是一语双关，正着读，是好爸爸不应该缺席。反着读，就是只要不缺席，就算是好爸爸。</p><p>好，不缺席，这大前提就摆在这。接下来，具体到行动，这里有两个参考。</p><p>第一个，来自《何以为父》的作者迈克尔·J.戴蒙德。他说，跟母亲相比，父亲最大的价值在于，他更能提供一个让孩子安全犯错的环境。没错，与其说父爱如山，倒不如说更像地基。他的价值在于，能让孩子踏踏实实地尝试，能尽快走出一次次的失败和逆境。因此，<strong>作为父亲，可以多鼓励孩子尝试他们不是很擅长的事，并且陪伴他们在逆境中成长。</strong></p><p>第二个建议，来自咱们得到的CEO脱不花老师。脱不花老师说，在子女的童年阶段，家里可以设立一个父亲日。父亲每周拿出半天或者一天，什么都不做，只陪着孩子，并且满足他们的一切要求。去游乐园、吃冰激凌、买好看的衣服，通通满足。这个方法，能让父亲和孩子快速黏在一起。</p><p>当然，跳出具体的方法之外，回到那个最高原则，最重要的还是陪伴。人类学家项飙老师曾经写过一本书，叫《把自己作为方法》。放在当父亲这件事上，或许也成立。把自己作为方法，你自己就是方法。始终陪在孩子身边，比什么都重要。</p><p>借用精神分析师孙平的一段话，是这么说的。<strong>父亲们，活下来，活下去。在孩子童年，不死亡，不退场。熬过生活的艰辛，熬过妻子从对你向对孩子的情感转移，熬过孩子对你的亲近和依恋，熬过他们对你的理想化，熬过他们的愤怒，熬过他们的失望，熬过他们把你一会儿视为神和一会儿视为虫的戏剧性起伏。</strong> 最终在他们心中成为一个普通的却深爱着他们的老男人。你还站在那里，你还在坚韧地存在着，只因为你是一名父亲。</p><p>再来看今天的第二条。最近在抖音上，有个赛道特别火，叫离职博主。据说这已经成了2024年最拥挤的赛道之一。那么，离职博主主要做什么呢？</p><p>看几个例子你就明白了。比如，离职空姐分享日用好物、大厂离职员工告诉你职场潜规则、离职创业的365天、离职之后我才想清楚这8个人生道理、天知道我离职时HR对我说了什么，等等。据说在离职博主里，又细分成字节离职赛道、腾讯离职赛道、阿里离职赛道等等。</p><p>尽管看起来五花八门，但实际上，这些博主的主题不外乎分成这么三类。</p><p>第一类，提供爽感。你不敢辞的职，我替你辞，你不敢说的话，我替你说。说白了，就是当打工人的嘴替，替你宣泄心中的情绪。</p><p>第二类，提供信息差。主打揭露所谓的职场潜规则。但实际内容，跟一般的职场培训课很接近。只不过，顶着离职博主的头衔，这些信息就好像显得更真实。</p><p>第三类，讲创业故事。主打的是离职去创业，实际上以摆摊的居多。假如你早上7点或者晚上7点打开抖音，会看到很多早餐或者夜市摆摊的直播，其中有不少就是离职博主。</p><p>当然，这里面不排除有剧本，或者自导自演的成分。但是，我们今天要说的重点，倒不是离职博主本身，而是透过离职博主，你有没有感受到一个信号。就是现在的抖音赛道，都玩得这么野了吗？前段时间，我还看到一个类型，叫公交博主。也就是，坐着公交车去另一个城市。而且公交博主之间内卷到什么程度？以前是坐公交从北京去哈尔滨。现在已经发展到坐公交从北京去拉萨。</p><p>注意，前面这些，可不是什么凤毛麟角的尝试，而是已经有很多人加入，成为一个特定的品类，内容上已经形成特定的章法、流程和格式。</p><p>说到这，就引出一个问题。这么千奇百怪的设定，都是怎么想出来的呢？</p><p>今天，咱们就来回答这个问题。乍一看，这个事好像很依赖创意。毕竟，故事的创作天马行空，哪有什么规则？但事实上，<strong>在编剧的圈子里，一直流传一个说法，故事的可能性，其实是有限的。</strong></p><p>比如，法国剧作家乔治·普罗第曾经总结过，他认为所有的戏剧，本质上都可以划分成36种模型。而你看到的所有的故事，不外乎是这36种模型的排列组合。包括，不幸、壮举、复仇、人和神的斗争、恋爱一个仇敌，等等。</p><p>再比如，有个叫维多利亚·林恩·施密特的故事导师，也提出过一个观点，认为所有的角色，不外乎就45种类型。没错，正派反派加起来，都跑不出这45种类型。包括蛇蝎美人、救世主与毁灭者、神秘主义者与背叛者，等等。</p><p>尽管角度不同，但这些观点都指向一个结论。这就是，<strong>故事不是纯粹的无中生有，它是特定桥段的排列组合。</strong> 比如，离职博主去创业，按照普罗第的分类，就属于鲁莽+壮举。再比如，离职博主diss前上司，就属于不幸+复仇。</p><p>从这个角度看，<strong>生产创意其实有一个相对确定的路径，大概分成两步。第一步，记录。把你看过的有趣的桥段，全都记录在一个文档里。包括，有哪些有趣的角色、有哪些好玩的故事包袱。第二步，组合。也就是，把你记下来的桥段重新排列，没准就会出现一个有趣的创意。</strong></p><p>最后，总结一下，今天说了两个话题。</p><p>第一，父亲在孩子成长中扮演什么角色？我们说了迈克尔·J.戴蒙德的观点。他认为，父亲最应该做的事情，是始终陪在孩子身边。父亲的角色与其说像高山，倒不如说更像地基，为孩子提供一个能安全犯错的成长空间。</p><p>第二，怎么创造有趣的故事点子？很多时候，故事的创作，并不是漫无边界的想象，而是基于特定模板的排列组合。关键在于，积攒足够多的零件，并且做出足够丰富的排列组合。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;节选自：得到头条 | 398&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今天，将从两个话题出发，为你提供知识服务。第一个是，2024年父亲节特刊。第二个是，离职博主，成为新晋流量密码。&lt;/p&gt;
&lt;p&gt;先来看今天的第一条。这周日，是2024年的父亲节</summary>
      
    
    
    
    <category term="杂记" scheme="http://ai.mak.cn/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
    <category term="杂记" scheme="http://ai.mak.cn/tags/%E6%9D%82%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Three.js 开源组件</title>
    <link href="http://ai.mak.cn/2024/06/02/%E5%89%8D%E7%AB%AF/Three.js%20%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"/>
    <id>http://ai.mak.cn/2024/06/02/%E5%89%8D%E7%AB%AF/Three.js%20%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/</id>
    <published>2024-06-01T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.520Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Three-js-开源组件"><a href="#Three-js-开源组件" class="headerlink" title="Three.js 开源组件"></a>Three.js 开源组件</h1><p><code>Three.js</code> 是一款基于 <code>WebGL</code> 的强大 <code>JavaScript</code> 库，让浏览器中的 <code>3D</code> 图形创建与显示变得前所未有的简单。</p><p>通过精心设计的 API，Three.js 成功地降低了 <code>WebGL</code> 的<code>复杂性</code>，使得即使是没有深厚技术背景的开发者也能轻松打造出令人惊艳的 <code>3D 场景</code>、<code>模型</code>、<code>动画</code>以及<code>粒子系统</code>。</p><p>虽然 Three.js 目的是为了简化开发者创建和显示 3D 图形的难度，但对于初学者来说，上手仍然可能会有一定的<code>挑战性</code>。这主要是因为以下几个方面的原因：</p><ul><li><strong>技术门槛</strong>：尽管 Three.js 简化了 WebGL 的复杂性，但它仍然涉及到<code>图形学</code>、&#96;&#96;线性代数、<code>3D 编程</code>等相对复杂的概念。对于没有相关背景知识的开发者来说，理解这些概念可能需要一定的时间和努力。</li><li><strong>API 学习曲线</strong>：Three.js 提供了一个<code>庞大的 API</code>，涵盖了从场景创建、物体渲染到光照、动画等多个方面。初学者需要花费一定的时间去熟悉这些 API，并了解它们的使用方法和最佳实践。</li><li><strong>编程经验</strong>：编写 3D 应用程序通常比编写传统的 2D 应用程序&#96;&#96;更加复杂，需要处理更多的数据和对象。因此，具备一定的编程经验和逻辑思维能力会更有助于上手 Three.js。</li><li><strong>资源获取</strong>：找到高质量的<code>教程</code>、<code>示例代码</code>和<code>项目模板</code>对于初学者来说非常重要。然而，网络上的资源可能质量参差不齐，需要花费一定的时间和精力进行筛选和学习。</li></ul><p>今天就来给大家推荐两个开源项目合集，带你快速入门 <code>Three.js</code>！！！</p><h2 id="vis-three"><a href="#vis-three" class="headerlink" title="vis-three"></a><strong>vis-three</strong></h2><blockquote><p>开源地址：<code>https://github.com/vis-three</code></p></blockquote><p><code>vis-three</code> 是一款基于 three.js 的<code>组装式</code>前端 3D 开发框架，为了解决 three.js 相关项目开发的<code>代码组织</code>问题，降低功能代码间的耦合，提高功能复用性，提高扩展能力，vis-three 提出了<code>功能插件</code>化概念，采用了<code>插件化</code>的组织形式，vis-three 的核心引擎提供功能插件的拔插能力，对各模块各功能进行解耦开发，持续集成，兼容拓展。</p><blockquote><ul><li>演示地址：<code>https://vis-three.github.io/exhibition-hall//SimpleSmartCity/index.html</code></li></ul></blockquote><blockquote><ul><li>演示地址：<code>https://vis-three.github.io/exhibition-hall//easyFactory/index.html</code></li></ul></blockquote><h2 id="icegl-three-vue-tres"><a href="#icegl-three-vue-tres" class="headerlink" title="icegl-three-vue-tres"></a><strong>icegl-three-vue-tres</strong></h2><blockquote><p>开源地址：<code>https://gitee.com/ice-gl/icegl-three-vue-tres</code></p></blockquote><p><code>icegl-three-vue-tres</code> 是 <code>icegl 图形学社区</code> 出品，一款让你的三维可视化项目快速落地的开源框架并且永久开源免费商用。</p><p>icegl 图形学社区提供了上百个 Threejs <code>可视化案例</code>供开发者学习，并且有免费完善的 WebGL 从入门到实战的<code>教程</code>，帮助大家快速上手。</p><p>案例展示：</p><blockquote><ul><li>演示地址：<code>https://opensource.icegl.cn/#/plugins/digitalCity/roadLines</code></li></ul></blockquote><blockquote><ul><li>演示地址：<code>https://opensource.icegl.cn/#/plugins/simpleGIS/mapBuildings</code></li></ul></blockquote><blockquote><ul><li>演示地址：<code>https://opensource.icegl.cn/#/plugins/industry4/deviceLightReflector</code></li></ul></blockquote><p>参考连接:</p><ul><li><a href="https://github.com/vis-three/vis-three">https://github.com/vis-three/vis-three</a></li><li><a href="https://gitee.com/ice-gl/icegl-three-vue-tres">https://gitee.com/ice-gl/icegl-three-vue-tres</a></li><li><a href="https://icegl.cn/">https://icegl.cn/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Three-js-开源组件&quot;&gt;&lt;a href=&quot;#Three-js-开源组件&quot; class=&quot;headerlink&quot; title=&quot;Three.js 开源组件&quot;&gt;&lt;/a&gt;Three.js 开源组件&lt;/h1&gt;&lt;p&gt;&lt;code&gt;Three.js&lt;/code&gt; 是一款基</summary>
      
    
    
    
    <category term="前端" scheme="http://ai.mak.cn/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="js" scheme="http://ai.mak.cn/tags/js/"/>
    
  </entry>
  
  <entry>
    <title>深入理解Transformer</title>
    <link href="http://ai.mak.cn/2024/05/18/ai/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Transformer%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/"/>
    <id>http://ai.mak.cn/2024/05/18/ai/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Transformer%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/</id>
    <published>2024-05-17T16:00:00.000Z</published>
    <updated>2025-02-11T03:10:23.330Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深入理解Transformer"><a href="#深入理解Transformer" class="headerlink" title="深入理解Transformer"></a>深入理解Transformer</h1><blockquote><p>作者：得物技术<br>链接：<a href="https://juejin.cn/post/7358743626166222874">https://juejin.cn/post/7358743626166222874</a><br>来源：稀土掘金</p></blockquote><blockquote><ul><li>本文深入介绍了 Transformer 技术原理，包括注意力机制、架构设计、Token 处理、编解码器工作流程等，还讲解了 Transformer-XL 提升上下文长度的方法，分享了 Transformer 相关应用，如 BERT 掩词填充、BART 文本摘要等，并提供了参考文档。</li></ul></blockquote><p>谷歌在2017年发布Transformer架构的论文时，论文的标题是：Attention Is All You Need。重点说明了这个架构是基于注意力机制的。</p><h2 id="一、什么是注意力机制"><a href="#一、什么是注意力机制" class="headerlink" title="一、什么是注意力机制"></a><strong>一</strong>、<strong>什么是注意力机制</strong></h2><p>在深入了解Transformer的架构原理之前，我们首先要了解下，什么是注意力机制。人类的大脑对于信息的获取也存在注意力机制，下面我举几个简单的例子：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0012f40763e84cc8baa4a6063c572357~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=293&s=137911&e=png&b=daf9fe" alt="图片">从上面的图片中，我们可能更容易关注，颜色更深的字、字号更大的字，另外像“震惊”这种吸引人眼球的文案也非常容易吸引人的关注。我们知道在海量的互联网信息中，往往那些起着“标题党”的文章更能吸引人的注意，从而达到吸引流量的目的，这是一种简单粗暴的方式。另外在大量的同质化图片中，如果有一张图片它的色彩、构图等都别出一格，那你也会一眼就能注意到它，这也是一种简单的注意力机制。假设有以下这两段文字，需要翻译成英文：<strong>1、我在得物上买了最新款的苹果，体验非常好。</strong> <strong>2、我在得物上买了阿克苏的苹果，口感非常好。</strong> 我们人类能很快注意到第一段文字中的苹果是指苹果手机，那么模型在翻译时就需要把他翻译成iPhone，而第二段文字中的苹果就是指的苹果这种水果，模型翻译时就需要将他翻译成apple。人类的大脑为什么能分辨出这两个苹果是指代的不同的意思呢？原因就是人类的大脑能从上下文中获取到关键信息，从而帮助我们理解每种苹果是什么意思。其实说到这里，我们就已经揭开了Transformer架构的核心，即注意力机制的原理：从文本的上下文中找到需要注意的关键信息，帮助模型理解每个字的正确含义。但是实际的实现方式又是非常复杂的。接下来让我们一起深入理解下Transformer的架构原理。</p><h2 id="二、Transformer架构设计"><a href="#二、Transformer架构设计" class="headerlink" title="二、Transformer架构设计"></a><strong>二</strong>、<strong>Transformer架构设计</strong></h2><p>Transformer的架构设计如下图所示：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/59a5ad40a7f348feb23baf45a07b2dfe~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=672&h=1066&s=16767&e=png&b=fff6df" alt="图片"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ee0296b74bc4dd5ad99c446d3c8c115~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=680&h=980&s=241104&e=png&b=fdf7f7" alt="图片">Transformer架构中有两个核心的组件Encoder和Decoder，左边的这张图是Transformer架构的一个简单表示形式，右边的这张图是Transformer架构的一个完整表示形式，其中有一个重要的Multi-Head Attention组件，称为注意力层。Transformer架构中的两个核心的组件Encoder和Decoder，每个组件都可以单独使用，具体取决于任务的类型：</p><ul><li>Encoder-only models: 适用于需要理解输入的任务，如句子分类和命名实体识别。</li><li>Decoder-only models: 适用于生成任务，如文本生成。</li><li>Encoder-decoder models 或者 sequence-to-sequence models: 适用于需要根据输入进行生成的任务，如翻译或摘要。</li></ul><h2 id="三、理解Transformer中的Token"><a href="#三、理解Transformer中的Token" class="headerlink" title="三、理解Transformer中的Token"></a><strong>三</strong>、<strong>理解Transformer中的Token</strong></h2><p>因为模型是无法直接处理文本的，只能处理数字，就跟ASCII码表、Unicode码表一样，计算机在处理文字时也是先将文字转成对应的字码，然后为每个字码编写一个对应的数字记录在表中，最后再处理。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ab0e81aa7c724b89862bc74f56667b48~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=713&h=368&s=231277&e=png&b=eeee80" alt="图片"></p><p><strong>将文本拆分成token</strong></p><p>所以模型在处理文本时，第一步就是先将文本转换成对应的字码，也就是大模型中的token，但是怎么将文本转换成对应的token却是一个复杂的问题，在Transformers(HuggingFace提供的一个对Transformer架构的具体实现的组件库)中提供了专门的Tokenizer分词器来实现这个任务，一般来说有以下几种方式：</p><p><strong>基于单词的分词器</strong></p><p>第一种标记器是基于单词的(word-based)。它通常很容易设置和使用，只需几条规则，并且通常会产生不错的结果。例如，在下图中，目标是将原始文本拆分为单词，并为每个单词找到一个映射的数字表达：将文本拆分成单词，也有很多不同的方式，比如通过空格来拆分、通过标点符号来拆分。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/093a0cc2b57e4b878bc43d525fb555b1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=234&s=28625&e=png&b=ffffff" alt="图片">如果我们想使用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都创建一个数字标记，这将生成大量的标记。除此之外，还可能存在一些无法覆盖的单词，因为单词可能存在很多的变种情况，比如：dogs是dog的变种，running是run的变种。如果我们的标识符中没有覆盖所有的单词，那么当出现一个未覆盖的单词时，标记器将无法准确知道该单词的数字标记是多少，进而只能标记为未知：UNK。如果在文本转换的过程中有大量的文本被标记为UNK，那么也将影响后续模型推理。</p><p><strong>基于字符的标记器</strong></p><p>为了减少未知标记数量的一种方法是使用更深一层的标记器(tokenizer)，即基于字符的(character-based)标记器(tokenizer)。基于字符的标记器(tokenizer)将文本拆分为字符，而不是单词。这有两个主要好处：</p><ul><li>词汇量要小得多。</li><li>未知的标记(token)要少得多，因为每个单词都可以从字符构建。</li></ul><p>但是这里也出现了一些关于空格和标点符号的问题：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/678e68a6a5f3414a946017eede9d777d~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=77&s=15103&e=png&b=f4efff" alt="图片">这种方法也不是完美的。由于现在表示是基于字符而不是单词，因此人们可能会争辩说，从直觉上讲，它的意义不大：每个字符本身并没有多大意义，而单词就是这种情况。然而，这又因语言而异；例如，在中文中，每个字符比拉丁语言中的字符包含更多的信息。另一件要考虑的事情是，我们的模型最终会处理大量的词符(token)：虽然使用基于单词的标记器(tokenizer)，单词只会是单个标记，但当转换为字符时，它很容易变成 10 个或更多的词符(token)。为了两全其美，我们可以使用结合这两种方法的第三种技术：<strong>子词标记化(subword tokenization)</strong> 。</p><p><strong>基于子词的标记器</strong></p><p>子词分词算法依赖于这样一个原则，<strong>即不应将常用词拆分为更小的子词，而应将稀有词分解为有意义的子词。</strong> 例如，“annoyingly”可能被认为是一个罕见的词，可以分解为“annoying”和“ly”。这两者都可能作为独立的子词出现得更频繁，同时“annoyingly”的含义由“annoying”和“ly”的复合含义保持。下面这张图，展示了基于子词标记化算法，如何标记序列“Let’s do tokenization!”：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/56d7c23fbcfa4978bb05d6b485b51514~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=79&s=19357&e=png&b=f4efff" alt="图片">这些子词最终提供了很多语义含义：例如，在上面的示例中，“tokenization”被拆分为“token”和“ization”，这两个具有语义意义同时节省空间的词符(token)（只需要两个标记(token)代表一个长词）。这使我们能够对较小的词汇表进行相对较好的覆盖，并且几乎没有未知的标记。</p><p><strong>向量、矩阵、张量</strong></p><p>了解完token之后，我们还要了解下向量、矩阵和张量的概念，因为他们是大模型计算中基础的数据结构。</p><p><strong>向量(Vector)</strong></p><p>向量是一个有序的数字列表，通常用来表示空间中的点或者方向。在数学中，向量可以表示为一个列向量或行向量，具体取决于上下文。例如，一个三维空间中的点可以用一个三维列向量表示，如 <strong>v</strong>&#x3D;[x,y,z]T，其中 x,y,z 是实数。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c4d6e10f7c749bbac754abd7a0fed51~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=580&h=148&s=2904&e=png&b=b1dd9f" alt="图片"></p><p><strong>矩阵(Matrix)</strong></p><p>矩阵是一个二维数组，由行和列组成，可以被视为向量的一个特例。矩阵在数学中用于表示线性变换、系统方程的系数等。矩阵的维度通常表示为 m×n，其中 m 是行数，n 是列数。例如，一个 4×3 的矩阵有四行三列。</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e8c534a55e44f8e91289aad7224bfac~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=320&h=418&s=2874&e=png&b=dcdde0" alt="图片"></p><p><strong>张量(Tensor)</strong></p><p>张量是一个多维数组，可以看作是向量和矩阵的更底层的表示，向量和矩阵是张量的特例。例如向量是一维的张量，矩阵是二维的张量。张量可以有任意数量的维度，而不仅仅是一维（向量）或二维（矩阵）。张量在物理学中用来表示多维空间中的物理量，如应力、应变等。在深度学习中，张量用于表示数据和模型参数的多维结构。</p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8b49f57afd704065b77f84befd690645~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=376&h=308&s=17035&e=png&b=f3c890" alt="图片"></p><p><strong>将token转换成向量</strong></p><p>在获取到token之后，再将每个token映射为一个数字，当然了，Transformer能够处理的数据，并不是简单的1&#x2F;2&#x2F;3这样的数字，而是一种向量数据，如下图所示：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa5c6f8c2cd646d98330eaf04fb13dea~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=776&h=282&s=6746&e=png&b=ffffff" alt="图片"></p><p><strong>将向量转换成嵌入</strong></p><p>得到向量之后，再将向量转换成词嵌入，也就是我们所熟知的embeddings。在Transformer模型中，编码器接收的词嵌入（embeddings）可以被视为矩阵。这些词嵌入是将输入序列中的每个token映射到一个固定维度的向量空间中的结果。每个词嵌入都是一个向量，而这些向量按顺序排列形成一个矩阵。具体来说，如果你有一个句子或序列，其中包含了N个token，每个token都被映射到一个d维的向量空间中，那么你将得到一个N×d的矩阵，其中N是序列的长度，d是嵌入向量的维度。这个矩阵就是词嵌入矩阵，它是一个二维张量，因为它具有两个维度：序列长度（时间步长）和嵌入维度。在Transformer模型的编码器中，这个嵌入矩阵首先会通过一个线性层（可选）进行处理，然后添加位置编码（positional encoding），最后输入到自注意力（self-attention）和前馈网络（feed-forward network）等组件中进行进一步的处理，具体细节我接下来会进行详细解释。总结来说，编码器接收的词嵌入是一个矩阵，这个矩阵可以被视为一个二维张量，其中包含了序列中每个词的d维向量表示。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f92ebed94e1f43e99a85449e0b1cb216~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=562&s=181589&e=png&b=dafcfe" alt="图片"></p><h2 id="四、理解Transformer的编解码器"><a href="#四、理解Transformer的编解码器" class="headerlink" title="四、理解Transformer的编解码器"></a><strong>四</strong>、<strong>理解Transformer的编解码器</strong></h2><p>下面让我们以文本翻译来深入理解Transformer中的Encoder和Decoder是怎样工作的，假设我们有以下这个翻译任务，输入是一段法文，输出是英文。整个流程是Transformer将输入的input，经过Encoders处理后，将结果投递到Decoders中处理，最后输出翻译后的结果。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8288479eccc94453b9cfdf9d048f9c65~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=697&h=422&s=32125&e=png&b=fdfaf9" alt="图片">但是实际在Transformer的内部，是由多个独立的Encoder和Decoder组成的，这里我们使用6个做验证，当然我们也可以使用其他数量的Encoder和Decoder，笔者怀疑6个是经过验证后相对折中的一个值。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eefd8b316e414ad193183072eaaa62e1~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=900&h=610&s=104117&e=png&b=fdfbfa" alt="图片">这6个Encoder和Decoder在结构上都是相同的，但是Encoder和Decoder的内部还有更细分的组件：每一层的Encoder由2个子组件组成：自注意力层和前馈网络层，<strong>其中文本的输入会先流入自注意力层，正是由于自注意力层的存在，帮助Encoder在对特定文本进行遍历时，能够让Encoder观察到文本中的其他单词</strong>。然后自注意力层的结果被输出到前馈网络层。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bca0128b67d54c8caf9630f707ca9100~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=749&h=339&s=27550&e=png&b=ffffff" alt="图片">每一层的Decoder由3个子组件组成：除了自注意力层、前馈网络层，<strong>在两者之间还有一个编解码注意力层，这个组件主要是帮助Decoder专注于输入句子的相关部分</strong>。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f873eb1637ac46d8ab79952502b65af4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=877&h=291&s=34928&e=png&b=ffffff" alt="图片"></p><h2 id="五、理解Token在编码器中的流转"><a href="#五、理解Token在编码器中的流转" class="headerlink" title="五、理解Token在编码器中的流转"></a><strong>五</strong>、<strong>理解Token在编码器中的流转</strong></h2><p>现在我们已经知道了Transformer模型中的核心组件Encoder和Decoder，接下来我们来看Token在Transformer中是怎么流转的，换句话说Encoder和Decoder是怎么处理Token的。拿最开始的法文翻译的例子，模型将文本转换token后，紧接着就是将每个token转换成向量表达，在下图中，我们用x1、x2、x3这样的box来表示每个token的向量：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f6062eb2df44c88b6ae3afa6dcb054b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=773&h=138&s=21424&e=png&b=ffffff" alt="图片">得到每个token的向量之后，从最底层的Encoder开始，每个token会沿着自己的路径从下往上流转，经过每一个Encoder，对每个Encoder来说，共同点是他们接收的向量的维度都是相同的，为了保证这一点，所有的token都需要被embedding成相同大小的向量。</p><p><strong>对Token进行位置编码</strong></p><p>在流经每个Encoder时，向量都会从自注意力层流向前馈层，如下图所示：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c25b413c86074abd8d2597cfef9539dd~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=903&h=590&s=52118&e=png&b=fff8f7" alt="图片">这里需要注意的是，<strong>不同位置的向量在进入自注意力层的时候，相互之间是有依赖关系的，这就是注意力层需要关注的上下文的信息。</strong> 而当自注意力层处理后的向量进入前馈层后，前馈层是可以并行计算以提升性能的，因为在前馈层中向量之间不存在依赖关系。每个向量在经过Encoder处理后，得到的仍然是一个相同大小的向量，然后再提交给下一个Encoder进行处理。</p><p>为什么说不同位置的向量相互之间是有依赖关系的呢？我们可以想象一下，如果不关注一整个句子中token的位置信息，那么翻译出来的结果是不准确的，比如：</p><ol><li><strong>Sam is always looking for trouble</strong></li><li><strong>Trouble is always looking for Sam</strong></li></ol><p>为了让模型知道每个token的位置信息，传统的RNN网络的做法是，顺序处理每个token，这样在处理当前token时，可以往前查看已经处理过的token的信息，但是缺点是所有的token节点都共用一套相同的参数，即下图中的：</p><p>U：输入参数</p><p>W：隐藏参数</p><p>V：输出参数</p><p><strong>由于RNN的窗口较小，这种方案带来的问题是，当token数变大时，模型无法参考更早之前已经参考过的token，这样就会造成上下文记忆丢失。</strong> <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3478560935041b38b8802ca67062ad0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=864&h=353&s=98342&e=png&b=fefefe" alt="图片">那么Transformer是怎么对token进行位置编码的呢？首先我们知道每个token会被转换成512维(或更高的维度)的向量，比如：[0.12142,0.34181,….,-0.21231] 可以将这个向量分为两个部分，奇数和偶数部分。奇数部分使用cos函数，加上当前token的位置信息pos，通过cos编码得到一个奇数编码值；偶数部分使用sin函数，加上当前token的位置信息pos，通过sin编码得到一个偶数编码值；<strong>最后拿token的embeddings和pos的embeddings相加，得到位置编码后的positional input embeddings。</strong> <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1f11e50479854b6fa1ddfa8268c9a0a5~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=877&h=676&s=129759&e=png&b=010101" alt="图片"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/56f435fdbe9a4b7d92a3a1533a78c030~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=884&h=681&s=129345&e=png&b=010101" alt="图片"></p><p><strong>自注意力机制</strong></p><p>有了位置编码的信息后，模型将接收经过位置编码的embeddings输入，为了方便描述，我们把token换成更简单的文本，如下图所示，Encoder在接收到两个向量之后，通过自注意力层，将原始向量转换成携带了自注意力值的向量，也就是图中的z1和z2。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b5778c1edfc64a4c9c42621c79f69f5b~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=657&s=64603&e=png&b=fff9f8" alt="图片"></p><p><strong>计算注意力值</strong></p><p>那z1和z2这两个向量是怎么得到的呢？原论文中给出了计算公式：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/82707b451c4442b09b2c468889eea6c2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=440&h=82&s=7578&e=png" alt="图片">这个公式是用来计算注意力值的，借助了Q、K、V这三个矩阵：首先通过Q矩阵**和转置后的K矩阵转置相乘，得到结果后再除以dk的开平方，再通过softmax函数得到一个归一化的结果，最后将归一化的结果和矩阵V相乘就得到了表示注意力值的矩阵。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8e8a8de27d5f4abe9094fa5003b39776~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=893&h=349&s=14389&e=png&b=ffffff" alt="图片">这里的Q、K、V三个矩阵(查询矩阵、键矩阵、值矩阵)是通过原始token的embedding矩阵计算得到的，具体的方法是，先训练出三个矩阵：Wq,Wk,Wv, 然后使用embedding处理后的X矩阵和这三个矩阵相乘得到：</p><p><strong>Q&#x3D;Wq * X</strong></p><p><strong>K&#x3D;Wk * X</strong></p><p><strong>V&#x3D;Wv * X</strong></p><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/050c3ab21ac1434ea7f2b3d92b75e1cf~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=581&h=658&s=13223&e=png&b=ffffff" alt="图片"><strong>需要注意的是我们embedding输入原本是向量，并不是矩阵，这里是将所有的向量打包之后，形成了一个矩阵，方便矩阵之间的计算。</strong> 下面我们一步步了解下注意力值是怎么计算的，使用原始的embedding，而不是打包后的矩阵，<strong>首先模型将会为句子中的每个token都计算出一个score分数，这个分数表示了该token对句子中其他token的关注程度，分数越高关注度越高。</strong> <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/770bcbac42004c0eb9f8b14e05e06f84~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=697&s=87041&e=png&b=ffffff" alt="图片">但是需要注意的是这里计算得到的中间向量q、k、v的维度是<strong>64维</strong>，小于Encoder接收的输入向量的维度，这是一个经过计算后得到的相对稳定的数值。如下图所示，当我们在计算Tinking这个token的注意力值时，会依次计算出Thinking对其他token(在这里也就是Thinking和Machines)的注意力值，计算token1对其他各个token的score的方式是：q1 与 k1 做点积，q1 与 k2 做点积。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/87a6ccfaa33e4973844d82aa126df4bc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=685&h=358&s=20465&e=png&b=ffffff" alt="图片">得到每个score后，再把score除以K向量维度的平方根也就是√64&#x3D;8，然后将结果通过Softmax进行归一化，得到一个0<del>1之间的概率值，所有的归一化的加和值等于1。![图片](<a href="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c612b31ae464ce592b2f17a644dfba3">https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c612b31ae464ce592b2f17a644dfba3</a></del>tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w&#x3D;867&amp;h&#x3D;546&amp;s&#x3D;35500&amp;e&#x3D;png&amp;b&#x3D;ffffff)最后将Softmax的值，与V向量相乘，得到自注意力层的输出结果向量：z1和z2，需要注意的是相乘的过程中会将不相关的token的关注度降低。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/39f75aa4e9e44189bbd34c6d023baff4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=786&h=747&s=40925&e=png&b=ffffff" alt="图片">到这里其实已经把Encoder是怎么计算每个token对句子中其他token的注意力值的方法解释清楚了，下面我们用一张图从更高的层面来观察这个过程，假设我们想要翻译下面这个句子：<strong>The animal didn’t cross the street because it was too tired.</strong> 句子中的it是表示什么呢，是animal还是street？模型就是通过自注意力值来确定的，当对it进行编码时，Encoder会对句子中的每个token都计算一遍注意力值，模型会将it更关注的“The animal”编码到it中。这样模型在后续的处理过程中就能知道it指代的是“The animal”。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e33ed7ffc7e1414c840370969f5fc3c6~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=437&h=413&s=33175&e=png&b=fef7f6" alt="图片"></p><p><strong>多头注意力机制</strong></p><p>论文中通过引入多个Encoder形成了一种“多头注意力”的机制，对自注意力层进行了能力的提升，主要包括：</p><ol><li>多头注意力扩展了模型关注不同位置的能力</li><li>多头注意力为自注意力层提供了多个子空间</li></ol><p>Transformer模型使用了8个注意力头，所以在计算每个Encoder的输出时，我们会得到8个z向量。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6b3bea63f03b486297ff4804f1365afc~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=638&s=43187&e=png&b=ffffff" alt="图片"><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b0ce1bd6398842b7b2b82e3f37868a88~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1018&h=483&s=30082&e=png&b=ffffff" alt="图片">但是前馈层只能接收1个z向量，所以我们还需要将这8个z向量做压缩得到1个向量，具体的做法是将这8个z向量链接起来，然后乘以一个附加的权重矩阵Wo，最后得到z向量，如下图所示：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3d0c0449f7d491c90d043ceda3171d9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=597&s=68367&e=png&b=ffffff" alt="图片">最后我们用一张完整的大图来描述下在多个Encoder下，注意力值的计算过程，也就是多头注意力机制：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6661db08ea6b4e468b83abf6f5863120~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=605&s=128979&e=png&b=ffffff" alt="图片">下面我们可以看下，在多头注意力机制下，在编码it这个token时，模型在注意哪些其他的token：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0628d2f3618346ad92352ec43ba1dcf8~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=438&h=395&s=35228&e=png&b=fdf8f8" alt="图片"><strong>可以看到其中一个头(橙色)更关注“The Animal”，因为这两个token对应的橙色更深，另外一个头(绿色)则更关注“tired”，因为这两个token对应的绿色更深。</strong></p><p><strong>残差网络</strong></p><p>首先我们了解下什么是残差网络，残差网络（Residual Network，简称ResNet）是一种深度卷积神经网络（CNN）架构，由Microsoft Research Asia的Kaiming He**等人在2015年提出。ResNet的核心思想是通过引入“残差学习”（residual learning）来解决深度神经网络训练中的退化问题（degradation problem）。在传统的深度神经网络中，随着网络层数的增加，理论上网络的表示能力应该更强，但实际上，过深的网络往往难以训练，性能反而不如层数较少的网络。这种现象被称为“退化问题”，即随着网络深度的增加，网络的准确率不再提升，甚至下降。ResNet通过引入“跳跃连接”（skip connections）或“捷径连接”（shortcut connections）来解决这个问题。在ResNet中，输入不仅传递给当前层，还直接传递到后面的层，跳过一些中间层。这样，后面的层可以直接学习到输入与输出之间的残差（即差异），而不是学习到未处理的输入。这种设计允许网络学习到恒等映射（identity mapping），即输出与输入相同，从而使得网络可以通过更简单的路径来学习到正确的映射关系。在Transformer模型中，残差网络的使用主要是为了解决自注意力机制（self-attention）带来的问题。Transformer模型完全基于注意力机制，没有卷积层，但其结构本质上也是深度网络。在Transformer中，每个编码器（encoder）和解码器（decoder）层都包含自注意力和前馈网络，这些层的参数量非常大，网络深度也很容易变得很深。使用残差连接可以帮助Transformer模型更有效地训练深层网络。在Transformer的自注意力层中，输入通过自注意力和前馈网络后，与原始输入相加，形成残差连接。这种设计使得网络即使在增加更多层数时，也能保持较好的性能，避免了退化问题。总结来说，残差网络在Transformer模型中的应用解决了以下几个问题：</p><ol><li><strong>缓解退化问题：</strong> 通过残差学习，使得网络即使在增加层数时也能保持或提升性能。</li><li><strong>加速收敛：</strong> 残差连接提供了梯度的直接路径，有助于梯度在深层网络中的传播，加速训练过程。</li><li><strong>提高表示能力：</strong> 允许网络学习更复杂的函数，同时保持对简单函数的学习能力。</li></ol><p>Transformer模型的成功部分归功于残差连接的设计，这使得它能够构建更深、更强大的模型，从而在自然语言处理（NLP）和计算机视觉等领域取得了显著的成果。<strong>可以使用下面这张图来解释残差网络，原始向量x在经过自注意力层之后得到z向量，为了防止网络过深带来的退化问题，Transformer模型使用了残差网络，具体做法是使用计算得到的z矩阵，在和原始输入的x矩阵做残差链接，即图中的X+Z，然后使用LayerNorm函数进行层归一化，计算得到新的z向量，然后输入到前馈层。</strong> <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8a8c412a4463458a87d04c47092172e4~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=712&h=666&s=59798&e=png&b=fffbfa" alt="图片">将 <strong>Add &amp; Normalize</strong> 简化之后表示为如下：<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d54dcf5be99426c9eeb12dd416070b2~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=666&h=483&s=48032&e=png&b=fffcfc" alt="图片"></p><p><strong>前馈网络</strong></p><p>归一化后的残差输出，被送入点对点前馈网络进行进一步处理，点对点前馈网络是几个线性层，中间有ReLU激活函数，将点对点输出的结果与前馈网络的输入进行相加做残差链接，然后再做进一步的归一化。点对点前馈网络主要用于进一步处理注意力的输出，让结果具有更丰富的表达。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7c325276afe5419c9309efb34bce9480~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=936&h=716&s=116576&e=png&b=000000" alt="图片">到这里编码器就已经介绍完了，编码器输出的结果中携带了每个token的相关注意力信息，将用以帮助解码器在解码过程中关注输入中的特定的token信息。</p><h2 id="六、理解Token在解码器中的流转"><a href="#六、理解Token在解码器中的流转" class="headerlink" title="六、理解Token在解码器中的流转"></a><strong>六</strong>、<strong>理解Token在解码器中的流转</strong></h2><p>每个解码器拥有与编码器相似的结构但也有不同的地方，它有两个多头注意力层，一个点对点前馈网络层，并且在每个子层之后都有残差链接和层归一化。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/12979fcd9e1d4c1ba44e0ffd0d4281c9~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=470&h=1306&s=124247&e=png&b=fefefe" alt="图片">解码器是自回归的，它将前一个Decoder输出的结果和来自编码器输出的注意力信息当做解码器的输入。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b32f24652d7d47a4b6e48e523ddcbae0~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=614&s=174102&e=png&b=fff7f4" alt="图片">这里我们需要了解清楚，解码器的先前的输出结果是怎么得到的，即解码器的第一个输出结果从哪得到的。在Transformer模型的训练过程中，解码器的第一个输出序列通常是根据特定的起始标记（start token）或者一个预先定义的初始状态得到的。这个起始标记是一个特殊的符号，它标志着输出序列的开始。以下是解码器如何获得第一个输出序列的详细过程：<strong>1. 起始标记：</strong> 在训练阶段，解码器的输入序列通常以一个起始标记（例如）开始。这个标记是一个预定义的词汇表中的词，它告诉解码器输出序列的生成即将开始。<strong>2. 初始化状态：</strong> 解码器在开始生成输出之前，会接收到编码器的输出，即编码器的上下文向量。这些上下文向量包含了输入序列（如源语言文本）的信息。在某些情况下，解码器的初始状态也可以通过一个额外的向量（如位置编码）来初始化，以提供序列中每个位置的信息。<strong>3. 自注意力机制：</strong> 在第一个时间步，解码器使用自注意力机制处理起始标记。由于此时没有之前的输出，自注意力机制会关注起始标记本身，以及可能的位置编码。<strong>4. 编码器-解码器注意力：</strong> 解码器接着使用编码器-解码器注意力机制来关注编码器的输出。这允许解码器在生成第一个输出时就利用到输入序列的信息。<strong>5. 输出层：</strong> 解码器的输出层将上述步骤得到的向量转换为概率分布，这个分布表示了词汇表中每个词作为第一个输出的概率。<strong>6. 选择第一个输出：</strong> 在训练阶段，解码器可能会使用强制策略，这意味着解码器的第一个输出会直接使用目标序列中的第一个词。在推理阶段，解码器会根据概率分布选择概率最高的词作为第一个输出。<strong>7. 迭代生成：</strong> 一旦获得了第一个输出，解码器就会将其作为下一个时间步的输入，并重复上述过程来生成后续的输出序列。在实际应用中，解码器的第一个输出序列的生成方式可能会根据具体的任务和模型配置有所不同。例如，在某些情况下，解码器可能会接收到一个完整的前缀序列，而不是单一的起始标记，这在文本摘要任务中较为常见。在这种情况下，解码器会基于这个前缀序列来生成剩余的输出序列。</p><p><strong>Masked多头注意力机制</strong></p><p>需要注意的是解码器中的第一层是一个特殊的多头注意力层，是一个执行了mask的注意力层，因为解码器是自回归的，并且依次生成结果token，我们需要防止它在处理某个token时，对未来的token进行处理，<strong>原因是模型在训练的时候，是不知道未来输出的token是什么的，为了保证训练过程和解码的过程的一致性，我们需要让解码器在计算某个token的注意力值的时候，只关注这个句子中已经出现过的token，而屏蔽掉句子中当前token之后的其他token的信息。</strong> 可以通过以下这张图来描述Mask的过程，当解码器在处理it时，应该把it之后的所有token屏蔽掉。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1032e811031e4b0191c6a429b8c3aeef~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=928&h=832&s=201434&e=png&b=fcf9f9" alt="图片"></p><p><strong>计算注意力值</strong></p><p>在解码器中计算注意力值时，是用Encoder最后的输出，和每一个Decoder进行交互，这就需要Decoder中的第二层Encoder-Decoder Attention。每个Decoder计算出结果之后，再作为输入传递给下一个Decoder。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1777def6d9af4fe9963659f73ce518c3~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=703&s=111525&e=png&b=ffffff" alt="图片"></p><p><strong>线性分类器&amp;Softmax</strong></p><p>当最后一个Decoder计算完毕后，解码器得到了一个输出结果的向量数据。我们如何把它变成一个词呢？这就是最后一个 Linear 层的工作，后面是 Softmax 层。线性层是一个简单的全连接神经网络，它将解码器产生的向量投影到一个更大的向量中，称为 logits 向量。假设我们的模型知道从训练数据集中学习的 10,000 个独特的英语单词。这将使 logits 向量有 10,000 个单元格宽——每个单元格对应一个唯一单词的分数，这就是我们解释线性层模型输出的方式。然后，softmax 层将这些分数转换为概率（全部为正，全部加起来为 1.0）。选择概率最高的单元格，并生成与其关联的单词作为该时间步的输出。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/94a8e0c600e14c91900fe9d2f916134e~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=869&h=561&s=53732&e=png&b=fffefe" alt="图片"></p><p><strong>编解码器的协同工作</strong></p><p>现在让我们看看编码器和解码器之间是如何协同工作的。编码器首先处理输入的文本token，然后输出一组注意力向量 K 和 V。这些向量将由每个解码器在其“编码器-解码器注意力”层中使用，这有助于解码器关注输入序列中的特定token的位置信息，具体计算注意力值的方法跟编码器中是一样的，<strong>需要注意的是，这里的K、V矩阵来自于编码器的输出，而Q矩阵来自于解码器的输入。</strong> <img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cb7c17840eec47bfbbe9ff960c9b0736~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=671&s=121710&e=png&b=fefcfc" alt="图片">重复回归以上的步骤，直到出现结束符号的标识，表示解码器已完成其输出。每个步骤的输出在下一个时间步骤中被反馈到底部解码器，并且解码器像编码器一样向上反馈其解码结果。就像我们对编码器输入所做的那样，我们将位置编码嵌入并添加到这些解码器输入中以指示每个单词的位置。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/94d20a1917514e9db702c64ffd4e9811~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=603&s=110156&e=png&b=fefcfc" alt="图片">至此，已经分析完Transformer的编码器和解码器的全流程了。</p><h2 id="七、Transformer-XL怎样提升上下文长度"><a href="#七、Transformer-XL怎样提升上下文长度" class="headerlink" title="七、Transformer-XL怎样提升上下文长度"></a><strong>七</strong>、<strong>Transformer-XL怎样提升上下文长度</strong></h2><p>传统的Transformer模型中，上下文长度是固定的，主要有以下几个原因：</p><ol><li><strong>计算效率：</strong> 在最初的设计中，Transformer模型是为了处理序列到序列的任务，如机器翻译。对于这类任务，输入序列（如源语言句子）和输出序列（如目标语言句子）通常具有相似的长度，因此固定上下文长度可以简化模型设计，提高计算效率。</li><li><strong>模型复杂度：</strong> Transformer模型的核心是自注意力机制，该机制在计算时需要对序列中的每个元素进行成对的比较，以计算注意力权重。如果上下文长度不固定，那么每次添加或删除元素时，都需要重新计算整个序列的注意力权重，这会导致计算复杂度和内存需求急剧增加。</li><li><strong>训练稳定性：</strong> 固定长度的上下文可以提供稳定的训练环境，有助于模型学习到更加一致和可靠的表示。如果上下文长度不固定，模型可能需要在每次迭代中适应新的序列长度，这可能会影响训练的稳定性和模型的收敛速度。</li><li><strong>硬件限制：</strong> 在实际应用中，硬件资源（如GPU内存）是有限的。固定长度的上下文可以确保模型在任何时候都不会超出硬件资源的限制，从而避免因资源不足而导致的训练中断。</li><li><strong>模型泛化：</strong> 固定长度的上下文允许模型在训练时学习到特定长度范围内的依赖关系，这有助于模型在实际应用中泛化到类似的序列长度上。</li></ol><p>然而，固定上下文长度也带来了一些限制，特别是在处理长序列时，模型无法捕获超过固定长度的依赖关系，这限制了模型在某些任务（如长文本生成和理解）上的性能。为了解决这个问题，Transformer-XL等模型通过引入新的机制来允许处理更长的上下文，从而在不牺牲计算效率的情况下捕获更长期的依赖关系。国产开源公司月之暗面的大模型产品kimi，能够处理长达20万字的超长上下文，那么他是怎么做到的呢，核心是他的模型在Transformer的基础上做了扩展，实现了自己的Transformer-XL架构。Transformer-XL通过引入两个关键的技术改进来提升token上下文长度的处理能力：片段递归机制（segment-level recurrence）和相对位置编码机制（relative positional encoding）。</p><ol><li><strong>片段递归机制：</strong> 在传统的Transformer模型中，由于上下文长度是固定的，模型无法捕获超过预定义上下文长度的长期依赖性。Transformer-XL通过引入循环机制来解决这个问题。具体来说，它不再从头开始计算每个新片段的隐藏状态，而是重复使用之前片段中获得的隐藏状态，并将这些状态作为当前片段的“记忆”。这样，信息就可以在不同片段之间传递，从而捕获更长的依赖关系。这种机制允许模型在不引起时间混乱的前提下，超越固定长度去学习依赖性，同时解决了上下文碎片化问题。</li><li><strong>相对位置编码机制：</strong> 在Transformer-XL中，为了能够在不造成时间混乱的情况下重复使用状态，引入了相对位置编码的概念。相对位置编码与传统的绝对位置编码不同，它只编码token之间的相对位置关系，而不是token与固定起始点的绝对位置。这种编码方式使得模型能够在处理长序列时更有效地利用位置信息，并且可以泛化至比在训练过程中观察到的长度更长的注意力长度。</li></ol><p>通过这两种机制，Transformer-XL显著提升了模型在处理长序列时的性能。</p><h2 id="八、Transformer相关应用分享"><a href="#八、Transformer相关应用分享" class="headerlink" title="八、Transformer相关应用分享"></a><strong>八</strong>、<strong>Transformer相关应用分享</strong></h2><p><strong>使用BERT做掩词填充</strong></p><p>BERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言表示模型，由Google AI在2018年提出。BERT的核心创新在于利用Transformer架构的编码器部分来学习文本数据的深层次双向表示。这种表示能够捕捉到文本中词汇的上下文关系，从而在多种自然语言处理（NLP）任务中取得了显著的性能提升。以下是BERT模型的一些关键特点：<strong>双向上下文理解：</strong> 与之前的单向语言模型不同，BERT通过在预训练阶段使用掩码语言模型（Masked Language Model, MLM）任务，学习了词汇在句子中的双向上下文信息。这意味着模型能够同时考虑一个词前后的词汇来理解其含义。<strong>预训练和微调：</strong> BERT采用了两阶段的训练策略。在预训练阶段，BERT在大量文本数据上进行无监督学习，学习语言的通用模式。在微调阶段，BERT可以通过少量标注数据针对特定任务进行有监督学习，以适应各种NLP任务，如情感分析、问答系统、命名实体识别等。<strong>Transformer架构：</strong> BERT基于Transformer的编码器部分，这是一种注意力机制（Attention Mechanism）的架构，它允许模型在处理序列数据时考虑序列中所有位置的信息。<strong>大规模预训练：</strong> BERT在非常大的文本语料库上进行预训练，这使得模型能够学习到丰富的语言知识。预训练的规模和质量对模型性能有重要影响。<strong>多样化的任务适应性：</strong> 通过微调，BERT可以适应多种不同的NLP任务，而不需要对模型架构进行大的修改。这使得BERT成为了一个灵活且强大的工具。BERT的推出标志着NLP领域的一个重大进展，它在多项NLP任务上刷新了记录，并催生了一系列基于BERT的改进模型，如RoBERTa、ALBERT、DistilBERT等。这些模型在不同的方面对BERT进行了优化，以提高性能、减少计算资源消耗或改善特定任务的表现。以下是使用BERT做掩词填充的示例，输入一段文本，让模型预测出下一个被掩盖的词：<a href="https://link.juejin.cn/?target=https://huggingface.co/google-bert/bert-base-uncased!%5B%E5%9B%BE%E7%89%87">huggingface.co&#x2F;google-bert…</a>](<a href="https://link.juejin.cn/?target=https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5872dfd6621145abbcd888a90d28f7e4~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image%23?w=755&h=477&s=42042&e=png&b=ffffff">p3-juejin.byteimg.com&#x2F;tos-cn-i-k3…</a>)</p><p><strong>使用BART做文本摘要</strong></p><p>BART（Bidirectional and Auto-Regressive Transformers）是一种先进的自然语言处理（NLP）模型，它结合了BERT（Bidirectional Encoder Representations from Transformers）和GPT（Generative Pre-trained Transformer）的特点，用于文本理解和生成任务。BART模型特别擅长处理不同类型的文本噪声和序列变换，使其在多种NLP任务中表现出色。<strong>设计原理和结构</strong></p><p>BART是基于Transformer架构的自编码自回归模型。它通过两个主要步骤进行预训练：</p><ol><li>使用任意噪声函数破坏文本（例如，随机打乱句子顺序、删除或遮蔽token）。</li><li>模型学习重建原始文本。</li></ol><p>这种预训练方式使得BART能够有效地处理文本生成、翻译和理解等任务。BART的编码器是双向的，能够捕捉文本的前后文信息，而解码器是自回归的，能够基于前面的输出生成后续的内容。<strong>应用</strong>BART在多种NLP任务上取得了显著的成绩，包括但不限于：</p><ul><li>文本摘要</li><li>机器翻译</li><li>对话生成</li><li>问答系统</li><li>文本分类</li></ul><p><strong>与其他模型的对比</strong>与其他预训练模型相比，BART在处理文本生成任务时尤其出色。它在自然语言理解任务中也有很好的表现，与BERT和GPT等模型相比，BART在多个基准数据集上取得了竞争性或更好的结果。<strong>预训练和微调</strong>BART模型通过大量的文本数据进行预训练，然后在特定任务上进行微调。预训练阶段，模型学习如何从噪声文本中恢复原始文本，而微调阶段则是针对特定任务调整模型参数，以优化任务性能。以下是使用BART做文本摘要的应用示例：<a href="https://link.juejin.cn/?target=https://huggingface.co/facebook/bart-large-cnn!%5B%E5%9B%BE%E7%89%87">huggingface.co&#x2F;facebook&#x2F;ba…</a>](<a href="https://link.juejin.cn/?target=https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6ed7a1c38b024c40814d3f6e3a658338~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image%23?w=708&h=615&s=111417&e=png&b=fefefe">p3-juejin.byteimg.com&#x2F;tos-cn-i-k3…</a>)</p><p><strong>使用DistilBERT做问答</strong></p><p>DistilBERT是一种轻量级的BERT模型，它通过知识蒸馏（knowledge distillation）技术从预训练的BERT模型中学习知识。这种方法的核心思想是使用一个较小的BERT模型作为“学生”模型，而原始的、较大的BERT模型则充当“教师”模型。在训练过程中，学生模型尝试复制教师模型的输出，以此来学习教师模型的知识。<strong>主要特点和优势</strong> <strong>模型大小和效率：</strong> DistilBERT的模型大小和参数量都比原始的BERT模型小，这使得它在资源受限的环境中（如移动设备）更加实用。它的推理速度也比BERT快，因为它需要处理的参数更少。<strong>知识蒸馏：</strong> DistilBERT使用了一种称为“软目标”的知识蒸馏方法。在这种方法中，学生模型不仅学习来自训练数据的标签，还学习教师模型的输出，这些输出被视为附加的、软性的标签。<strong>保持性能：</strong> 尽管DistilBERT的模型大小减小了，但它仍然保持了与原始BERT模型相当的性能，特别是在自然语言理解任务上。<strong>灵活性：</strong> DistilBERT保留了BERT模型的基本架构，包括Transformer的串联层，这使得它可以很容易地适应各种下游任务。<strong>结构和训练</strong>DistilBERT的结构相对简单，它仅保留了BERT的6层Transformer，删除了token type embedding和pooler层。在训练过程中，它使用了一种称为“模型压缩”的技术，通过这种方法，模型的层数被减半，同时从教师模型的层初始化学生模型的层。<strong>应用场景</strong>由于其较小的模型大小和较快的推理速度，DistilBERT适用于需要快速处理和低资源消耗的NLP任务，例如文本分类、情感分析、问答系统和语言模型等。总的来说，DistilBERT是一个高效的BERT变体，它通过知识蒸馏技术实现了模型的压缩，同时保持了良好的性能，特别适合在资源受限的环境中使用。以下是是使用DistilBERT做问答的实例：<a href="https://link.juejin.cn/?target=https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad!%5B%E5%9B%BE%E7%89%87">huggingface.co&#x2F;distilbert&#x2F;…</a>](<a href="https://link.juejin.cn/?target=https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/03d3a52d36404160aaa932cf8bee79b6~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image%23?w=657&h=627&s=119134&e=png&b=fdfdfd">p3-juejin.byteimg.com&#x2F;tos-cn-i-k3…</a>)</p><p><strong>使用T5做文本翻译</strong></p><p>T5模型，全称为“Text-to-Text Transfer Transformer”，是由Google Research团队开发的一种自然语言处理（NLP）模型。T5模型的核心思想是将所有NLP任务统一转换为文本到文本（Text-to-Text）的格式，从而可以使用同一个模型和训练过程来处理多种不同的任务，如翻译、摘要、问答等。<strong>主要特点和优势</strong> <strong>统一的框架：</strong> T5模型通过将任务转换为文本到文本的格式，简化了不同NLP任务的处理方式。例如，对于翻译任务，输入可以是“translate English to German: [English text]”，输出则是翻译后的文本。基于Transformer架构：T5模型采用了Transformer的encoder-decoder架构，这是一种高效的网络结构，特别适合处理序列数据。<strong>预训练和微调：</strong> T5模型首先在大规模的数据集上进行预训练，学习语言的通用表示，然后可以针对特定任务进行微调，以优化任务性能。<strong>广泛的应用场景：</strong> T5模型可以应用于多种NLP任务，包括但不限于文本分类、命名实体识别、情感分析、机器翻译和对话生成等。<strong>高效的计算能力：</strong> T5模型的设计允许它高效地处理大规模数据集，并且具有强大的并行处理能力。<strong>训练和应用</strong>T5模型在训练时使用了一种称为“C4”的大规模数据集，这个数据集由经过清洗的Common Crawl数据组成。模型通过不同的预训练目标和策略进行训练，包括自回归、自编码和文本重排等。在应用方面，T5模型的强大语言表示能力和广泛的应用场景使其成为NLP领域的一个重要工具。它可以通过微调来适应不同的领域和任务，从而在多个NLP任务上取得优异的性能。T5模型通过其创新的Text-to-Text框架和基于Transformer的架构，在自然语言处理领域提供了一种新的解决方案，能够处理多种复杂的语言任务，并且具有很好的扩展性和适应性。以下是使用T5做文本翻译的示例：<a href="https://link.juejin.cn/?target=https://huggingface.co/google-t5/t5-base!%5B%E5%9B%BE%E7%89%87">huggingface.co&#x2F;google-t5&#x2F;t…</a>](<a href="https://link.juejin.cn/?target=https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4b35f058ef3646969c0ba292300f0ebb~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image%23?w=666&h=451&s=33125&e=png&b=ffffff">p3-juejin.byteimg.com&#x2F;tos-cn-i-k3…</a>)</p><p><strong>使用GPT-2写小说</strong></p><p>GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的自然语言处理（NLP）模型，它是GPT系列模型的第二代。GPT-2在自然语言理解和生成方面表现出色，能够生成连贯、相关且多样化的文本。这个模型在发布时因其生成文本的质量和多样性而受到广泛关注。<strong>主要特点和优势</strong> <strong>大规模预训练：</strong> GPT-2通过在大规模的互联网文本数据集上进行预训练，学习到了丰富的语言模式和知识。这种预训练使得模型能够理解和生成自然语言文本。<strong>Transformer架构：</strong> GPT-2基于Transformer模型架构，这是一种依赖于自注意力（self-attention）机制的深度学习架构，非常适合处理序列数据，如文本。<strong>无监督学习：</strong> GPT-2采用无监督学习方法，通过预测下一个词的任务来预训练模型。这种训练方式不依赖于标注数据，使得模型能够学习到更广泛的语言知识。<strong>生成能力：</strong> GPT-2特别擅长文本生成任务，能够生成连贯、有逻辑的段落和文章，甚至能够模仿特定的写作风格。<strong>多样性：</strong> GPT-2能够处理多种语言任务，包括文本生成、翻译、问答、摘要等。<strong>版本和规模</strong>GPT-2有多个版本，不同版本之间主要区别在于模型的大小和参数数量。例如，最小的版本有1.17亿个参数，而最大的版本（GPT-2 1.5 Billion）有15亿个参数。随着模型规模的增加，性能和生成文本的质量也相应提高。<strong>应用场景</strong>GPT-2可以应用于多种场景，如聊天机器人、文本摘要、内容创作辅助、语言翻译等。它的生成能力使得在创意写作、新闻生成和其他需要自然语言生成的领域中具有潜在的应用价值。<strong>挑战和限制</strong>尽管GPT-2在生成文本方面表现出色，但它也面临一些挑战和限制，包括生成文本的偏见问题、事实准确性问题以及潜在的滥用风险。因此，OpenAI在发布GPT-2时采取了谨慎的态度，逐步放开对模型的访问权限。总的来说，GPT-2是一个强大的NLP模型，它在文本生成和理解方面的能力使其成为自然语言处理领域的一个重要里程碑。以下是使用GPT-2做文本生成的示例：<a href="https://link.juejin.cn/?target=https://huggingface.co/openai-community/gpt2!%5B%E5%9B%BE%E7%89%87">huggingface.co&#x2F;openai-comm…</a>](<a href="https://link.juejin.cn/?target=https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/05b480b976be4351a9aee2ddf51a6a75~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image%23?w=688&h=391&s=39014&e=png&b=ffffff">p3-juejin.byteimg.com&#x2F;tos-cn-i-k3…</a>%E5%AE%8C%E6%95%B4%E7%9A%84%E4%BD%93%E9%AA%8C%E5%9C%B0%E5%9D%80%EF%BC%9Ahttps%3A%2F%2Ftransformer.huggingface.co%2Fdoc%2Fgpt2-large%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%BE%93%E5%85%A5%E4%B8%80%E6%AE%B5%E5%B0%8F%E8%AF%B4%E7%9A%84%E5%BC%80%E5%A4%B4%EF%BC%8C%E6%AF%94%E5%A6%82%EF%BC%9AAs) aliens entered our planet，然后Transformer就会依据我们输入的文本，自动脑补剩下的小说情节。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d3275388f85411382c53e30e252401c~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=655&s=159877&e=png&b=fefefe" alt="图片">那Transformer是怎么做到的呢？如下图所示，Transformer在生成每一个token时，会参考前面所有的token，并生成与之相符的token，这样循环往复就能生成完整的一段内容。<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6b73860b769a4d43bfa7ebff36d232fb~tplv-k3u1fbpfcp-jj-mark:3024:0:0:0:q75.awebp#?w=1080&h=336&s=39118&e=png" alt="图片"></p><h2 id="九、参考文档"><a href="#九、参考文档" class="headerlink" title="九、参考文档"></a><strong>九</strong>、<strong>参考文档</strong></h2><p><a href="https://link.juejin.cn/?target=https://arxiv.org/pdf/1706.03762.pdfhttps://jalammar.github.io/illustrated-transformer/https://www.bilibili.com/video/BV1ih4y1J7rx">arxiv.org&#x2F;pdf&#x2F;1706.03…</a></p><p>*<strong>文&#x2F;</strong> 逅弈</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;深入理解Transformer&quot;&gt;&lt;a href=&quot;#深入理解Transformer&quot; class=&quot;headerlink&quot; title=&quot;深入理解Transformer&quot;&gt;&lt;/a&gt;深入理解Transformer&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;作者：得物</summary>
      
    
    
    
    <category term="AI" scheme="http://ai.mak.cn/categories/AI/"/>
    
    
    <category term="深度学习" scheme="http://ai.mak.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
